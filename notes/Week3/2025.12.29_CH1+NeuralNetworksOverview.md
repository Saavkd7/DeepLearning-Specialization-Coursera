---
# My Diary

Today  **Monday, 29 Dec 2025**.

## General Counter
It has been    **134** days since I started this diary.

# Repository Counter

Day **6** From I started this repository



# Deep Learning Fundamentals: Neural Network Overview

## 1. Executive Summary
The "Shallow" Neural Network you are about to build is simply **Logistic Regression repeated twice**. By stacking these operations, the model can learn more complex non-linear patterns. If you understood the Forward and Backward propagation graphs from the previous videos, you already understand 90% of Neural Networksâ€”we are just adding more "nodes" to the graph.



---

## 2. Technical Deep Dive

### The Architecture Shift
* **Logistic Regression (Previous):**
    * Input $x$ $\rightarrow$ $z = w^T x + b$ $\rightarrow$ $a = \sigma(z)$ $\rightarrow$ Loss $\mathcal{L}(a, y)$.
    * This is a single computation step.

* **Neural Network (New):**
    We stack the previous concept. The output of the first step becomes the input of the second step.
    1.  **Layer 1 (Hidden Layer):**
        * Input: $x$
        * Compute: $z^{[1]} = W^{[1]} x + b^{[1]}$
        * Activation: $a^{[1]} = \sigma(z^{[1]})$
    2.  **Layer 2 (Output Layer):**
        * Input: $a^{[1]}$ (The output from Layer 1)
        * Compute: $z^{[2]} = W^{[2]} a^{[1]} + b^{[2]}$
        * Activation: $a^{[2]} = \sigma(z^{[2]})$ (This is your final prediction $\hat{y}$).

### Crucial Notation Update (`[]` vs `()`)
As we add layers, the notation becomes crowded. You must distinguish between **Example Index** and **Layer Index**.

* **Round Brackets $(i)$:** Refers to the **Training Example**.
    * $x^{(i)}$ is the $i$-th image in your dataset.
* **Square Brackets $[l]$:** Refers to the **Layer** of the network.
    * $W^{[1]}$ are the weights for the first layer.
    * $z^{[2]}$ is the linear calculation for the second layer.
* **Combined Example:** $a^{[2](i)}$
    * "The activation of the **2nd layer** for the **$i$-th training example**."

---

## 3. "In Plain English"

### The "Assembly Line" Analogy
* **Logistic Regression:** A single worker takes raw metal ($x$), welds it ($z$), paints it ($a$), and ships it.
* **Neural Network:** An assembly line with two stations.
    * **Station 1 (Layer 1):** Worker A takes raw metal ($x$), welds it into a frame ($z^{[1]}$), and polishes it ($a^{[1]}$). He passes this frame to Worker B.
    * **Station 2 (Layer 2):** Worker B takes the frame ($a^{[1]}$) (NOT raw metal), adds the engine ($z^{[2]}$), paints it ($a^{[2]}$), and ships the finished car.

---

## 4. Forward & Backward Flow
Just like before, the computation is organized into two passes:
1.  **Forward Propagation (Blue Arrows):**
    * $x \rightarrow z^{[1]} \rightarrow a^{[1]} \rightarrow z^{[2]} \rightarrow a^{[2]} \rightarrow \mathcal{L}$
    * Calculates the prediction and the cost.
2.  **Backward Propagation (Red Arrows):**
    * $da^{[2]} \leftarrow dz^{[2]} \leftarrow da^{[1]} \leftarrow dz^{[1]}$
    * Calculates the gradients ($dW^{[2]}, db^{[2]}, dW^{[1]}, db^{[1]}$) to update the weights.

---

