---
# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository

# Deep Learning Fundamentals: Neural Network Representation

## 1. Executive Summary
A Neural Network is composed of layers of nodes. The standard architecture discussed here is a **"2-Layer Neural Network"** (which actually has 3 parts: Input, Hidden, Output). The term "2-Layer" is used because we do not count the input layer as an official layer.


---

## 2. Technical Deep Dive

### The Architecture
1.  **Input Layer ($a^{[0]}$):**
    * Contains the raw feature vector $X$ ($x_1, x_2, x_3$).
    * Notation: $a^{[0]} = X$.
    * **Note:** This passes the data to the next layer but does not perform computation itself.

2.  **Hidden Layer ($a^{[1]}$):**
    * This is "Layer 1". It receives inputs from Layer 0 and performs intermediate calculations.
    * **Why "Hidden"?** In a supervised training set, you see the inputs $X$ and the targets $Y$. You do **not** see the values calculated in the middle of the network. Because these values are not observed in the training data, they are called "hidden".
    * **Parameters:** Associated with weights $W^{[1]}$ and bias $b^{[1]}$.

3.  **Output Layer ($a^{[2]}$):**
    * This is "Layer 2". It receives values from the hidden layer and generates the final prediction $\hat{y}$.
    * Notation: $\hat{y} = a^{[2]}$.
    * **Parameters:** Associated with weights $W^{[2]}$ and bias $b^{[2]}$.

### Notation Guide
The course introduces specific notation to handle the multiple layers:
* **$a^{[l]}$**: The activation vector for layer $l$.
* **$a^{[l]}_i$**: The activation of the $i$-th node in layer $l$.
* **Dimensions Example:**
    * If Layer 1 has 4 nodes and input has 3 features:
        * $W^{[1]}$ shape: $(4, 3)$ (Rows = current nodes, Cols = input features).
        * $b^{[1]}$ shape: $(4, 1)$.
    * If Layer 2 has 1 node and receives 4 inputs from Layer 1:
        * $W^{[2]}$ shape: $(1, 4)$.
        * $b^{[2]}$ shape: $(1, 1)$.

---

## 3. "In Plain English"

### The "Hidden" Logic
Imagine you are baking a cake.
* **Input ($x$):** Flour, Sugar, Eggs. (You see these).
* **Output ($y$):** A delicious Cake. (You see this).
* **Hidden Layer:** The chemical reactions happening inside the oven. You don't see the "fluffy batter state" or the "rising state" directlyâ€”they are hidden processes that transform the raw inputs into the final output. You only care that the parameters (temperature, time) are set correctly to make the output match your target.

---

## 4. Expert Nuance

### Counting Layers
In Deep Learning research, if you read a paper about a "100-layer network" (like ResNet-100), they are counting the **learnable** layers (Hidden + Output). They never count the Input layer.
* **Input Layer:** Layer 0 (No parameters).
* **Hidden Layer:** Layer 1 (Has parameters).
* **Output Layer:** Layer 2 (Has parameters).
* **Total Count:** 2-Layer Network.
