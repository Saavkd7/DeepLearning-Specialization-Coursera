# Deep Learning Fundamentals: Derivatives of Activation Functions

## 1. Executive Summary
To train a neural network using **Backpropagation**, we must calculate the **gradient** (slope) of the loss function with respect to the weights. Since weights affect the output through the activation function $g(z)$, we need to compute the derivative $$g'(z) \text{ (or } \frac{d}{dz}g(z)\text{)}$$.

## 2. Technical Deep Dive

### 1. Sigmoid Derivative
* **Function:** $g(z) = \frac{1}{1 + e^{-z}}$
* **Derivative Formula:**
  $$g'(z) = g(z)(1 - g(z)) = a(1 - a)$$
* **Mechanics:**
  * If $z = 10$, $a \approx 1$. Derivative $\approx 0$.
  * If $z = 0$, $a = 0.5$. Max derivative $= 0.25$.

### 2. Tanh Derivative
* **Function:** $g(z) = \tanh(z)$
* **Derivative Formula:**
  $$g'(z) = 1 - (g(z))^2 = 1 - a^2$$
* **Mechanics:**
  * If $z = 10$, $a \approx 1$. Derivative $\approx 0$.
  * If $z = 0$, $a = 0$. Max derivative $= 1$.

### 3. ReLU Derivative
* **Function:** $g(z) = \max(0, z)$
* **Derivative Formula:**
$$
g'(z) = \begin{cases} 
0 & \text{if } z < 0 \\
1 & \text{if } z > 0 
\end{cases}
$$
* **Mechanics:** Constant gradient of 1 when active. Prevents vanishing gradients.

### 4. Leaky ReLU Derivative
* **Function:** $g(z) = \max(0.01z, z)$
* **Derivative Formula:**
  $$
  g'(z) = \begin{cases} 
  0.01 & \text{if } z < 0 \\ 
  1 & \text{if } z > 0 
  \end{cases}
  $$
* **Mechanics:** Prevents "dead neurons" by allowing a small gradient for negative values.

## 3. "In Plain English"

### The "Hill Speed" Analogy
Imagine the activation function is a hill you are climbing. The **derivative** is simply the **steepness** of the hill at your current position.
* **Sigmoid/Tanh:** The hill is flat at the bottom (left) and flat at the top (right). The only steep part is the middle. If you are too far to the left or right, your "speed" (gradient) becomes zero.
* **ReLU:** It's a flat floor on the left (speed 0) and a constant 45-degree ramp on the right (constant speed).
* **Backpropagation:** We use this "steepness" value to decide how fast to run down the hill to minimize error.

## 4. Expert Nuance

### The "Sub-gradient" at Zero
Technically, ReLU is non-differentiable at $z=0$ (a sharp corner).
* In rigorous math, this is a problem.
* In Deep Learning, the probability of your floating-point input being *exactly* `0.0000000000` is statistically zero.
* Even if it happens, defining the derivative as a **sub-gradient** (any value between 0 and 1) keeps the optimization valid. We usually just pick 1 or 0 in the `if/else` block and move on.

## 5. Implementation & Visualization

This script visualizes the activation functions (Blue) and their derivatives (Orange) to show where the gradients vanish.

```python
import numpy as np
import matplotlib.pyplot as plt

def plot_activation_and_derivative():
    # Range of Z values (Input)
    z = np.linspace(-10, 10, 200)

    # --- DEFINE FUNCTIONS ---
    def sigmoid(x): return 1 / (1 + np.exp(-x))
    def sigmoid_deriv(x): return sigmoid(x) * (1 - sigmoid(x))

    def tanh(x): return np.tanh(x)
    def tanh_deriv(x): return 1 - np.tanh(x)**2

    def relu(x): return np.maximum(0, x)
    def relu_deriv(x): return np.where(x > 0, 1, 0)

    def leaky_relu(x): return np.maximum(0.1 * x, x)
    def leaky_relu_deriv(x): return np.where(x > 0, 1, 0.1)

    # --- PLOTTING SETUP ---
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    fig.suptitle('Activation Functions (Blue) vs Derivatives (Orange)', fontsize=16)
    
    # Helper to plot
    def plot_on_ax(ax, func, deriv, title):
        ax.plot(z, func(z), label='Activation (g)', color='blue', linewidth=2)
        ax.plot(z, deriv(z), label="Derivative (g')", color='orange', linestyle='--', linewidth=2)
        ax.set_title(title)
        ax.grid(True, alpha=0.3)
        ax.legend()
        ax.axhline(0, color='black', linewidth=0.5)
        ax.axvline(0, color='black', linewidth=0.5)

    # Plot 1: Sigmoid
    plot_on_ax(axes[0, 0], sigmoid, sigmoid_deriv, "Sigmoid\n(Notice deriv approaches 0 at ends)")

    # Plot 2: Tanh
    plot_on_ax(axes[0, 1], tanh, tanh_deriv, "Tanh\n(Steeper derivative than Sigmoid)")

    # Plot 3: ReLU
    plot_on_ax(axes[1, 0], relu, relu_deriv, "ReLU\n(Derivative is 0 or 1)")

    # Plot 4: Leaky ReLU
    plot_on_ax(axes[1, 1], leaky_relu, leaky_relu_deriv, "Leaky ReLU\n(Derivative is 0.1 or 1)")

    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    plot_activation_and_derivative()
