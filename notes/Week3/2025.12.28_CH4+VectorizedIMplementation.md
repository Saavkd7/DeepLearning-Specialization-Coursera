---
# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository

# Deep Learning Fundamentals: Vectorizing Across Multiple Examples

## 1. Executive Summary
Vectorization across multiple examples is the technique of processing the entire training dataset ($m$ examples) in a single mathematical step, rather than iterating through them one by one. By stacking individual input vectors into a large matrix, we allow the neural network to compute predictions for all examples simultaneously using optimized linear algebra operations. This removes the need for slow explicit `for` loops and drastically speeds up training.

## 2. Technical Deep Dive

### The Transition: From Vector to Matrix
To vectorize, we change the input from a single vector $x$ to a matrix $X$.
* **Input Matrix $X$**: We stack the $m$ training examples side-by-side as columns.
    $$X = \begin{bmatrix} | & | & & | \\ x^{(1)} & x^{(2)} & \dots & x^{(m)} \\ | & | & & | \end{bmatrix}$$
    * **Shape**: $(n_x, m)$, where $n_x$ is the number of input features and $m$ is the number of examples.

### The Vectorized Equations
Instead of running a loop `for i in 1 to m`, we apply the standard neural network equations to the entire matrix $X$ at once. The linear algebra naturally broadcasts the operations across all columns (examples).

1.  **Layer 1 (Hidden)**:
    $$Z^{[1]} = W^{[1]} X + b^{[1]}$$
    $$A^{[1]} = \sigma(Z^{[1]})$$
    * **Result Shape**: $(n^{[1]}, m)$. Rows correspond to hidden units; columns correspond to examples.

2.  **Layer 2 (Output)**:
    $$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$$
    $$A^{[2]} = \sigma(Z^{[2]})$$
    * **Result Shape**: $(n^{[2]}, m)$. For binary classification ($n^{[2]}=1$), this is a row vector containing predictions for all $m$ examples.

### Matrix Interpretation
In the matrices $Z$ and $A$:
* **Horizontal Axis (Columns)**: Scans across different **training examples** ($i=1 \dots m$).
* **Vertical Axis (Rows)**: Scans across different **nodes/units** in the layer.
    * Example: $A^{[1]}_{2, 5}$ is the activation of the **2nd hidden unit** for the **5th training example**.

## 3. "In Plain English"

### The "Assembly Line" Analogy
Imagine a car factory.
* **Non-Vectorized (For-Loop)**: A robot picks up one car frame, welds it, paints it, and finishes it. Then it goes back to pick up the next frame. This is slow.
* **Vectorized (Matrix)**: You have a massive conveyor belt with 1,000 car frames moving side-by-side. A giant stamping machine comes down and stamps **all 1,000 frames** at the exact same instant. The "paint sprayer" (Activation function) sprays a mist over the entire belt at once. You process the whole batch in the time it takes to process one.

## 4. Expert Nuance

### Broadcasting Logic
A common point of confusion is how the bias vector $b$ (shape $(n^{[1]}, 1)$) interacts with the matrix product $W X$ (shape $(n^{[1]}, m)$).
* **Strict Math**: You cannot add a vector to a matrix.
* **Python/NumPy**: "Broadcasting" implicitly copies the vector $b$ horizontally $m$ times to match the matrix dimensions.
* **Pitfall**: If your $b$ vector is a rank-1 array `(n,)` instead of a column vector `(n, 1)`, broadcasting might behave unpredictably. Always ensure your bias vectors are explicitly reshaped to `(n, 1)`.

## 5. Implementation Code

Here is the Python implementation comparing the loop approach (conceptual) to the vectorized approach.

```python
import numpy as np

def vectorized_forward_prop(X, W1, b1, W2, b2):
    """
    Computes forward propagation for the entire dataset X (m examples).
    
    Arguments:
    X -- Input data (n_x, m)
    W1 -- Weights layer 1 (n_h, n_x)
    b1 -- Bias layer 1 (n_h, 1)
    W2 -- Weights layer 2 (n_y, n_h)
    b2 -- Bias layer 2 (n_y, 1)
    
    Returns:
    A2 -- The output predictions for all m examples (n_y, m)
    """
    
    # --- LAYER 1 ---
    # Matrix Multiplication: (n_h, n_x) dot (n_x, m) -> (n_h, m)
    # Bias b1 is broadcasted to all m columns
    Z1 = np.dot(W1, X) + b1
    
    # Activation
    A1 = np.tanh(Z1)
    
    # --- LAYER 2 ---
    # Matrix Multiplication: (n_y, n_h) dot (n_h, m) -> (n_y, m)
    Z2 = np.dot(W2, A1) + b2
    
    # Activation (Sigmoid)
    A2 = 1 / (1 + np.exp(-Z2))
    
    return A2

# --- TEST ---
if __name__ == "__main__":
    # 3 Features, 4 Hidden Units, 1 Output Unit
    # 5 Examples (m=5)
    X_input = np.random.randn(3, 5) 
    
    # Initialize random parameters
    W1 = np.random.randn(4, 3)
    b1 = np.zeros((4, 1))
    W2 = np.random.randn(1, 4)
    b2 = np.zeros((1, 1))
    
    predictions = vectorized_forward_prop(X_input, W1, b1, W2, b2)
    
    print(f"Input shape: {X_input.shape}")      # (3, 5)
    print(f"Predictions shape: {predictions.shape}") # (1, 5)
    print("Successfully processed 5 examples in parallel.")
