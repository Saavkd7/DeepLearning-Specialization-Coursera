---
# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository

# Deep Learning Fundamentals: Activation Functions

## 1. Executive Summary
Activation functions are the mathematical "gates" that determine the output of a neural node given an input or set of inputs. Their primary role is to introduce **non-linearity** into the network. Without them, a neural network—no matter how many layers it has—would behave exactly like a single linear regression model. While the **Sigmoid** function was the historical standard, modern Deep Learning defaults to **ReLU** (Rectified Linear Unit) for hidden layers due to its efficiency and ability to prevent vanishing gradients.

## 2. Technical Deep Dive

### 1. Sigmoid Function
* **Formula**: $\sigma(z) = \frac{1}{1 + e^{-z}}$
* **Range**: $(0, 1)$
* **Usage**: Almost exclusively used for the **Output Layer** in binary classification (where you need a probability between 0 and 1).
* **Drawback**: Vanishing Gradient. When $z$ is very large or very small, the slope (derivative) is nearly 0, causing learning to stall.

### 2. Tanh (Hyperbolic Tangent)
* **Formula**: $\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$
* **Range**: $(-1, 1)$
* **Usage**: Superior to Sigmoid for **Hidden Layers**.
* **Why**: It is "zero-centered." Because its output ranges from -1 to 1, the average of the activations is closer to 0. This centers the data for the next layer, making optimization easier and faster than the strictly positive Sigmoid.

### 3. ReLU (Rectified Linear Unit) - The Standard
* **Formula**: $a = \max(0, z)$
* **Derivative**:
    * $1$ if $z > 0$
    * $0$ if $z < 0$
* **Usage**: The default choice for **Hidden Layers**.
* **Advantage**: It does not suffer from vanishing gradients when $z$ is positive. Computationally, it is incredibly fast (just a threshold check).
* **Disadvantage**: The derivative is exactly 0 when $z$ is negative. If a neuron enters this state, it stops learning ("Dead ReLU").

### 4. Leaky ReLU
* **Formula**: $a = \max(0.01z, z)$
* **Usage**: An alternative to ReLU if "Dead neurons" are a problem. It allows a small gradient ($0.01$) to flow even when $z$ is negative.

## 3. "In Plain English"

### The "Volume Knob" Analogy
Think of a neuron as a radio.
* **Linear (No Activation)**: The volume knob is stuck. Whatever signal comes in goes out. You can't filter anything.
* **Sigmoid/Tanh**: A "Squashing" knob. If the signal is quiet, it turns it to 0. If it's loud, it caps it at a maximum. The problem is, extreme signals get flattened so much you lose the detail (gradient).
* **ReLU**: A "Noise Gate".
    * If the signal is negative (static/garbage), it cuts it to **zero silence**.
    * If the signal is positive (music), it lets it through **unchanged** at full volume.
    * This simple "Silence the bad, Pass the good" logic is surprisingly effective for training deep networks.

## 4. Expert Nuance

### The "Zero-Centered" Optimization
The transcript highlights a subtle reason why **Tanh** beats **Sigmoid**: **Data Centering**.
* In Machine Learning, we always normalize input data to have $\mu=0$.
* If you use Sigmoid, the output of Layer 1 is always positive $[0, 1]$.
* This means Layer 2 receives "un-centered" data, which introduces a "zig-zagging" inefficiency in the gradient descent path for the next layer's weights.
* Tanh outputs $[-1, 1]$, effectively re-centering the data at every layer, keeping the optimization path straighter and faster.

## 5. Implementation Code

Here is how to implement these functions and their derivatives (needed for backprop) in NumPy.

```python
import numpy as np

class ActivationFunctions:
    @staticmethod
    def sigmoid(z):
        """Standard choice for Binary Output Layer"""
        return 1 / (1 + np.exp(-z))

    @staticmethod
    def tanh(z):
        """Better choice for Hidden Layers (Zero-centered)"""
        return np.tanh(z)

    @staticmethod
    def relu(z):
        """Best choice for Hidden Layers (No vanishing gradient)"""
        return np.maximum(0, z)

    @staticmethod
    def leaky_relu(z, alpha=0.01):
        """Fixes 'Dead ReLU' problem"""
        return np.maximum(alpha * z, z)

    # --- DERIVATIVES (For Backpropagation) ---
    
    @staticmethod
    def relu_derivative(z):
        """
        Gradient is 1 if z > 0, else 0.
        Technically undefined at z=0, but we set it to 0 or 1 in practice.
        """
        dz = np.array(z, copy=True)
        dz[z <= 0] = 0
        dz[z > 0] = 1
        return dz

# --- DEMO ---
if __name__ == "__main__":
    z_test = np.array([-10, -1, 0, 1, 10])
    
    print(f"Input: {z_test}")
    print(f"Sigmoid:    {ActivationFunctions.sigmoid(z_test)}")
    print(f"Tanh:       {ActivationFunctions.tanh(z_test)}")
    print(f"ReLU:       {ActivationFunctions.relu(z_test)}")
    print(f"Leaky ReLU: {ActivationFunctions.leaky_relu(z_test)}")
