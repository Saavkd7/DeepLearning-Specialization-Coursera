Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository

# Deep Learning Fundamentals: Logistic Regression & The Sigmoid

## 1. Executive Summary
Logistic Regression is a learning algorithm used for **binary classification** problems, where the output label $y$ is either 0 or 1 (e.g., Cat vs. Non-Cat). Unlike Linear Regression, which can output any arbitrary number, Logistic Regression is designed to output a **probability** ($\hat{y}$), representing the chance that the input $x$ belongs to class 1 ($P(y=1|x)$), bounded strictly between 0 and 1.

---

## 2. Technical Deep Dive

### The Computation Graph
The goal is to map an input feature vector $x \in \mathbb{R}^{n_x}$ to a prediction $\hat{y}$. This is done in two distinct steps:

1.  **Linear Transformation ($z$):**
    First, we compute a linear combination of the inputs using weights $w$ and bias $b$.
    $$z = w^T x + b$$
    * $w \in \mathbb{R}^{n_x}$: The weight vector (defines the orientation of the decision boundary).
    * $b \in \mathbb{R}$: The bias term (shifts the decision boundary).
    * *Note:* If we stopped here, this would be Linear Regression. The value $z$ could be $1,000,000$ or $-500$, which is invalid for probability.

2.  **Activation Function (Sigmoid):**
    To force the output into the valid probability range $[0, 1]$, we apply the **Sigmoid Function** $\sigma(z)$ .
    $$\hat{y} = \sigma(z) = \frac{1}{1 + e^{-z}}$$

### Mathematical Behavior
The Sigmoid function acts as a "squashing" function:
* **If $z$ is large positive ($z \to \infty$):** $e^{-z} \approx 0 \Rightarrow \sigma(z) \approx \frac{1}{1+0} = 1$.
* **If $z$ is large negative ($z \to -\infty$):** $e^{-z}$ becomes huge $\Rightarrow \sigma(z) \approx \frac{1}{\text{huge}} \approx 0$.
* **If $z = 0$:** $e^{0} = 1 \Rightarrow \sigma(0) = \frac{1}{1+1} = 0.5$.

### Notation Alert: The "Theta" Convention
In traditional statistics or other ML courses, you might see a notation where $x_0 = 1$ is added to the input, allowing $b$ to be folded into the weights vector $\theta$.
* **Theta Notation:** $\theta^T x$ (where $\theta$ contains both $w$ and $b$).
* **Deep Learning Notation:** We **explicitly avoid** this. We keep $w$ and $b$ separate.
    * **Why?** In deep neural networks, we often treat $w$ and $b$ differently (e.g., regularization usually applies only to $w$, not $b$). Separating them simplifies the implementation of complex architectures later.

---

## 3. "In Plain English"

### The Analogy: The Bouncer and the ID Scanner
Imagine a nightclub (the classifier) trying to decide if a person is over 21 (Class 1) or under 21 (Class 0).

* **The Linear Step ($z$):** This is the raw calculation. The bouncer looks at your height, beard, and confidence. He assigns you a "Maturity Score." This score could be anything: +500 (very mature), -200 (very immature).
* **The Problem:** You can't be "500% over 21."
* **The Sigmoid Step ($\sigma$):** This is the ID scanner. It takes that raw "Maturity Score" and translates it into a strict **Probability**.
    * A score of +500 becomes **99.9%** probability.
    * A score of -200 becomes **0.01%** probability.
    * A score of 0 (unsure) becomes **50%** probability.

---

## 4. Expert Nuance

### The "Saturating Gradient" Problem
While the Sigmoid is great for the final output of a binary classifier, it is dangerous to use in the *hidden layers* of a deep network.
* **The Issue:** Look at the "flat" parts of the Sigmoid curve (where $z$ is very large or very small) .
* **Consequence:** In these flat regions, the slope (gradient) is almost zero. During training (Backpropagation), if a neuron gets stuck in this "saturated" region, the gradient becomes zero, and the weights stop updating. The neuron effectively "dies." This is known as the **Vanishing Gradient Problem**.

---

