---
# My Diary

Today  **Tuesday, 30 Dec 2025**.

## General Counter
It has been    **135** days since I started this diary.

# Repository Counter

Day **7** From I started this repository



# Deep Learning Implementation: The Forward & Backward Pipeline

## 1. Executive Summary
To implement a Deep Neural Network, we chain together small, reusable functions.
* **Forward Prop:** Moves from Input $X \rightarrow$ Output $\hat{y}$. At each step, we calculate $Z$ and $A$, and **cache** them to use later.
* **Backward Prop:** Moves from Loss $L \rightarrow$ Input. We use the **cached** values to calculate how much each weight contributed to the error (Gradients).

## 2. Technical Deep Dive

### Phase 1: Forward Propagation (Layer $l$)
* **Input:** $A^{[l-1]}$ (Activation from previous layer).
* **Process:**
    1.  Linear: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$
    2.  Activation: $A^{[l]} = g^{[l]}(Z^{[l]})$
* **Output:** $A^{[l]}$ and `cache` (stores $Z^{[l]}, W^{[l]}, b^{[l]}$).
* **Initialization:** The input to the first layer $A^{[0]}$ is the data $X$.

### Phase 2: Backward Propagation (Layer $l$)
* **Input:** $dA^{[l]}$ (Error from next layer) and `cache`.
* **Process:**
    1.  $dZ^{[l]} = dA^{[l]} * g'^{[l]}(Z^{[l]})$  *(Element-wise)*
    2.  $dW^{[l]} = \frac{1}{m} dZ^{[l]} A^{[l-1]T}$
    3.  $db^{[l]} = \frac{1}{m} \text{np.sum}(dZ^{[l]}, \text{axis}=1, \text{keepdims=True})$
    4.  $dA^{[l-1]} = W^{[l]T} dZ^{[l]}$  *(Error to pass back)*
* **Initialization:** For the final layer (Output), the initial error $dA^{[L]}$ is derived from the Loss Function:
    $$dA^{[L]} = - \frac{y}{a} + \frac{1-y}{1-a}$$
   .

## 3. The Full Architecture

### The Pipeline Visualization
1.  **Input:** $X$
2.  **Layer 1 (ReLU):** Forward $\rightarrow$ Cache
3.  **Layer 2 (ReLU):** Forward $\rightarrow$ Cache
4.  **Layer 3 (Sigmoid):** Forward $\rightarrow$ Loss $L(\hat{y}, y)$
5.  **Layer 3 (Back):** Use Loss to find $dW^{[3]}, db^{[3]}$ $\rightarrow$ Pass error back.
6.  **Layer 2 (Back):** Use error to find $dW^{[2]}, db^{[2]}$ $\rightarrow$ Pass error back.
7.  **Layer 1 (Back):** Use error to find $dW^{[1]}, db^{[1]}$.
8.  **Update:** $W = W - \alpha dW$.

## 4. Implementation Code

This pseudo-code reflects the exact logic described in the transcript for the Backward Step.

```python
import numpy as np

def linear_activation_backward(dA, cache, activation):
    """
    Implements the backward propagation for a single layer.
    
    Arguments:
    dA -- post-activation gradient for current layer l
    cache -- tuple of values (linear_cache, activation_cache) stored during forward pass
    activation -- "relu" or "sigmoid"
    """
    linear_cache, activation_cache = cache
    A_prev, W, b = linear_cache
    Z = activation_cache
    m = A_prev.shape[1]
    
    # 1. Calculate dZ (Derivative of activation function)
    if activation == "relu":
        # ReLU derivative: 1 if Z>0, else 0
        dZ = np.array(dA, copy=True)
        dZ[Z <= 0] = 0
        
    elif activation == "sigmoid":
        # Sigmoid derivative: s * (1-s)
        s = 1 / (1 + np.exp(-Z))
        dZ = dA * s * (1 - s)
        
    # 2. Calculate Gradients (Linear Backward)
    dW = (1 / m) * np.dot(dZ, A_prev.T)
    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)
    dA_prev = np.dot(W.T, dZ) # The gradient to pass to the previous layer
    
    return dA_prev, dW, db
