
---
# My Diary

Today  **Tuesday, 30 Dec 2025**.

## General Counter
It has been    **135** days since I started this diary.

# Repository Counter

Day **7** From I started this repository


# Deep Learning Fundamentals: Hyperparameters

## 1. Executive Summary
In Deep Learning, there are two types of variables:
1.  **Parameters ($W, b$):** The internal numbers the model learns *by itself* through gradient descent.
2.  **Hyperparameters:** The external "settings" *you* must choose before training starts. These control *how* the model learns.

## 2. Technical Deep Dive

### The List of Hyperparameters
You cannot train a network without deciding these values first:
* **Learning Rate ($\alpha$):** The size of the step gradient descent takes. (Most critical).
* **Number of Iterations:** How long the loop runs.
* **Number of Hidden Layers ($L$):** The depth of the network.
* **Number of Hidden Units ($n^{[l]}$):** The width of each layer.
* **Activation Functions:** Choice of ReLU, Tanh, Sigmoid, etc..
* *(Future concepts: Momentum, Mini-batch size, Regularization)*.

### The "Empirical" Process
Deep Learning is an **empirical** (trial-and-error) science. It is nearly impossible to mathematically calculate the "perfect" learning rate in advance.
* **The Loop:** **Idea** ("Maybe $\alpha=0.01$?") $\rightarrow$ **Code** (Implement it) $\rightarrow$ **Experiment** (Run it and check Cost $J$).
* **The Adjustment:** If the cost oscillates, your rate is too high. If it's too slow, your rate is too low. You adjust and repeat.

## 3. "In Plain English"

### The "Oven" Analogy
Baking a cake is like training a model.
* **Parameters ($W, b$):** The chemical changes happening inside the batter (flour turning into cake). You don't control this directly; it just happens.
* **Hyperparameters:** The **Temperature** of the oven (350°F) and the **Baking Time** (30 mins). You *do* control these.
* **The Outcome:** If you set the temperature (learning rate) too high, the cake burns (diverges). If too low, it never cooks (slow convergence). You have to guess, check, and adjust the knobs for the next cake.

## 4. Expert Nuance

### Domain Transfer is Hard
Intuition often fails when moving between fields. A hyperparameter setting that works for **Computer Vision** (Images) might completely fail for **NLP** (Text) or **Advertising** (Structured Data).
* **Advice:** Do not assume. Always test a range of values (e.g., $0.001, 0.01, 0.1$) whenever you start a new problem.

## 5. Implementation Code

This snippet shows exactly where Hyperparameters live in your code—usually at the very top as constants.

```python
# --- HYPERPARAMETERS (YOU SET THESE) ---
LEARNING_RATE = 0.0075  # Alpha
NUM_ITERATIONS = 2500   # Loop count
LAYERS_DIMS = [12288, 20, 7, 5, 1] # Network Architecture (L & n)
ACTIVATION = "relu"     # Choice of function

def train_model(X, Y):
    # --- PARAMETERS (THE MODEL LEARNS THESE) ---
    # We only initialize them; the machine updates them.
    parameters = initialize_parameters(LAYERS_DIMS)
    
    for i in range(0, NUM_ITERATIONS):
        # Forward Prop
        # Cost Calculation
        # Backward Prop
        
        # Update Parameters using Hyperparameter Alpha
        parameters = update_parameters(parameters, grads, LEARNING_RATE)
        
    return parameters
