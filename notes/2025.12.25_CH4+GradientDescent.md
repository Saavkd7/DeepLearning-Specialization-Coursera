Today Thursday, 25 Dec 2025.

General Counter
It has been 130 days since I started this diary.

Repository Counter
Day 2 From I started this repository


# Deep Learning Fundamentals: Gradient Descent

## 1. Executive Summary
**Gradient Descent** is an iterative optimization algorithm used to find the specific values of $w$ and $b$ that minimize the Cost Function $J(w, b)$. Because the Logistic Regression cost function is **convex** (shaped like a perfect bowl), Gradient Descent is guaranteed to converge to the global minimum (the best possible solution) if run long enough.

---

## 2. Technical Deep Dive

### The Goal
We want to find $\min_{w, b} J(w, b)$.
We start at a random point (or zeros) and take steps proportional to the negative of the gradient (slope) to move "downhill".

### The Algorithm
Repeat the following steps until the parameters stop changing (convergence):

1.  **Update Weights ($w$):**
    $$w := w - \alpha \frac{\partial J(w,b)}{\partial w}$$
2.  **Update Bias ($b$):**
    $$b := b - \alpha \frac{\partial J(w,b)}{\partial b}$$

### Key Components:
* **$\alpha$ (Alpha):** The **Learning Rate**. It controls the "step size."
    * If $\alpha$ is too large: You might overshoot the minimum and diverge.
    * If $\alpha$ is too small: Training will take forever.
* **Derivative ($\frac{\partial J}{\partial w}$):** This represents the **slope** of the function at your current location. It tells you which direction is "down."
    * **Positive Slope:** The curve goes up to the right. We subtract ($\alpha \times \text{positive}$), moving left (downhill).
    * **Negative Slope:** The curve goes up to the left. We subtract ($\alpha \times \text{negative}$), effectively adding, moving right (downhill).

### Notation Check: $d$ vs. $\partial$
The transcript highlights a common point of confusion:
* **$d$ (Regular Derivative):** Used when the function has only **one** variable (e.g., $J(w)$).
* **$\partial$ (Partial Derivative):** Used when the function has **two or more** variables (e.g., $J(w, b)$).
* **Practical Meaning:** For coding purposes, they mean the exact same thing: **The Slope**. In your code, you will simply use the variable names `dw` and `db` to represent these values.

---

## 3. "In Plain English"

### The "Misty Mountain" Analogy
Imagine you are standing on top of a mountain range at night (the Cost Function surface) and there is thick fog. You cannot see the bottom valley (the Global Minimum), but you can feel the slope of the ground under your feet.

* **The Derivative:** You feel the ground with your foot to calculate which way tilts downward.
* **The Learning Rate ($\alpha$):** This is how long your legs are.
    * If you take tiny baby steps, you will eventually reach the bottom, but it might take years.
    * If you take massive leaps, you might accidentally jump *over* the valley and land on the uphill slope of the next mountain (Overshooting).
* **Gradient Descent:** You repeatedly feel the slope and take a step downhill. Eventually, you reach the point where the ground is flat (slope = 0). You have arrived.

---

## 4. Expert Nuance

### Convexity is a Luxury
The transcript emphasizes that Logistic Regression has a **Convex** cost function. This means there is only one "valley," so you can never get stuck in a "fake" minimum.

**The Catch:** Deep Neural Networks (which you will learn later) are **Non-Convex**. They have many "local minima" (shallow valleys that aren't the bottom). While simple Gradient Descent works perfectly here, later we will need advanced optimizers (like Adam or RMSprop) to navigate those complex landscapes.

---

