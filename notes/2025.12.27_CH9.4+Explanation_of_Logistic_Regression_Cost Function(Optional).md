# Deep Learning Theory: Deriving the Cost Function

## 1. Executive Summary
The "Log Loss" function is mathematically derived from probability theory.
* **The Goal:** We want to find parameters ($w, b$) that maximize the probability (likelihood) of the labels in our training set.
* **The Result:** Maximizing this probability is mathematically equivalent to minimizing the Log Loss function.

---

## 2. Technical Derivation

### Step 1: Interpreting the Output
We interpret the output $\hat{y}$ as the probability that the label is 1.
* If $y=1$: $P(y|x) = \hat{y}$
* If $y=0$: $P(y|x) = 1 - \hat{y}$

### Step 2: The Combined Formula
We can merge these two distinct cases into a single mathematical equation using exponents:
$$P(y|x) = \hat{y}^y (1 - \hat{y})^{(1-y)}$$
* **Check the Math:**
    * If $y=1$: $\hat{y}^1 (1-\hat{y})^0 = \hat{y}$ (Correct).
    * If $y=0$: $\hat{y}^0 (1-\hat{y})^1 = 1 - \hat{y}$ (Correct).

### Step 3: From Probability to Loss (The Log Trick)
We want to **maximize** this probability $P(y|x)$.
Because logarithms are **strictly monotonically increasing** functions, maximizing a function $P$ is the same as maximizing $\log(P)$.
$$\log P(y|x) = \log \left( \hat{y}^y (1 - \hat{y})^{(1-y)} \right)$$
Using the log rule $\log(a^b) = b \log(a)$, this simplifies to:
$$y \log(\hat{y}) + (1-y) \log(1 - \hat{y})$$
* **Result:** Since we want to *minimize* error (Loss) rather than *maximize* probability, we just add a negative sign:
$$L(\hat{y}, y) = -(y \log \hat{y} + (1-y) \log (1-\hat{y}))$$

### Step 4: From Single Loss to Global Cost ($J$)
If we assume all training examples are generated independently (**I.I.D. assumption**), the total probability of the dataset is the **product** of the individual probabilities.
$$P(\text{labels}) = \prod_{i=1}^{m} P(y^{(i)}|x^{(i)})$$
Taking the log turns this massive **Product** into a **Sum**:
$$\log P(\text{labels}) = \sum_{i=1}^{m} \log P(y^{(i)}|x^{(i)}) = -\sum_{i=1}^{m} L(\hat{y}^{(i)}, y^{(i)})$$
* **Final Cost Function:** To turn this sum into an average (for scale invariance), we divide by $m$ and minimize the negative:
$$J(w, b) = -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)} \log \hat{y}^{(i)} + (1-y^{(i)}) \log (1-\hat{y}^{(i)})]$$

---

## 3. "In Plain English"

### The "Product" Problem
Imagine you have 100 coin flips. The probability of getting that specific sequence is a tiny number (e.g., $0.5 \times 0.5 \times \dots$).
* If you multiply 100 probabilities (numbers between 0 and 1), the result becomes so small (e.g., $0.00000000...1$) that the computer loses precision (Numerical Underflow).
* **The Log Solution:** Logs turn multiplication into addition.
    * $\log(0.5 \times 0.5) = \log(0.5) + \log(0.5)$.
    * Adding negative numbers (like -0.69 + -0.69) is computationally safe and stable.
    * This is why "Log Likelihood" is the standard tool in statistics.

---

## 4. Expert Nuance

### Maximum Likelihood Estimation (MLE)
This derivation proves that Logistic Regression is an **MLE** method.
* **MLE Principle:** "Choose the parameters that make the data we observed the most probable outcome."
* By minimizing Cross-Entropy Loss ($J$), you are strictly performing MLE under the assumption that the data follows a Bernoulli distribution.
