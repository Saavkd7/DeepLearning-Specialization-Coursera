---
# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository

# Deep Learning Fundamentals: The Computation Graph

## 1. Executive Summary
The **Computation Graph** is a diagrammatic representation of mathematical operations. It breaks down complex functions (like a neural network's cost function) into a sequence of simple, step-by-step operations (nodes) connected by edges. This structure explains why deep learning is organized into two phases: a **Forward Pass** (Left-to-Right) to calculate the output, and a **Backward Pass** (Right-to-Left) to calculate derivatives.

---

## 2. Technical Deep Dive

### The Scenario
We want to compute a function $J$ composed of three variables $a, b, c$:
$$J(a, b, c) = 3(a + bc)$$
Let's use concrete values: $a=5$, $b=3$, $c=2$.

### The Forward Pass (Left-to-Right)
The graph decomposes the function into three atomic steps:

1.  **Step 1 (Multiplication):** Compute the inner term $bc$.
    * $u = b \times c = 3 \times 2 = 6$
2.  **Step 2 (Addition):** Add $a$ to the result.
    * $v = a + u = 5 + 6 = 11$
3.  **Step 3 (Final Scale):** Multiply by the constant $3$.
    * $J = 3 \times v = 3 \times 11 = 33$

### The Backward Pass (Right-to-Left)
While the transcript introduces this concept for the next video, the diagram shows red arrows flowing backwards. This represents **Backpropagation**.
* The goal is to find the derivative of the final output $J$ with respect to the inputs (e.g., $\frac{dJ}{da}$).
* By traversing the graph backwards, we can use the **Chain Rule** to compute these derivatives step-by-step without solving complex calculus all at once.

---

## 3. "In Plain English"

### The Assembly Line Analogy
Imagine a car factory.
* **Forward Pass (Blue Arrows):** You start with raw materials (steel, rubber, glass).
    * Station 1: Weld steel into a frame ($u$).
    * Station 2: Attach the engine to the frame ($v$).
    * Station 3: Paint the car ($J$).
    * *Result:* A finished car (Output 33).
* **Backward Pass (Red Arrows):** The Quality Control inspector finds a defect in the final paint job.
    * He walks backward from Station 3 to Station 2, then to Station 1, to find out exactly which raw material or process caused the error.
    * In Deep Learning, this "defect" is the Error (Cost), and the "walk back" is calculating how much each parameter contributed to that error.

---

## 4. Expert Nuance

### Why Graphs Matter for Frameworks
You might wonder, "Why draw boxes for simple math?"
Modern frameworks like **TensorFlow** and **PyTorch** literally build these graphs in memory.
* **Static Graphs (TensorFlow 1.x):** You had to define the entire graph of boxes and arrows *before* running any data through it. It was fast but rigid.
* **Dynamic Graphs (PyTorch):** The graph is built on-the-fly as you write code (e.g., when you type `u = b * c`). This makes debugging much easier because you can print the value of `u` instantly.

---
