---
# My Diary

Today  **Tuesday, 30 Dec 2025**.

## General Counter
It has been    **135** days since I started this diary.

# Repository Counter

Day **7** From I started this repository

# Deep Learning Fundamentals: Matrix Dimensions & Debugging

## 1. Executive Summary
The most effective debugging tool for Deep Neural Networks is a **"Paper and Pencil Check"**.
When implementing a network, mismatches in matrix dimensions are the most common source of errors. To fix this, you should manually write down the shape of every matrix ($W, b, Z, A, X$) based on your network architecture ($n^{[l]}$) and batch size ($m$). If the dimensions don't align in your equations, the code will fail.

## 2. Technical Deep Dive

### 1. Notation & Setup
* **$L$:** Total number of layers (excluding input).
* **$n^{[l]}$:** Number of units (neurons) in layer $l$.
* **$m$:** Number of training examples.
* **Example Architecture:** $n^{[0]}=2, n^{[1]}=3, n^{[2]}=5, n^{[3]}=4, n^{[4]}=2, n^{[5]}=1$.

### 2. Parameter Dimensions (Fixed)
These dimensions depend **only** on the network structure, not the data size.
* **Weights $W^{[l]}$:**
    * **Formula:** $(n^{[l]}, n^{[l-1]})$
    * *Logic:* It maps inputs from the previous layer ($n^{[l-1]}$ columns) to the current layer ($n^{[l]}$ rows).
* **Bias $b^{[l]}$:**
    * **Formula:** $(n^{[l]}, 1)$
    * *Logic:* One bias term per neuron in the current layer.
* **Gradients ($dW, db$):** Always match the shape of their corresponding parameter ($W, b$).

### 3. Data Dimensions (Vectorized)
These dimensions change based on the batch size $m$.
* **Input $X$:** $(n^{[0]}, m)$
* **Activations $Z^{[l]}, A^{[l]}$:**
    * **Formula:** $(n^{[l]}, m)$
    * *Logic:* We compute one activation value per neuron ($n^{[l]}$) for every single example ($m$).

## 3. "In Plain English" (The Logic Check)

Let's verify the core Forward Propagation equation:
$$Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$$

**Dimensional Analysis:**
$$
\underbrace{(n^{[l]}, m)}_{Z} = \underbrace{(n^{[l]}, n^{[l-1]})}_{W} \cdot \underbrace{(n^{[l-1]}, m)}_{A_{prev}} + \underbrace{(n^{[l]}, 1)}_{b}
$$
1.  **Dot Product:** The inner dimensions ($n^{[l-1]}$) cancel out. Result is $(n^{[l]}, m)$.
2.  **Broadcasting:** The bias $(n^{[l]}, 1)$ is automatically copied $m$ times by Python to match the size of $Z$.
3.  **Result:** The output is $(n^{[l]}, m)$, which fits perfectly.

## 4. Expert Nuance

### Broadcasting's Role
In standard math, you cannot add a matrix of size $(3, 1)$ to a matrix of size $(3, 100)$.
In Python/NumPy, this works via **Broadcasting**. The $(3, 1)$ bias vector is implicitly duplicated 100 times to create a $(3, 100)$ matrix, allowing the element-wise addition to proceed without errors. This is convenient but can hide bugs if you aren't careful about checking your dimensions manually.

## 5. Implementation Code: The Dimension Debugger

Use this script to print the expected shapes for any network architecture you design.

```python
import numpy as np

def print_network_dimensions(layer_dims, m):
    """
    Prints the matrix dimensions for W, b, Z, and A.
    
    Arguments:
    layer_dims -- list containing [n_input, n_hidden1, ..., n_output]
    m -- number of examples (batch size)
    """
    L = len(layer_dims) - 1  # Number of layers
    
    print(f"--- NETWORK ARCHITECTURE (L={L}, m={m}) ---")
    print(f"Nodes per layer: {layer_dims}\n")
    
    for l in range(1, L + 1):
        # Current and Previous layer sizes
        n_curr = layer_dims[l]
        n_prev = layer_dims[l-1]
        
        # 1. Parameter Shapes (Static)
        shape_W = (n_curr, n_prev)
        shape_b = (n_curr, 1)
        
        # 2. Data Shapes (Dynamic)
        shape_Z = (n_curr, m)
        
        print(f"Layer {l}:")
        print(f"  W[{l}]: {shape_W} \t(Maps {n_prev} inputs -> {n_curr} neurons)")
        print(f"  b[{l}]: {shape_b} \t(1 bias per neuron)")
        print(f"  Z[{l}]: {shape_Z} \t(Activations for {m} examples)")
        print("-" * 50)

if __name__ == "__main__":
    # Example from Transcript: 
    # Input(2) -> Layer1(3) -> Layer2(5) -> Layer3(4) -> Layer4(2) -> Output(1)
    layers = [2, 3, 5, 4, 2, 1]
    batch_size = 200
    
    print_network_dimensions(layers, batch_size)
