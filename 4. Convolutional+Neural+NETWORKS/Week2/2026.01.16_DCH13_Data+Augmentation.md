# My Diary

Today  **Saturday, 17 Jan 2026**.

## General Counter
It has been    **153** days since I started this diary.

# Repository Counter

Day **25** From I started this repository



# Practical Advice: Data Augmentation

## 1. Why Data Augmentation?
In computer vision, the rule of thumb is almost always: **"More data is better."** However, collecting labeled data is expensive.
**Data Augmentation** allows you to create "new" training examples from your existing ones by applying realistic distortions. This teaches the network that a cat is still a cat, whether it's flipped, shifted, or slightly discolored.

## 2. Common Augmentation Techniques

### A. Mirroring & Random Cropping
* **Mirroring:** Flipping the image horizontally.
    * *Intuition:* A cat facing left is the same object as a cat facing right.
* **Random Cropping:** Taking random slices (crops) of the image.
    * *Intuition:* Helps the network recognize the object even if the framing changes.


### B. Color Shifting
* **Concept:** Adding random values to the R, G, and B channels.
* **Example:** $(R, G, B) + (20, -20, 20)$ makes the image slightly purple.
* **Intuition:** Simulates different lighting conditions (e.g., yellow sunlight vs. indoor lighting). The object identity ($y$) remains unchanged despite color shifts.
* **Advanced:** **PCA Color Augmentation** (used in AlexNet) intelligently shifts colors based on the principal components of the image's pixel distribution.


## 3. Implementation Details
Data augmentation is usually performed **on the fly** during training to save disk space.
1.  **Storage:** Store original images on the Hard Disk.
2.  **CPU Thread:** Loads an image and applies random distortions (Crop, Flip, Tint) to create a mini-batch.
3.  **GPU Thread:** Receives the distorted mini-batch and trains the network.
* *Note:* The CPU and GPU run in parallel to maximize efficiency.


## 4. Relation to Earlier Concepts
* **Regularization:** Data Augmentation acts as a powerful regularizer. By constantly showing the network slightly different versions of the data, it prevents the network from "memorizing" specific pixel values (overfitting), similar to Dropout.
* **Invariance:** It explicitly trains the network to have **Invariance** to translation (cropping), orientation (mirroring/rotation), and illumination (color shifting).

## 5. Basic Chunk Implementation (PyTorch Transforms)
We use `torchvision.transforms` to chain these augmentations together.

```python
import torchvision.transforms as transforms

def get_training_transforms():
    # Define a pipeline of augmentations
    aug_pipeline = transforms.Compose([
        transforms.RandomHorizontalFlip(p=0.5),      # Mirroring
        transforms.RandomResizedCrop(size=(224, 224)), # Random Cropping
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # Color Shifting
        transforms.ToTensor()
    ])
    return aug_pipeline

# Usage: dataset = MyDataset(..., transform=get_training_transforms())
