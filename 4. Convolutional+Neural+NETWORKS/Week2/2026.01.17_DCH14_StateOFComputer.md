# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository



# The State of Computer Vision

## 1. The Data Spectrum
Machine learning problems fall on a spectrum based on the amount of available data relative to the problem's complexity.
* **Lots of Data:** (e.g., Speech Recognition, Image Classification). Algorithms can be simpler; the neural network learns features directly from data.
* **Little Data:** (e.g., Object Detection, Medical Imaging). Obtaining labeled data (bounding boxes) is expensive.
* **Implication:** When data is scarce, **Hand-Engineering** (carefully designing architectures and features) becomes crucial. Since Computer Vision often struggles with having "enough" data for its complexity, it historically relies heavily on complex, hand-engineered architectures (like Inception or ResNet).

## 2. Two Sources of Knowledge
A learning algorithm acquires knowledge from two places:
1.  **Labeled Data:** $(x, y)$ pairs.
2.  **Hand-Engineering:** Insight injected by the researcher (network architecture, hyperparameters, feature design).
* *Rule of Thumb:* The less data you have, the more you must rely on hand-engineering (architecture design). The more data you have, the more you can rely on the algorithm learning on its own.



## 3. Benchmark vs. Production Strategies
Researchers often use techniques to win competitions (benchmarks) that are too slow for real-world production systems.

### A. Ensembling
* **Method:** Train several networks independently (e.g., 3, 5, or 7) and **average their outputs** ($\hat{y}$).
* **Note:** Do **not** average their weights; average their predictions.
* **Pros:** Usually boosts performance by 1-2%.
* **Cons:** Increases memory and compute by a factor of $N$. Rarely used in production unless budget is unlimited.

### B. Multi-Crop at Test Time (e.g., 10-Crop)
* **Method:** Instead of running the classifier on just the single test image, you generate multiple versions:
    1.  **Central Crop**
    2.  **4 Corner Crops** (Top-Left, Top-Right, etc.)
    3.  **Mirrored Versions** of the above 5.
    * **Total:** 10 images.
* **Action:** Run all 10 through the network and average the results.
* **Cons:** Slows down inference by 10x.



## 4. Practical Advice for You
Because Computer Vision often sits in the "High Complexity / Small Data" regime:
1.  **Use Open Source:** Don't reinvent the wheel. Use architectures that others have spent months hand-engineering.
2.  **Use Pre-trained Models:** Transfer learning effectively gives you "free" knowledge, compensating for small datasets.
3.  **Stick to Single Models for Production:** Avoid ensembling or heavy multi-cropping if latency matters.
