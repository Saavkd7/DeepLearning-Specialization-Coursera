# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository

# CNN Case Studies: EfficientNet

## 1. The Scaling Problem
When deploying a neural network to different devices (e.g., a high-end smartphone vs. a budget IoT device), you often need to adjust the model size to fit the computational budget.
* **The Challenge:** How do you scale the network?
    * Should you make it deeper (more layers)?
    * Should you make it wider (more channels)?
    * Should you use higher resolution images?

## 2. Three Dimensions of Scaling
EfficientNet proposes that there are three "knobs" you can turn:

1.  **Resolution ($r$):** Using higher resolution input images (e.g., $224 \to 400$).
    * *Pros:* More fine-grained details.
    * *Cons:* Compute grows quadratically.
2.  **Depth ($d$):** Adding more layers.
    * *Pros:* Captures more complex features.
    * *Cons:* Harder to train (vanishing gradients), diminishing returns.
3.  **Width ($w$):** Adding more channels (filters) per layer.
    * *Pros:* Captures more fine-grained features, easier to parallelize.
    * *Cons:* Can fail to capture higher-level features if depth is low.


## 3. Compound Scaling (The Solution)
Instead of randomly tuning $r, d, w$, EfficientNet uses **Compound Scaling**.
* **Observation:** These dimensions are coupled. If you have higher resolution, you need more depth (receptive field) to see the whole object, and more width to capture the fine patterns.
* **Method:** Scale all three dimensions uniformly using a compound coefficient $\phi$.
    * If you have $2 \times$ more compute, scale $r, d, w$ by specific constants ($\alpha, \beta, \gamma$) derived from a small grid search.
    * This systematic approach yields much better accuracy for the same FLOPs compared to manual scaling.

## 4. Relation to Earlier Concepts
* **MobileNet:** EfficientNet often uses the MobileNet inverted bottleneck block (MBConv) as its base layer type because it is already highly efficient.
* **Hyperparameter Tuning:** In Course 2, we learned about random search for hyperparameters. EfficientNet essentially automates the search for the specific "architecture hyperparameters" ($r, d, w$) in a principled way.

## 5. Basic Chunk Implementation (Loading EfficientNet)
Modern libraries like `torchvision` or `timm` provide pre-scaled EfficientNets (B0 to B7).

```python
import torchvision.models as models

def load_efficientnet(scale='b0'):
    """
    Loads a pre-scaled EfficientNet model.
    b0 is the baseline, b7 is the largest.
    """
    if scale == 'b0':
        # Baseline model
        model = models.efficientnet_b0(weights='DEFAULT')
    elif scale == 'b7':
        # Scaled up model (Resolution, Depth, Width all increased)
        model = models.efficientnet_b7(weights='DEFAULT')
    
    return model

# Compare sizes
b0 = load_efficientnet('b0')
# b7 = load_efficientnet('b7') # Requires more memory
