# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository


# Practical Advice: Transfer Learning

## 1. What is Transfer Learning?
Instead of training a Convolutional Neural Network from scratch (random initialization), you download weights from a network that someone else has already trained on a massive dataset (like **ImageNet**, which has 1000 classes and millions of images). You then use these weights as a starting point to solve your specific problem.

### The Analogy
If you want to learn to read Sanskrit, you don't start by relearning how to see shapes and lines. You use your existing visual cortex (which already knows how to see) and just learn the new symbols. Transfer learning does the same: it reuses the "visual cortex" of a powerful pre-trained network.

## 2. Implementation Strategy: "Freeze and Train"
How you use the pre-trained model depends entirely on how much data *you* have.

### Scenario A: Small Dataset (e.g., Identifying your 2 pet cats)
* **Action:**
    1.  Download a network (e.g., ResNet trained on ImageNet).
    2.  **Delete** the final Softmax layer (1000 outputs).
    3.  **Add** your own Softmax layer (e.g., 3 outputs: Tigger, Misty, Neither).
    4.  **Freeze** all early layers (make parameters non-trainable).
    5.  **Train** *only* your new Softmax layer.
* **Optimization:** Since early layers are frozen, you can **pre-compute** their activations for your dataset and save them to disk. This makes training the Softmax layer instant, as you don't need to run the deep network every epoch.

### Scenario B: Medium Dataset
* **Action:**
    1.  **Freeze** the early layers (which detect generic features like edges/curves).
    2.  **Unfreeze** the later layers (which detect complex shapes) and your new Softmax.
    3.  **Train** the unfrozen layers using Gradient Descent.

### Scenario C: Large Dataset
* **Action:**
    1.  Use the downloaded weights simply as **Initialization** (instead of random).
    2.  **Unfreeze Everything.**
    3.  Train the entire network (Fine-tuning).
* **Why?** Since you have lots of data, you can afford to update all parameters to perfectly fit your specific domain, but starting with ImageNet weights typically converges faster than random initialization.

## 3. Relation to Earlier Concepts
* **Feature Extraction:** When you freeze the early layers, the network acts as a fixed function $f(x)$ that converts raw pixels into a high-level feature vector. You are essentially running Logistic Regression (the final softmax) on these high-quality features.
* **Initialization:** In Course 2, we learned that good initialization is crucial. Transfer learning provides the *ultimate* initializationâ€”weights that already know how to "see" the world.

## 4. Basic Chunk Implementation (Freezing Layers)
Here is how you freeze layers in PyTorch to perform Transfer Learning.

```python
import torchvision.models as models
import torch.nn as nn

def setup_transfer_learning(num_classes):
    # 1. Download Pre-trained Model (e.g., ResNet18)
    model = models.resnet18(weights='DEFAULT')
    
    # 2. Freeze ALL layers
    for param in model.parameters():
        param.requires_grad = False
        
    # 3. Replace the 'head' (Final Fully Connected Layer)
    # The new layer is created with requires_grad=True by default
    num_features = model.fc.in_features
    model.fc = nn.Linear(num_features, num_classes)
    
    return model

# Example: 3 Classes (Tigger, Misty, Neither)
my_model = setup_transfer_learning(3)

# Verify: Only the last layer should be trainable
# for name, param in my_model.named_parameters():
#     print(f"{name}: {param.requires_grad}")
