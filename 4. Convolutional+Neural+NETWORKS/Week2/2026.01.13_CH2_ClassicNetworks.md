# My Diary

Today  **Saturday, 17 Jan 2026**.

## General Counter
It has been    **153** days since I started this diary.

# Repository Counter

Day **25** From I started this repository


# Classic Networks: LeNet, AlexNet, and VGG

## 1. LeNet-5 (1998)
Designed by Yann LeCun for handwritten digit recognition (MNIST), this network is small by modern standards (~60k parameters) but introduced the standard "Conv $\to$ Pool $\to$ Conv $\to$ Pool $\to$ FC" pattern.

* **Input:** $32 \times 32 \times 1$ (Grayscale).
* **Architecture:**
    1.  **Conv1:** 6 filters, $5 \times 5$, stride 1 $\to$ $28 \times 28 \times 6$.
    2.  **Pool1:** **Average Pooling** (common back then), $f=2, s=2$ $\to$ $14 \times 14 \times 6$.
    3.  **Conv2:** 16 filters, $5 \times 5$ $\to$ $10 \times 10 \times 16$.
    4.  **Pool2:** Average Pooling, $f=2, s=2$ $\to$ $5 \times 5 \times 16$.
    5.  **FC Layers:** 400 $\to$ 120 $\to$ 84 $\to$ Output (10 classes).
* **Activation:** Used Sigmoid/Tanh (ReLU wasn't standard yet).


## 2. AlexNet (2012)
Named after Alex Krizhevsky, this network kickstarted the Deep Learning revolution by winning the ImageNet competition. It is similar to LeNet but much larger (~60 million parameters) and uses modern activations.

* **Input:** $227 \times 227 \times 3$ (RGB).
* **Key Improvements:**
    * **ReLU:** Used ReLU instead of Tanh/Sigmoid for faster training.
    * **Max Pooling:** Replaced Average Pooling.
    * **Dropout:** Used in FC layers (not detailed in this video, but standard in AlexNet).
    * **GPUs:** Originally trained split across two GPUs due to hardware limits.


## 3. VGG-16 (2015)
The VGG network (Simonyan & Zisserman) is famous for its **Simplicity** and **Uniformity**. Instead of tuning filter sizes (11x11 vs 5x5), it uses the same hyperparameters everywhere.

* **Design Rules:**
    * **Convolutions:** Always $3 \times 3$ filters, stride 1, padding "Same".
    * **Pooling:** Always Max Pool $2 \times 2$, stride 2.
* **Systematic Growth:**
    * Every time pooling reduces $H \times W$ by 2, the number of filters (channels) **doubles** ($64 \to 128 \to 256 \to 512$).
* **Size:** Very deep (16 layers) and large (~138 million parameters).


## 4. Relation to Earlier Concepts
* **Evolution of Dimensions:** All three networks follow the pattern we observed earlier: as you go deeper, Height/Width decreases ($n_H, n_W \downarrow$) while Channels increase ($n_C \uparrow$).
    * *LeNet:* $1 \to 6 \to 16$ channels.
    * *VGG:* $3 \to 64 \to 128 \to 256 \to 512$ channels.
* **Building Blocks:** They all utilize the exact layer types (Conv, Pool, FC) defined in the previous section, just arranged in deeper configurations.

## 5. Basic Chunk Implementation (VGG Block)
Since VGG is modular, we can define a function to create a "block" of convolutions followed by a pool, which is repeated throughout the network.

```python
import torch.nn as nn

def vgg_block(in_channels, out_channels, num_convs):
    """
    Creates a VGG block: [Conv3x3 -> ReLU] x num_convs -> MaxPool2x2
    """
    layers = []
    for _ in range(num_convs):
        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))
        layers.append(nn.ReLU())
        in_channels = out_channels # Next conv takes this output
        
    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))
    return nn.Sequential(*layers)

# Example: The first block of VGG-16 (2 convs, 64 filters)
block1 = vgg_block(3, 64, 2)
# print(block1)
