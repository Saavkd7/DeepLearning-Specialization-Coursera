# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository

# CNN Case Studies: Inception Network Motivation

## 1. The Core Idea: "Why Choose?"
When designing a layer, you often face a dilemma: Should I use a $1 \times 1$ filter? A $3 \times 3$? A $5 \times 5$? Or maybe Max Pooling?
* **Inception's Answer:** Do them all!
* **Mechanism:** Create an **Inception Module** that applies $1 \times 1$, $3 \times 3$, $5 \times 5$ convolutions, and Max Pooling *in parallel* to the same input.
* **Output:** Concatenate (stack) all the resulting volumes together into a single output volume. This lets the network learn which features are most useful.


## 2. The Problem: Computational Cost
Naive implementation of Inception modules is incredibly expensive.
* **Example:**
    * Input: $28 \times 28 \times 192$.
    * Operation: $5 \times 5$ Convolutions (32 filters).
    * Output: $28 \times 28 \times 32$.
* **Cost Calculation:**
    * You compute $28 \times 28 \times 32$ outputs.
    * Each output requires a $5 \times 5 \times 192$ filter operation.
    * Total Operations: $28 \times 28 \times 32 \times 5 \times 5 \times 192 \approx \mathbf{120 \text{ Million}}$.
    * *Verdict:* Too slow.

## 3. The Solution: Bottleneck Layers (1x1 Convs)
We use a $1 \times 1$ convolution to shrink the number of channels *before* running the expensive $5 \times 5$ convolution. This is called a **Bottleneck Layer**.

**The Optimized Steps:**
1.  **Bottleneck:** Compress the 192 channels down to 16 using $1 \times 1$ filters.
    * Cost: $28 \times 28 \times 16 \times 1 \times 1 \times 192 \approx 2.4 \text{ Million}$.
2.  **Convolution:** Run the $5 \times 5$ convolution on the smaller volume ($28 \times 28 \times 16$).
    * Cost: $28 \times 28 \times 32 \times 5 \times 5 \times 16 \approx 10 \text{ Million}$.
3.  **Total Cost:** $2.4M + 10M = \mathbf{12.4 \text{ Million}}$.

**Result:** You reduced computation by a factor of **10** (120M $\to$ 12.4M) without hurting performance significantly.


## 4. Relation to Earlier Concepts
* **1x1 Convolutions:** In the previous section, we learned $1 \times 1$ convs can reduce channels. Here, we see the "Killer App" for that concept: using it as a computational funnel (bottleneck) to enable complex architectures.
* **Concatenation:** In standard layers, we just pass output to input. In Inception, we use concatenation (stacking along the channel axis) to combine features from different receptive fields ($1 \times 1$ vs $5 \times 5$).

## 5. Basic Chunk Implementation (The Inception Block Logic)
This snippet shows how to define the parallel branches.

```python
import torch
import torch.nn as nn

class InceptionBlockNaive(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        # Branch 1: 1x1 conv
        self.branch1 = nn.Conv2d(in_channels, 64, kernel_size=1)
        
        # Branch 2: 3x3 conv (Naive: directly on input)
        self.branch2 = nn.Conv2d(in_channels, 128, kernel_size=3, padding=1)
        
        # Branch 3: 5x5 conv (Naive: directly on input)
        self.branch3 = nn.Conv2d(in_channels, 32, kernel_size=5, padding=2)
        
        # Branch 4: Max Pool
        self.branch4 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)

    def forward(self, x):
        # Calculate all branches
        out1 = self.branch1(x)
        out2 = self.branch2(x)
        out3 = self.branch3(x)
        out4 = self.branch4(x) # Pooling usually needs 1x1 after to match channels
        
        # Concatenate along channel dimension (dim=1)
        return torch.cat([out1, out2, out3, out4], dim=1)
