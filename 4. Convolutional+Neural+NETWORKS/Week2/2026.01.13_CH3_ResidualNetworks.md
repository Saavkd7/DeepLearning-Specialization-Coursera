# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository


# CNN Case Studies: Residual Networks (ResNets)

## 1. The Problem: Vanishing Gradients in Deep Networks
In theory, making a neural network deeper should always reduce training error (or at least not hurt it). In practice, with "Plain" networks (standard stacks of layers), performance degrades as you go deeper (e.g., 20 layers $\to$ 56 layers).
* **Vanishing/Exploding Gradients:** As the signal propagates through many layers, gradients can shrink to zero or explode, making optimization difficult.


## 2. The Solution: The Residual Block
A ResNet is built by stacking **Residual Blocks**. A residual block adds a "Shortcut" (or Skip Connection) that allows information to flow directly from one layer to a deeper layer, bypassing the intermediate steps.

### The Math
* **Main Path:**
    1.  Linear: $z^{[l+1]} = W^{[l+1]}a^{[l]} + b^{[l+1]}$
    2.  ReLU: $a^{[l+1]} = g(z^{[l+1]})$
    3.  Linear: $z^{[l+2]} = W^{[l+2]}a^{[l+1]} + b^{[l+2]}$
* **Shortcut (Skip Connection):**
    * Take $a^{[l]}$ and inject it directly into the calculation of $a^{[l+2]}$.
    * **New Formula:**
        $$a^{[l+2]} = g(z^{[l+2]} + a^{[l]})$$
    * *Crucial Detail:* The addition happens **before** the final ReLU activation.


## 3. Why ResNets Work
* **Easy to learn Identity Function:** If the deeper layers are unnecessary, the network can easily set the weights $W$ to zero.
    * If $W \approx 0$ and $b \approx 0$, then $z^{[l+2]} \approx 0$.
    * The formula becomes $a^{[l+2]} = g(0 + a^{[l]}) = a^{[l]}$ (assuming ReLU and non-negative input).
    * This means the block simply copies the input forward. It's easy for the network to "ignore" layers, whereas in a plain net, learning an identity mapping is difficult.
* **Result:** You can train networks with **100+ layers** (like ResNet-152) and training error continues to decrease.

## 4. Relation to Earlier Concepts
* **Vanishing Gradients:** In Course 2, we learned that deep networks are hard to train because derivatives multiply via the chain rule, often becoming tiny. The **Skip Connection** provides a "superhighway" for the gradient to flow backward during backpropagation without being multiplied by $W$ at every single step.
* **Computational Graph:** The skip connection changes the topology of the graph from a simple chain to a chain with loops (shortcuts), altering how $da$ is computed.

## 5. Basic Chunk Implementation (Residual Block Logic)
This snippet shows the specific operation of adding the shortcut `X` to the main path `Z` before activation.

```python
import numpy as np

def residual_block_step(X_shortcut, Z_main_path):
    """
    Implements the skip connection step: a[l+2] = g(z[l+2] + a[l])
    """
    # 1. Add shortcut to main path
    Z_sum = Z_main_path + X_shortcut
    
    # 2. Apply ReLU Activation
    A_out = np.maximum(0, Z_sum)
    
    return A_out
