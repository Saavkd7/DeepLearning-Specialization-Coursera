# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository

# CNN Case Studies: MobileNet v1 and v2

## 1. MobileNet v1 Recap
* **Architecture:** Stacks **13 Depthwise Separable Convolution** layers (Depthwise + Pointwise).
* **Ending:** Average Pooling $\to$ FC $\to$ Softmax.
* **Result:** Efficient, but we can do better.


## 2. MobileNet v2: The Improvements
MobileNet v2 introduces two major changes to improve performance and efficiency:
1.  **Residual Connections:** Adds "Skip connections" just like ResNet to help gradients flow.
2.  **Inverted Residual Bottleneck Block:** A new block structure that expands and then compresses channels.


## 3. The Inverted Residual Block (The "Bottleneck" Block)
This block performs three steps: **Expansion $\to$ Depthwise $\to$ Projection**.

### Step A: Expansion ($1 \times 1$ Convolution)
* **Input:** Low-dimensional input (e.g., $n \times n \times 3$).
* **Action:** Expand channels by a factor (usually 6).
* **Output:** $n \times n \times 18$.
* **Why?** To allow the network to learn richer functions in a higher-dimensional space.

### Step B: Depthwise Convolution ($3 \times 3$)
* **Input:** $n \times n \times 18$.
* **Action:** Filter spatially (with padding to keep size same).
* **Output:** $n \times n \times 18$.
* **Why?** Efficient spatial filtering.

### Step C: Projection ($1 \times 1$ Convolution - Linear)
* **Input:** $n \times n \times 18$.
* **Action:** Project back down to low dimensions.
* **Output:** $n \times n \times 3$.
* **Why?** To save memory before passing data to the next block.

**"Inverted" Residual:** In classic ResNet, the residual block connects *high-dimensional* layers (bottleneck in the middle is narrow). In MobileNet v2, the residual connects *low-dimensional* bottlenecks (expansion in the middle is wide). Hence, "Inverted".

## 4. Relation to Earlier Concepts
* **Bottleneck vs. Expansion:** In Inception, we used $1 \times 1$ convs to *shrink* channels (Bottleneck) to save compute. In MobileNet v2, we use $1 \times 1$ convs to *expand* channels inside the block to learn features, then shrink them back down to save memory for the skip connection.
* **ResNet:** The skip connection $a^{[l]} + a^{[l+2]}$ is applied between the low-dimensional "Bottleneck" layers.

## 5. Basic Chunk Implementation (MobileNetV2 Block)
This implementation shows the Expansion -> Depthwise -> Projection flow.

```python
import torch.nn as nn

class InvertedResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride, expand_ratio):
        super().__init__()
        hidden_dim = in_channels * expand_ratio
        self.use_residual = (stride == 1 and in_channels == out_channels)

        self.conv = nn.Sequential(
            # 1. Expansion (1x1)
            nn.Conv2d(in_channels, hidden_dim, 1, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU6(inplace=True), # ReLU6 is often used in MobileNets
            
            # 2. Depthwise (3x3)
            nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU6(inplace=True),
            
            # 3. Projection (1x1) - Linear (No ReLU at end!)
            nn.Conv2d(hidden_dim, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels)
        )

    def forward(self, x):
        if self.use_residual:
            return x + self.conv(x)
        else:
            return self.conv(x)
