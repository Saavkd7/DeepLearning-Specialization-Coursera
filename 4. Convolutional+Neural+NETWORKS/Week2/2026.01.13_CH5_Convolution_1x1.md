

# CNN Basics: 1x1 Convolutions (Network in Network)

## 1. What is a 1x1 Convolution?
At first glance, a $1 \times 1$ convolution seems useless.
* **Single Channel:** If you have a $6 \times 6 \times 1$ image and convolve it with a $1 \times 1$ filter (value 2), you just multiply every pixel by 2. This is trivial.
* **Multi-Channel (The Power):** If you have a volume like $6 \times 6 \times 32$, the $1 \times 1$ filter actually has dimension $1 \times 1 \times 32$.
    * It looks at a specific pixel position $(h, w)$.
    * It takes the "slice" or "fiber" of **32 numbers** across the channels.
    * It performs an element-wise product (dot product) with the 32 weights in the filter, followed by a ReLU non-linearity.
    * **Result:** It collapses those 32 numbers into **1 single number** for that pixel position.


## 2. Analogy: "Pixel-wise" Fully Connected Layer
You can think of a $1 \times 1$ convolution as a standard **Fully Connected Neural Network** applied to every single pixel position independently.
* **Input:** A vector of size $n_C$ (e.g., 32).
* **Operation:** $Output = ReLU(W \cdot Input + b)$.
* **Output:** A vector of size $n_{filters}$.

## 3. Why is it Useful?
It serves two main purposes in deep networks:

1.  **Dimension Reduction (The Bottleneck Layer):**
    * Pooling layers shrink Height and Width ($n_H, n_W$).
    * $1 \times 1$ Convolutions shrink **Channels** ($n_C$).
    * *Example:* Reduce a $28 \times 28 \times 192$ volume to $28 \times 28 \times 32$. This drastically reduces computation for subsequent layers while retaining information.
2.  **Adding Non-Linearity:**
    * You can use it to keep dimensions the same ($192 \to 192$) but add a non-linear activation (ReLU). This allows the network to learn more complex functions without changing the spatial structure.


## 4. Relation to Earlier Concepts
* **Fully Connected Layers:** In a standard FC layer (Course 1), you flatten the whole image and mix everything. In a $1 \times 1$ Conv, you act like an FC layer *only along the depth axis*, preserving the spatial grid structure ($H \times W$).
* **Feature Combination:** Just as a standard neuron combines inputs to create a new feature, a $1 \times 1$ filter combines the existing features (channels) at a specific location to create a "super-feature."

## 5. Basic Chunk Implementation
This snippet shows how simple it is to implement this powerful concept using PyTorch.

```python
import torch.nn as nn
import torch

def one_by_one_conv_block(in_channels, out_channels):
    """
    Creates a 1x1 convolution block to reduce (or expand) channels.
    """
    # kernel_size=1 is the key here
    return nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)

# Example: Shrinking 192 channels to 32
reducer = one_by_one_conv_block(192, 32)
input_vol = torch.randn(1, 192, 28, 28) # Batch, Channels, H, W
output_vol = reducer(input_vol)

print(f"Input Shape: {input_vol.shape}")   # torch.Size([1, 192, 28, 28])
print(f"Output Shape: {output_vol.shape}") # torch.Size([1, 32, 28, 28])
