

# CNN Case Studies: The Inception Network

## 1. The Full Inception Module
In the previous section, we saw the "naive" module. The **full** Inception module includes **bottleneck layers ($1 \times 1$ convolutions)** to reduce computation before expensive operations, and a special handling for the pooling branch.

**The 4 Branches:**
1.  **Branch 1:** $1 \times 1$ Conv.
2.  **Branch 2:** $1 \times 1$ Conv (Bottleneck) $\to$ $3 \times 3$ Conv.
3.  **Branch 3:** $1 \times 1$ Conv (Bottleneck) $\to$ $5 \times 5$ Conv.
4.  **Branch 4 (Pooling):** Max Pooling (Same padding, Stride 1) $\to$ $1 \times 1$ Conv.
    * *Note on Branch 4:* Pooling usually preserves channel depth ($192 \to 192$). To prevent the pooling branch from dominating the output size, we add a $1 \times 1$ conv *after* pooling to shrink channels (e.g., to 32).

**Output:** All four branches produce output volumes with the same height/width ($28 \times 28$). We **concatenate** them along the channel dimension.
* $64 + 128 + 32 + 32 = 256$ channels total.


## 2. The Inception Network (GoogLeNet)
The full network is simply many Inception modules stacked on top of each other.
* **Side Branches (Auxiliary Classifiers):** You will notice branches sticking out from the middle of the network.
    * These take hidden layer activations and try to predict the output label (Softmax).
    * **Purpose:** They ensure that features in the middle of the network are useful for classification, providing a **regularizing effect** and preventing vanishing gradients.


## 3. Fun Fact: The Name
The name "Inception" comes from the "We Need to Go Deeper" meme (referencing the movie *Inception*), cited directly in the research paper as motivation for building deeper networks.

## 4. Relation to Earlier Concepts
* **Bottleneck Layer:** We apply the $1 \times 1$ conv concept from the previous section *three times* in this module (before 3x3, before 5x5, and after Pooling) to keep parameters low.
* **Padding:** To concatenate outputs, they must have the same Height/Width. This requires **"Same" padding** for all convs and the pooling layer.

## 5. Basic Chunk Implementation (Full Module)
Here is how you implement the specific structure of the Inception block shown in the diagram.

```python
import torch
import torch.nn as nn

class InceptionModule(nn.Module):
    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_pool):
        super().__init__()
        
        # Branch 1: 1x1 conv
        self.branch1 = nn.Sequential(
            nn.Conv2d(in_channels, out_1x1, kernel_size=1),
            nn.ReLU()
        )
        
        # Branch 2: 1x1 (reduce) -> 3x3
        self.branch2 = nn.Sequential(
            nn.Conv2d(in_channels, red_3x3, kernel_size=1),
            nn.ReLU(),
            nn.Conv2d(red_3x3, out_3x3, kernel_size=3, padding=1), # Padding=1 for SAME
            nn.ReLU()
        )
        
        # Branch 3: 1x1 (reduce) -> 5x5
        self.branch3 = nn.Sequential(
            nn.Conv2d(in_channels, red_5x5, kernel_size=1),
            nn.ReLU(),
            nn.Conv2d(red_5x5, out_5x5, kernel_size=5, padding=2), # Padding=2 for SAME
            nn.ReLU()
        )
        
        # Branch 4: MaxPool -> 1x1
        self.branch4 = nn.Sequential(
            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),
            nn.Conv2d(in_channels, out_pool, kernel_size=1),
            nn.ReLU()
        )

    def forward(self, x):
        y1 = self.branch1(x)
        y2 = self.branch2(x)
        y3 = self.branch3(x)
        y4 = self.branch4(x)
        return torch.cat([y1, y2, y3, y4], 1)
