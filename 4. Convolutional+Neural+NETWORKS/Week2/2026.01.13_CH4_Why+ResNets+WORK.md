# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository


# Why do Residual Networks Work?

## 1. The Core Intuition: The Identity Function
The main reason deep networks fail is that as they get deeper, it becomes harder for the optimization algorithm to find good parameters. Ideally, adding layers should not *hurt* performance (the deeper network should at least be able to copy the simpler network).

**ResNets make it easy to learn the Identity Function ($f(x) = x$).**
* If the extra layers are useful, the network learns them.
* If the extra layers are harmful or unnecessary, the network can easily turn them into "identity mappings" (effectively ignoring them).

## 2. The Math behind Identity Learning
Consider a Residual Block:
$$a^{[l+2]} = g(z^{[l+2]} + a^{[l]})$$
$$a^{[l+2]} = g(W^{[l+2]}a^{[l+1]} + b^{[l+2]} + a^{[l]})$$

**Scenario:**
Imagine the network uses L2 Regularization (Weight Decay), which forces weights ($W$) and biases ($b$) closer to zero.
* If $W^{[l+2]} \approx 0$ and $b^{[l+2]} \approx 0$:
    * The linear term becomes 0.
    * The equation becomes $a^{[l+2]} = g(0 + a^{[l]}) = g(a^{[l]})$.
* Since we typically use **ReLU** (and activations $a^{[l]}$ are non-negative), $g(a^{[l]}) = a^{[l]}$.
* **Result:** The block simply copies $a^{[l]}$ to $a^{[l+2]}$. This preserves the signal, preventing performance degradation.

## 3. Handling Dimensions
The addition $z^{[l+2]} + a^{[l]}$ requires both vectors to have the same shape.

* **Case 1: Same Dimensions:**
    * Use "Same" convolutions so dimensions are preserved.
* **Case 2: Different Dimensions (e.g., Pooling reduced the size):**
    * We multiply $a^{[l]}$ by a matrix $W_s$ to resize it to match $z^{[l+2]}$.
    * Alternatively, we can use zero-padding.
    * Formula: $a^{[l+2]} = g(z^{[l+2]} + W_s a^{[l]})$.


## 4. Relation to Earlier Concepts
* **L2 Regularization:** In Course 2, we learned that L2 regularization shrinks weights. In a *Plain* network, shrinking weights to zero destroys the signal (output $\to$ 0). In a *ResNet*, shrinking weights to zero reveals the identity (output $\to$ input), which is a much safer "default" state.
* **Vanishing Gradients:** The "Short Cut" acts as a gradient superhighway. During backprop, the gradient can flow through the skip connection without being multiplied by small weights $W$, ensuring earlier layers still learn.

## 5. Basic Chunk Implementation (Dimension Matching)
This snippet demonstrates how to apply the $W_s$ matrix (often implemented as a $1 \times 1$ convolution) when dimensions mismatch.

```python
import torch.nn as nn

def projection_shortcut(X_shortcut, out_channels):
    """
    If X_shortcut shape doesn't match the output shape,
    we use a 1x1 convolution (Ws) to project it.
    """
    in_channels = X_shortcut.shape[1]
    
    # Ws is represented by this 1x1 Conv layer
    # Stride can also be used if spatial dims mismatch
    projector = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)
    
    X_projected = projector(X_shortcut)
    return X_projected
