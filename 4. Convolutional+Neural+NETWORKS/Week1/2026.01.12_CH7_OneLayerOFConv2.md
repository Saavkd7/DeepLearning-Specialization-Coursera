# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository


# One Layer of a Convolutional Network

## 1. The Anatomy of a Layer
Just like a standard neural network layer takes an input $A^{[l-1]}$, computes $Z^{[l]}$, and outputs $A^{[l]}$, a Convolutional Layer does the same using volumes.

### The Steps:
1.  **Input:** A 3D volume (e.g., $6 \times 6 \times 3$).
2.  **Convolution ($Z$):** Convolve the input with multiple filters (e.g., 2 filters).
    * If you have **10 filters**, you get **10** 2D output maps.
    * Stack them to get a volume (e.g., $4 \times 4 \times 10$).
3.  **Bias ($+b$):** Add a real number (bias) to *every* element in the convolved output. Each filter has its own distinct bias.
4.  **Non-Linearity ($A$):** Apply an activation function (like **ReLU**) element-wise.
    * $A^{[l]} = g(Z^{[l]})$.


## 2. Parameter Count (Why CNNs are Efficient)
One of the "magic" features of CNNs is that the number of parameters is fixed, regardless of image size.

**Example:**
* **Filters:** 10 filters.
* **Filter Size:** $3 \times 3 \times 3$.
* **Calculation:**
    * Weights per filter: $3 \times 3 \times 3 = 27$.
    * Bias per filter: $1$.
    * Total params per filter: $28$.
    * **Total for layer:** $28 \times 10 = 280$ parameters.
* **Insight:** Whether the image is $6 \times 6$ or $5000 \times 5000$, this layer still only has **280 parameters**. This prevents overfitting.

## 3. Summary of Notation for Layer $l$
* $f^{[l]}$: Filter size (height/width).
* $p^{[l]}$: Padding.
* $s^{[l]}$: Stride.
* $n_C^{[l]}$: Number of filters (which equals the **number of channels** in the output).

**Input Dimensions:** $n_H^{[l-1]} \times n_W^{[l-1]} \times n_C^{[l-1]}$
**Output Dimensions:** $n_H^{[l]} \times n_W^{[l]} \times n_C^{[l]}$

$$n_H^{[l]} = \lfloor \frac{n_H^{[l-1]} + 2p^{[l]} - f^{[l]}}{s^{[l]}} + 1 \rfloor$$




## 4. Relation to Earlier Concepts
* **Standard NN Analogy:**
    * In a standard net: $Z = W \cdot A + b$.
    * In a CNN: The **Convolution** operation acts as the "product" ($W \cdot A$). The filter values are the Weights ($W$).
* **Broadcasting:** When adding the bias $b$ (a single real number) to a $4 \times 4$ output map, Python "broadcasts" it, meaning it adds $b$ to every one of the 16 pixels. This is identical to the broadcasting we learned in Course 1.
* **Activation:** Just like before, we pass the linear output $Z$ through a non-linear function $g$ (usually ReLU) to allow the network to learn complex patterns.

## 5. Basic Chunk Implementation
Here is the core logic for a single step of the convolution (one filter position):

```python
import numpy as np

def conv_single_step(a_slice_prev, W, b):
    """
    Apply one filter defined by parameters W on a single slice (a_slice_prev) 
    of the output activation of the previous layer.
    """
    # 1. Element-wise product between input slice and filter
    s = np.multiply(a_slice_prev, W)
    # 2. Sum over all entries
    Z = np.sum(s)
    # 3. Add bias b (cast to float)
    Z = Z + float(b)
    
    return Z
