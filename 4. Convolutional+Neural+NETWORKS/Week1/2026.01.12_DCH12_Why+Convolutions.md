# My Diary

Today  **Saturday, 17 Jan 2026**.

## General Counter
It has been    **153** days since I started this diary.

# Repository Counter

Day **25** From I started this repository

# Why Convolutions?

## 1. The Problem with Fully Connected Layers
If you tried to treat an image as a standard input vector for a classic neural network, the number of parameters would explode.

**Example:**
* **Input:** $32 \times 32 \times 3$ image ($3,072$ units).
* **Next Layer:** $28 \times 28 \times 6$ ($4,704$ units).
* **Fully Connected Parameters:**
    * Weight Matrix $W$: $3,072 \times 4,704 \approx \mathbf{14 \text{ Million}}$.
    * *Result:* Impossible to train for large images ($1000 \times 1000$ would be billions).


## 2. The Solution: Convolutional Layers
ConvNets solve this using two key ideas:

### A. Parameter Sharing
**Concept:** A feature detector (like a vertical edge detector) that is useful in one part of the image is likely useful in another part.
* Instead of learning a separate "edge detector" for the top-left corner and the bottom-right corner, we **share** the same filter weights across the entire image.
* **Conv Layer Parameters (Same Example):**
    * Filters: 6 filters of size $5 \times 5$.
    * Parameters per filter: $5 \times 5 \times 3 (\text{channels}) + 1 (\text{bias}) = 76$.
    * Total Parameters: $76 \times 6 = \mathbf{456}$.
    * *Comparison:* **14,000,000 vs 456**.

### B. Sparsity of Connections
**Concept:** In each layer, each output value depends on only a small number of inputs.
* An output pixel (e.g., top-left corner of the feature map) is connected *only* to the $5 \times 5$ region of pixels in the input image that the filter is currently touching. It is **not** connected to the pixels in the rest of the image.
* This is distinct from FC layers where every output sees every input.


## 3. Translation Invariance
Because of parameter sharing, CNNs naturally develop **Translation Invariance**.
* If the network learns to detect a "Cat" in the top-left, it can immediately detect a "Cat" in the bottom-right using the same shared filter.
* Shifting the image (translation) results in a similar shift in the feature map, but the features themselves are still detected.

## 4. Putting It Together: Training
Training a ConvNet is identical to a standard Neural Network:
1.  **Initialize** parameters $W$ (filters) and $b$.
2.  **Forward Prop** (Conv $\to$ ReLU $\to$ Pool $\to$ FC $\to$ Softmax $\to \hat{y}$).
3.  **Compute Cost** $J = \frac{1}{m} \sum \mathcal{L}(\hat{y}, y)$.
4.  **Backprop & Optimization** (Gradient Descent/Adam) to update $W$ and $b$.


## 5. Relation to Earlier Concepts
* **Matrix Multiplication:** In a standard FC layer, matrix multiplication $(W \cdot x)$ connects every input to every output. In a Conv layer, the "Convolution" operation effectively zeros out 99% of those connections (Sparsity) and forces the remaining connections to use identical values (Parameter Sharing).
* **Generalization:** Just as Regularization (L2/Dropout) prevents overfitting by constraining weights, Parameter Sharing acts as a massive constraint that forces the model to learn general features rather than memorizing specific pixel locations.

## 6. Basic Chunk Implementation (Parameter Comparison)
This snippet calculates the massive difference in parameter count discussed above.

```python
def compare_parameters(input_shape, filter_size, num_filters):
    n_H, n_W, n_C = input_shape
    f = filter_size
    
    # 1. Output Size
    out_H = n_H - f + 1
    out_W = n_W - f + 1
    output_units = out_H * out_W * num_filters
    input_units = n_H * n_W * n_C
    
    # 2. Fully Connected Params (Input units * Output units)
    fc_params = input_units * output_units
    
    # 3. Conv Params ((f*f*n_C + 1) * filters)
    conv_params = (f * f * n_C + 1) * num_filters
    
    return fc_params, conv_params

# Example from transcript: 32x32x3 Image, 5x5 filters (6 of them)
fc, conv = compare_parameters((32,32,3), 5, 6)
print(f"FC Parameters:   {fc:,}")   # ~14 Million
print(f"Conv Parameters: {conv:,}") # 456
