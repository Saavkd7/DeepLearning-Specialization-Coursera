# My Diary

Today  **Saturday, 17 Jan 2026**.

## General Counter
It has been    **153** days since I started this diary.

# Repository Counter

Day **25** From I started this repository


# Simple Convolutional Network Example

## 1. Building a Deep ConvNet
A typical Convolutional Neural Network (ConvNet) involves passing an image through a sequence of layers where the spatial dimensions (Height/Width) shrink while the depth (Channels) increases.

### The Example Architecture
Let's trace a $39 \times 39 \times 3$ input image through three convolutional layers.

**Step 0: Input**
* $n_H^{[0]} = n_W^{[0]} = 39$
* $n_C^{[0]} = 3$ (RGB)

**Step 1: CONV Layer 1**
* **Hyperparameters:** $f=3$, $s=1$, $p=0$ (Valid), 10 Filters.
* **Math:** $\lfloor \frac{39 + 0 - 3}{1} + 1 \rfloor = 37$.
* **Output:** $37 \times 37 \times 10$ volume.
* *Note:* The depth is 10 because we used 10 filters.

**Step 2: CONV Layer 2**
* **Hyperparameters:** $f=5$, $s=2$, $p=0$, 20 Filters.
* **Math:** $\lfloor \frac{37 - 5}{2} + 1 \rfloor = \lfloor \frac{32}{2} + 1 \rfloor = 17$.
* **Output:** $17 \times 17 \times 20$ volume.
* *Observation:* The dimensions shrunk significantly (from 37 to 17) because of **Stride = 2**.

**Step 3: CONV Layer 3**
* **Hyperparameters:** $f=5$, $s=2$, $p=0$, 40 Filters.
* **Math:** $\lfloor \frac{17 - 5}{2} + 1 \rfloor = \lfloor \frac{12}{2} + 1 \rfloor = 7$.
* **Output:** $7 \times 7 \times 40$ volume.

**Step 4: Flatten & Classification**
* Take the final volume ($7 \times 7 \times 40$) and **unroll** (flatten) it into a single vector.
* $7 \times 7 \times 40 = 1,960$ units.
* Feed this vector into a Logistic Regression or Softmax unit for the final prediction ($\hat{y}$).


## 2. General Trends in ConvNets
As you go deeper into the network, a standard pattern emerges:
1.  **Height ($n_H$) and Width ($n_W$) Decrease:** The image gets smaller spatially (e.g., $39 \to 37 \to 17 \to 7$).
2.  **Channels ($n_C$) Increase:** The number of filters (features) grows (e.g., $3 \to 10 \to 20 \to 40$).

## 3. Types of Layers
A full ConvNet typically uses three types of layers:
1.  **Convolution (CONV):** What we have focused on so far.
2.  **Pooling (POOL):** Used to shrink dimensions (discussed next).
3.  **Fully Connected (FC):** Standard neural network layers (like the flattened step).


## 4. Relation to Earlier Concepts
* **Logistic Regression Input:** In Course 1/2, we always flattened images (e.g., $64 \times 64 \times 3 \to 12288$ vector) *before* feeding them to the network. In CNNs, we keep the image structure ($H \times W \times C$) for as long as possible and only **flatten** at the very end to make the final classification.
* **Hyperparameters:** Just like tuning learning rate or layer size in standard nets, designing a CNN requires choosing filter sizes ($f$), strides ($s$), and padding ($p$).
* **Output Calculation:** We continue to use the universal formula derived in the "Strided Convolutions" section: $n_{out} = \lfloor \frac{n_{in} + 2p - f}{s} + 1 \rfloor$.

## 5. Basic Chunk Implementation (Output Size Calculator)
This snippet automates the math to verify layer shapes.

```python
import math

def calculate_output_shape(n_in, f, s=1, p=0):
    """
    Calculates the output dimension of a convolution layer.
    """
    n_out = math.floor((n_in + 2*p - f) / s) + 1
    return n_out

# Test with Layer 2 from example: 
# Input 37, f=5, s=2, p=0
print(f"Layer 2 Output: {calculate_output_shape(37, 5, 2, 0)}") 
# Result should be 17
