# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository

# CNN Example: LeNet-5 Inspired Architecture

## 1. Architecture Overview
We will build a network to classify a $32 \times 32 \times 3$ RGB image into one of 10 digits (0-9). The network follows a common pattern:
**Conv $\to$ Pool $\to$ Conv $\to$ Pool $\to$ FC $\to$ FC $\to$ Softmax**.

### Step-by-Step Walkthrough

**1. Input:** $32 \times 32 \times 3$.

**2. Layer 1 (Conv + Pool):**
* **Conv1:** $f=5, s=1, p=0$, **6 filters**.
    * Output: $28 \times 28 \times 6$ (Calculated via $\frac{32-5}{1}+1 = 28$).
* **Pool1:** Max Pool, $f=2, s=2$.
    * Output: $14 \times 14 \times 6$ (Reduces height/width by factor of 2).

**3. Layer 2 (Conv + Pool):**
* **Conv2:** $f=5, s=1, p=0$, **16 filters**.
    * Output: $10 \times 10 \times 16$ (Calculated via $\frac{14-5}{1}+1 = 10$).
* **Pool2:** Max Pool, $f=2, s=2$.
    * Output: $5 \times 5 \times 16$.

**4. Flattening:**
* Unroll the volume $5 \times 5 \times 16$ into a vector of size **400**.

**5. Layer 3 (Fully Connected):**
* **FC3:** Connect 400 inputs to **120 units**.
* Parameters: $(120 \times 400) + 120 \approx 48k$.

**6. Layer 4 (Fully Connected):**
* **FC4:** Connect 120 units to **84 units**.

**7. Output Layer:**
* **Softmax:** 84 inputs $\to$ **10 outputs** (probabilities for digits 0-9).


## 2. Key Patterns in ConvNets
1.  **Shrinking Spatial Dimensions:** Height and Width decrease deeper in the network ($32 \to 28 \to 14 \to 10 \to 5$).
2.  **Growing Depth:** The number of channels increases deeper in the network ($3 \to 6 \to 16$).
3.  **Parameter Distribution:**
    * **Conv Layers:** Have very few parameters (e.g., Conv1 has only $5 \times 5 \times 3 \times 6 + 6 = 456$).
    * **Pool Layers:** Have **0 parameters**.
    * **FC Layers:** Have the vast majority of parameters (e.g., FC3 has ~48,000).

## 3. Relation to Earlier Concepts
* **Feature Extraction vs. Classification:** The Conv and Pool layers act as "Feature Extractors," learning edges, shapes, and patterns. The FC layers act essentially as a "Classifier" (like a standard neural net) that takes those high-level features and makes a decision.
* **Activation Size:** Notice how the activation size (total memory usage) drops gradually. If it drops too fast, information is lost; if too slow, computation is expensive.
    * $3072 \to 4704 \to 1176 \to 1600 \to 400 \to 120 \to 84 \to 10$.

## 4. Basic Chunk Implementation (Parameter Counting)
This snippet calculates parameters for a layer, helping verify the table in the lecture.

```python
def count_parameters(layer_type, f, n_C_prev, n_C, n_inputs_flat=0, n_units=0):
    """
    Calculates parameters for different layer types.
    """
    if layer_type == "CONV":
        # Weights: (f * f * n_C_prev) * n_C_filters
        # Bias: 1 per filter
        return (f * f * n_C_prev * n_C) + n_C
    
    elif layer_type == "POOL":
        return 0
    
    elif layer_type == "FC":
        # Weights: n_inputs * n_units
        # Bias: n_units
        return (n_inputs_flat * n_units) + n_units

# Verify Conv1 (f=5, input=3, filters=6)
print(f"Conv1 Params: {count_parameters('CONV', 5, 3, 6)}") # Output: 456

# Verify FC3 (input=400, units=120)
print(f"FC3 Params: {count_parameters('FC', 0, 0, 0, 400, 120)}") # Output: 48120
