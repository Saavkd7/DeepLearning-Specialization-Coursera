# My Diary

Today  **Monday, 19 Jan 2026**.

## General Counter
It has been    **155** days since I started this diary.

# Repository Counter

Day **27** From I started this repository

# Object Localization

## 1. Localization vs. Detection
In computer vision, we distinguish between three main tasks:
* **Image Classification:** Determining *what* is in the image (e.g., "Car").
* **Classification with Localization:** Determining *what* is in the image and *where* it is (drawing a single bounding box around the main object).
* **Object Detection:** Finding *multiple* objects of different categories and localizing all of them.


## 2. Defining the Bounding Box
To localize an object, the neural network must output four numbers:
* $b_x, b_y$: The center coordinates of the bounding box.
* $b_h$: The height of the box.
* $b_w$: The width of the box.

**Coordinate Convention:**
* Top-left corner of the image: $(0, 0)$.
* Bottom-right corner: $(1, 1)$.
* This means all coordinate values range between 0 and 1 relative to the image dimensions.

## 3. The Output Target Label ($y$)
For a task with 3 classes (e.g., Pedestrian, Car, Motorcycle), the target vector $y$ has **8 components**:

$$y = \begin{bmatrix} p_c \\ b_x \\ b_y \\ b_h \\ b_w \\ c_1 \\ c_2 \\ c_3 \end{bmatrix}$$

* $p_c$: **Object Probability**. Is there an object? (1 if yes, 0 if background).
* $b_x, b_y, b_h, b_w$: Bounding box coordinates.
* $c_1, c_2, c_3$: Probabilities for classes 1, 2, and 3.

### The "Don't Care" Condition
* **If object present ($p_c=1$):** We care about all 8 values.
* **If background ($p_c=0$):** We don't care about the box or the class. The other 7 values are "don't cares" (often denoted as `?`).


## 4. The Loss Function
The loss function depends on whether an object is present ($y_1 = p_c$).

* **Case 1: $y_1 = 1$ (Object present)**
    * Loss is the sum of squared errors for all components (position and class).
    * $\mathcal{L}(\hat{y}, y) = (\hat{y}_1 - y_1)^2 + (\hat{b}_x - b_x)^2 + ... + (\hat{c}_3 - c_3)^2$
* **Case 2: $y_1 = 0$ (Background)**
    * Loss depends *only* on $p_c$. We ignore the rest.
    * $\mathcal{L}(\hat{y}, y) = (\hat{y}_1 - y_1)^2$.

*(Note: In practice, we often use Log Loss for the class probabilities $c_i$ and Squared Error for the box coordinates $b_i$.)*

## 5. Relation to Earlier Concepts
* **Regression vs. Classification:** In Course 1/2, we treated regression (predicting a number) and classification (predicting a label) separately. Here, we create a **Multi-Task Learning** setup where one network performs both regression (predicting $b_x, b_y...$) and classification (predicting $c_1...$) simultaneously.
* **Fully Connected Layers:** Just as we used a final FC layer to output a class vector in previous ConvNets, here we just increase the size of that final FC layer to output 8 numbers instead of just the class probabilities.

## 6. Basic Chunk Implementation (Loss Logic)
This snippet demonstrates how to calculate the specialized loss that ignores the bounding box if $p_c=0$.

```python
import torch

def localization_loss(y_pred, y_true):
    """
    y_pred, y_true: Shape (Batch, 8)
    Vector structure: [pc, bx, by, bh, bw, c1, c2, c3]
    """
    # 1. Split prediction into PC (Probability) and the rest
    pc_pred = y_pred[:, 0]
    pc_true = y_true[:, 0]
    
    # 2. Loss for Pc (e.g., MSE or Binary Cross Entropy)
    loss_pc = (pc_pred - pc_true)**2
    
    # 3. Loss for Box + Class (Only if pc_true == 1)
    # We create a mask where object exists
    mask = (pc_true == 1)
    
    # Calculate difference for all remaining 7 components
    diff = (y_pred[:, 1:] - y_true[:, 1:])**2
    loss_box_class = torch.sum(diff, dim=1)
    
    # Apply mask: If pc=0, loss_box_class becomes 0
    total_loss = loss_pc + (loss_box_class * mask)
    
    return torch.mean(total_loss)
