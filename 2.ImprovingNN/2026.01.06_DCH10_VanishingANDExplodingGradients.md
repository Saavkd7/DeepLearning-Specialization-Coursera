# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository



# Deep Learning Challenges: Vanishing & Exploding Gradients

## 1. The Problem
In a very deep neural network ($L=150$ layers), the signal (derivatives) must travel through all layers during Backpropagation.
* **Exploding Gradients:** The derivatives get **exponentially large**. This causes the weights to update wildly, leading to numerical overflow (NaN) and divergence.
* **Vanishing Gradients:** The derivatives get **exponentially small**. This causes the weights in the early layers to stop updating entirely, meaning the network fails to learn.


## 2. The Intuition (The Multiplication Effect)

Assume a network with linear activations ($g(z)=z$) and no bias ($b=0$). The output is simply the product of weight matrices:
$$\hat{y} = W^{[L]} \cdot W^{[L-1]} \cdots W^{[1]} X$$

### Scenario A: Exploding ($W > 1$)
Imagine every weight matrix is slightly larger than the Identity Matrix (e.g., diagonal elements are **1.5**).
* Calculation: $1.5 \times 1.5 \times 1.5 \dots$ (for $L$ layers).
* Result: $1.5^{L}$.
* If $L=100$, the signal grows massive. The output (and the gradient) explodes.

### Scenario B: Vanishing ($W < 1$)
Imagine every weight matrix is slightly smaller (e.g., diagonal elements are **0.5**).
* Calculation: $0.5 \times 0.5 \times 0.5 \dots$
* Result: $0.5^{L}$.
* If $L=100$, the signal becomes essentially **zero**. The gradient vanishes, and gradient descent takes infinitesimally small steps.

## 3. The Takeaway
If you initialize weights randomly without care, they are likely to trigger one of these two states.
* **Weights too big?** Explosion.
* **Weights too small?** Vanishing.
* **Solution:** We need a partial solution to start weights in the "Goldilocks Zone" (not too big, not too small). This is called **Weight Initialization** (covered next).
