# My Diary

Today  **Saturday, 17 Jan 2026**.

## General Counter
It has been    **153** days since I started this diary.

# Repository Counter

Day **25** From I started this repository



# Debugging Deep Nets: Numerical Gradient Approximation

## 1. Executive Summary
When you implement Backpropagation, it is easy to make subtle bugs that are hard to detect (the model trains, but poorly).
To verify your code, you use **Gradient Checking**. This involves calculating the slope (derivative) of your Cost Function manually using a simple geometry formula ("Rise over Run"). If this manual value matches your Backpropagation output, your code is correct.
* **The Trick:** Use a **Two-Sided Difference** (checking both left and right of the point) rather than a One-Sided Difference. It is twice as slow but much more accurate.

## 2. The Technique: Two-Sided Difference

To approximate the slope (gradient $g$) of a function $f$ at parameter $\theta$:

### The Formula
Instead of just nudging $\theta$ to the right ($\theta + \epsilon$), we nudge it both ways to create a centered triangle.
$$g(\theta) \approx \frac{f(\theta + \epsilon) - f(\theta - \epsilon)}{2\epsilon}$$
* **$\epsilon$ (Epsilon):** A tiny number, typically $10^{-7}$ or $0.01$.

### Concrete Example
Let $f(\theta) = \theta^3$. Let's find the slope at $\theta = 1$ (True derivative is $3\theta^2 = 3$).
* **Using $\epsilon = 0.01$:**
    * $\theta_{right} = 1.01 \rightarrow (1.01)^3 = 1.030301$
    * $\theta_{left} = 0.99 \rightarrow (0.99)^3 = 0.970299$
    * **Difference:** $1.030301 - 0.970299 = 0.060002$
    * **Divide by $2\epsilon$:** $0.060002 / 0.02 = \mathbf{3.0001}$
* **Result:** The error is $0.0001$. This is incredibly close to the true value of $3$.

## 3. "In Plain English"

### The "Triangle" Analogy
Imagine you are standing on a hill ($\theta$) and want to know the steepness (gradient).
* **One-Sided (Bad Way):** You look at a point one step ahead of you and draw a line. This line ignores the shape of the hill behind you.
* **Two-Sided (Good Way):** You look at a point one step ahead **and** one step behind you. You connect them with a straight line (the green triangle in the diagram). This line balances out the curvature of the hill, giving you a near-perfect estimate of the steepness right where you are standing.

## 4. Expert Nuance

### Why Two-Sided? ($O(\epsilon^2)$ vs $O(\epsilon)$)
For those familiar with Calculus (Taylor Series), the error rates differ significantly:
* **One-Sided Difference:** The error is proportional to $\epsilon$ ($O(\epsilon)$).
    * If $\epsilon = 0.01$, error $\approx 0.03$.
* **Two-Sided Difference:** The error is proportional to $\epsilon^2$ ($O(\epsilon^2)$).
    * If $\epsilon = 0.01$, error $\approx 0.0001$.
* **Takeaway:** Since $\epsilon$ is already tiny ($<1$), squaring it makes the error infinitesimally small. Always use the two-sided formula for Gradient Checking, even though it requires computing the cost function twice.

```python
import numpy as np

def gradient_check(parameters, gradients, X, Y, epsilon=1e-7):
    """
    Checks if backward_propagation computes the gradient correctly.
    
    Arguments:
    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", etc.
    gradients -- python dictionary containing your gradients "dW1", "db1", "dW2", "db2", etc.
    X -- input data of shape (input_size, m)
    Y -- true labels
    epsilon -- tiny shift to the input to compute approximated gradient
    
    Returns:
    difference -- difference between the approximated gradient and the backward propagation gradient
    """
    
    # 1. Convert dictionary parameters to a single vector (theta)
    # (Helper function 'dictionary_to_vector' flattens W1, b1... into a long 1D array)
    parameters_values, _ = dictionary_to_vector(parameters)
    
    # 2. Convert dictionary gradients to a single vector (grad)
    grad = gradients_to_vector(gradients)
    
    # Initialize the vector for numerical approximation
    num_parameters = parameters_values.shape[0]
    J_plus = np.zeros((num_parameters, 1))
    J_minus = np.zeros((num_parameters, 1))
    gradapprox = np.zeros((num_parameters, 1))
    
    # 3. Loop over every parameter to compute its numerical gradient
    for i in range(num_parameters):
        
        # Save the original value so we can restore it later
        thetaplus = np.copy(parameters_values)
        thetaminus = np.copy(parameters_values)
        
        # Compute J_plus [J(theta + epsilon)]
        thetaplus[i][0] = thetaplus[i][0] + epsilon
        # (Helper function 'vector_to_dictionary' converts vector back to W1, b1...)
        J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))
        
        # Compute J_minus [J(theta - epsilon)]
        thetaminus[i][0] = thetaminus[i][0] - epsilon
        J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus))
        
        # Compute Grad Approx (Two-sided difference)
        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)
    
    # 4. Compare gradapprox (Numerical) vs grad (Analytical/Backprop)
    # We use the Euclidean distance formula normalized by the lengths
    numerator = np.linalg.norm(grad - gradapprox)
    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)
    difference = numerator / denominator

    if difference > 2e-7:
        print(f"\033[91mMistake likely in backward propagation! difference = {difference}\033[0m")
    else:
        print(f"\033[92mYour backward propagation works perfectly. difference = {difference}\033[0m")
        
    return difference
