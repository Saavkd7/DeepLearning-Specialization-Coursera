# My Diary

Today  **Saturday, 17 Jan 2026**.

## General Counter
It has been    **153** days since I started this diary.

# Repository Counter

Day **25** From I started this repository




# Deep Learning Theory: Why Regularization Works

## 1. Executive Summary
Regularization works by simplifying the model.
When you increase the regularization parameter $\lambda$, you force the weights $W$ to be small.
* **Intuition 1 (Simpler Network):** Small weights weaken the connections between neurons. This effectively "turns down" the complexity, making a deep network behave more like a simple linear model.
* **Intuition 2 (Linear Activation):** Small weights constrain the inputs $Z$ to a small range. For activation functions like `tanh`, a small range corresponds to the **linear** part of the curve. A network of linear functions cannot model complex, jagged boundaries (overfitting).

## 2. Intuition 1: "Zeroing Out" Units

Imagine a complex Deep Network that is overfitting.
* **Action:** You increase $\lambda$ (the penalty for large weights).
* **Effect:** The cost function pushes the weight matrices $W$ toward zero.
* **Result:** Many hidden units now have near-zero impact.
    * It is *as if* those neurons were removed.
    * The network transitions from a "High Variance" giant brain to a "High Bias" simple logistic regression.
    * **The Sweet Spot:** By tuning $\lambda$, you find the middle ground: "Just Right".



## 3. Intuition 2: The Linear Regime (Tanh Example)

Consider the `tanh` function ($g(z) = \tanh(z)$).
* **Wide Range (Overfitting):** If $W$ is large, $Z$ varies wildly (e.g., -10 to +10). The function hits the flat non-linear curves. This allows the model to draw crazy, complex shapes.
* **Small Range (Regularized):** If $W$ is small (due to high $\lambda$), then $Z$ stays small (e.g., -0.1 to +0.1).
    * **The Key:** In this small center region, `tanh(z)` acts almost exactly like a straight line ($g(z) \approx z$).
    * **Consequence:** If every layer acts linearly, the whole deep network collapses mathematically into a single linear regression. Linear models *cannot* overfit wiggly data.

## 4. Expert Nuance

### Debugging Tip
When using Regularization, the Cost Function $J$ changes.
$$J_{new} = J_{old} + \text{Regularization Penalty}$$
* **Common Bug:** If you plot the cost to check for convergence, ensure you plot $J_{new}$. If you only plot the old cross-entropy loss ($J_{old}$), it might not decrease monotonically, making you think the code is broken when it isn't.
