# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository

# Debugging Strategy: Gradient Checking Implementation

## 1. Executive Summary
Gradient Checking ("Grad Check") is the ultimate unit test for Neural Networks. It verifies that your complex Backpropagation logic is calculating the *exact* same derivatives as a simple "Rise over Run" slope calculation.
If the difference between them is tiny ($< 10^{-7}$), your code is bug-free.

## 2. The Implementation Workflow

### Step 1: Reshape to Giant Vectors
Your neural network parameters are scattered across matrices ($W^{[1]}, b^{[1]}, W^{[2]}...$).
To make checking easier, you must **unroll** and **concatenate** them into two giant 1D vectors:
* **$\theta$ (Theta):** Contains every single parameter ($W, b$) from the entire network.
* **$d\theta$ (dTheta):** Contains every single gradient ($dW, db$) from your Backpropagation.
* *Goal:* Verify that $d\theta$ is truly the slope of $J(\theta)$.


### Step 2: The Loop (Numerical Approximation)
You loop through every single element $i$ of the vector $\theta$ and calculate its approximate slope:
$$d\theta_{\text{approx}}[i] = \frac{J(\theta_1, \dots, \theta_i + \epsilon, \dots) - J(\theta_1, \dots, \theta_i - \epsilon, \dots)}{2\epsilon}$$
* This effectively asks: "If I nudge parameter $i$ slightly up and down, how much does the cost change?".

### Step 3: The Comparison (The Euclidean Test)
You now have two vectors:
1.  **$d\theta_{\text{approx}}$:** The slow, accurate numerical slope.
2.  **$d\theta$:** The fast, analytical slope from your Backprop code.

You compare their similarity using this normalized formula:
$$\text{Difference} = \frac{||d\theta_{\text{approx}} - d\theta||_2}{||d\theta_{\text{approx}}||_2 + ||d\theta||_2}$$
* *Note:* The denominator prevents the error from looking huge just because the gradients themselves are large numbers.


## 3. How to Interpret the Result

When you print the `Difference` value, use this scorecard to decide if you have a bug:

| Difference Value | Diagnosis | Action |
| :--- | :--- | :--- |
| **$< 10^{-7}$** | **Great!** | Your backprop is correct. Turn off Grad Check and train. |
| **$\approx 10^{-5}$** | **Suspicious** | Look closely. Check if any specific index $i$ has a huge difference. |
| **$> 10^{-3}$** | **FAIL** | You definitely have a bug in Backprop. Find which $dW$ or $db$ is causing the spike. |
