# My Diary

Today  **Saturday, 17 Jan 2026**.

## General Counter
It has been    **153** days since I started this diary.

# Repository Counter

Day **25** From I started this repository

# Deep Learning Efficiency: Normalizing Inputs

## 1. Executive Summary
If your input features are on different scales (e.g., $x_1$ ranges from 0 to 1, but $x_2$ ranges from 0 to 1000), Gradient Descent will struggle. The optimizer will oscillate inefficiently, forcing you to use a tiny learning rate.
**Normalizing inputs** transforms all features to a similar scale (usually mean 0, variance 1). This makes the Cost Function symmetric ("spherical"), allowing the optimizer to march straight to the minimum with larger steps.

## 2. Technical Deep Dive

### The Algorithm
There are two mathematical steps applied to every training example $x$:

1.  **Zero Out the Mean:** Subtract the average ($\mu$) so the data centers around the origin.
    $$\mu = \frac{1}{m} \sum x^{(i)} \quad ; \quad x := x - \mu$$
2.  **Normalize Variance:** Divide by the variance ($\sigma^2$) to squash the spread.
    $$\sigma^2 = \frac{1}{m} \sum (x^{(i)})^2 \quad ; \quad x := x / \sigma^2$$
    *(Note: This creates a dataset where $x_1$ and $x_2$ both have variance $\approx 1$)*.


### The Geometry of Optimization
Why does this help?
* **Unnormalized (Elongated Bowl):** If ranges differ ($0 \to 1$ vs $0 \to 1000$), the Cost Function $J$ looks like a squashed, narrow valley. Gradient Descent bounces back and forth against the "walls" of the valley, moving slowly toward the center.
* **Normalized (Spherical Bowl):** If ranges are similar, $J$ looks like a perfect round bowl. The gradient points directly at the center, allowing for a straight, fast path.


## 3. "In Plain English"

### The "Hiking" Analogy
Imagine you are trying to walk to the bottom of a valley in the dark.
* **Unnormalized:** The valley is extremely long and narrow (like a hallway). If you take a big step, you crash into the wall. You have to take tiny, careful steps to stop yourself from bouncing off the sides. It takes forever.
* **Normalized:** The valley is a perfect circular crater. No matter where you are, you can take giant, confident steps directly toward the center. You arrive much faster.

## 4. Expert Nuance

### The "Golden Rule" of Test Sets
A critical mistake beginners make is calculating $\mu$ and $\sigma$ separately for the training and test sets. **Do not do this.**
* **Correct Way:** Calculate $\mu$ and $\sigma$ on your **Training Set**. Then use **those exact same numbers** to transform your **Test Set**.
* **Why:** Your model learned a specific transformation (e.g., "subtract 50"). If you subtract 50 from the train set but subtract 40 from the test set, you are feeding the model inconsistent data.

## 5. Implementation Code

```python
import numpy as np

def normalize_data(X_train, X_test):
    """
    Normalizes training and test sets using the Training Set statistics.
    """
    # 1. Calculate statistics on TRAINING set only
    mu = np.mean(X_train, axis=1, keepdims=True)
    sigma = np.std(X_train, axis=1, keepdims=True)
    
    # 2. Apply to Training set
    X_train_norm = (X_train - mu) / sigma
    
    # 3. Apply SAME statistics to Test set (Critical!)
    X_test_norm = (X_test - mu) / sigma
    
    return X_train_norm, X_test_norm
