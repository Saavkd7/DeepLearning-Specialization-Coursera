# Deep Learning Techniques: Weight Initialization

## 1. Executive Summary
Weight Initialization is a partial solution to the **Vanishing/Exploding Gradient** problem.
The goal is to initialize the weights $W$ in such a way that the activations $A$ neither explode to infinity nor vanish to zero as they pass through deep layers. We achieve this by keeping the **Variance** of the activations constant from one layer to the next.
* **The Rule:** If a neuron has a large number of inputs ($n$), its weights must be smaller to keep the total output $Z$ reasonable.

## 2. The Logic: Single Neuron Intuition

Consider a single neuron:
$$z = w_1 x_1 + w_2 x_2 + \dots + w_n x_n$$
* **The Math:** If the inputs $x$ have a variance of 1, and you want $z$ to also have a variance of 1, then the variance of the weights $w$ must be $\frac{1}{n}$.
* **Why:** If $n=1000$ and you leave $w$ large, the sum $z$ will be massive (Exploding). If you scale $w$ down by $\sqrt{1/n}$, the sum remains stable.



## 3. The Formulas (He vs. Xavier)

The specific formula depends on your **Activation Function**.

### A. He Initialization (For ReLU)
Because ReLU kills half the neurons (outputs 0 for negative inputs), we need to double the variance to keep the signal strong.
* **Variance:** $\frac{2}{n^{[l-1]}}$
* **Formula:**
    $$W^{[l]} = \text{np.random.randn}(\dots) \times \sqrt{\frac{2}{n^{[l-1]}}}$$
* **Best For:** ReLU, Leaky ReLU.

### B. Xavier Initialization (For Tanh/Sigmoid)
* **Variance:** $\frac{1}{n^{[l-1]}}$
* **Formula:**
    $$W^{[l]} = \text{np.random.randn}(\dots) \times \sqrt{\frac{1}{n^{[l-1]}}}$$
* **Best For:** Tanh, Sigmoid.

## 4. "In Plain English"

### The "Mixer" Analogy
Imagine you are mixing a song.
* **Inputs ($x$):** You have 100 different audio tracks (drums, vocals, guitars).
* **Weights ($w$):** The volume slider for each track.
* **Output ($z$):** The master volume.
* **The Problem:** If you keep all 100 sliders at max volume ($w=1$), the master output will be distorted and blown out (Exploding Gradient).
* **The Solution:** To keep the master volume clean, you must lower the individual sliders. If you add more tracks ($n$ increases), you must lower the average volume of each track ($\sqrt{1/n}$) to compensate.

## 5. Implementation Code

This snippet shows exactly how to initialize a layer in Python using "He Initialization".

```python
import numpy as np

def initialize_parameters_he(layers_dims):
    """
    Arguments:
    layer_dims -- python array (list) containing the size of each layer.
    """
    parameters = {}
    L = len(layers_dims) - 1 
    
    for l in range(1, L + 1):
        # n_in is the size of the previous layer (layers_dims[l-1])
        n_in = layers_dims[l-1]
        n_out = layers_dims[l]
        
        # HE INITIALIZATION (For ReLU)
        # 1. Generate Gaussian Random Numbers
        # 2. Scale by sqrt(2 / n_in)
        parameters['W' + str(l)] = np.random.randn(n_out, n_in) * np.sqrt(2. / n_in)
        
        # Bias is usually initialized to zero
        parameters['b' + str(l)] = np.zeros((n_out, 1))
        
    return parameters
