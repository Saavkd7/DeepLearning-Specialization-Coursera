# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository


# Hyperparameter Search: Workflow Strategies

## 1. Re-Evaluate Occasionally
Hyperparameters are not "set and forget." Even if you find the perfect settings today, they might not work in a few months.
* **Why?** Data changes, software libraries update, and hardware infrastructure evolves. All these shifts can make your old "best" settings stale.
* **Action:** Re-test your hyperparameters every few months to ensure they are still optimal.


## 2. The Two Strategic Approaches
How you search depends entirely on your **computational resources**. Andrew Ng uses a biology analogy to describe the two main styles.

### A. The "Panda" Strategy (Babysitting One Model)
* **Context:** You have **huge data** or a **huge model**, but **limited CPU/GPU resources**. You can only afford to train one model at a time.
* **Method:** You watch the model day-by-day.
    * *Day 1:* It's learning well? Good.
    * *Day 2:* It slowed down? Nudge the learning rate up slightly.
    * *Day 3:* It diverged? Stop, revert to the previous day's parameters, and lower the rate.
* **Analogy:** Like a Panda, you have one "baby" (model) and you invest high effort into keeping that single baby alive and healthy.


### B. The "Caviar" Strategy (Training in Parallel)
* **Context:** You have **abundant computational resources**. You can run dozens of models simultaneously.
* **Method:** You set up 10, 20, or 100 different models with different hyperparameter settings and launch them all at once. You don't "babysit" them. After a few days, you simply check the results and pick the one with the best curve (the "survivor").
* **Analogy:** Like fish or reptiles (Caviar), you lay thousands of eggs. You don't care if most of them fail; you only need one of them to succeed.

### Summary Table

| Feature | Panda Strategy | Caviar Strategy |
| :--- | :--- | :--- |
| **Resource Level** | Low Compute / Massive Model | High Compute |
| **Method** | Constant manual tuning | Parallel automated search |
| **Focus** | Survival of one model | Selection of the best survivor |
