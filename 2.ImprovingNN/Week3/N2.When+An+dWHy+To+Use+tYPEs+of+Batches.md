# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository



# Optimization Regimes: When and Why to Use Each

## 1. Batch Gradient Descent (Batch Size = $m$)

**Use this when:**
* **Dataset size is small ($m \le 2,000$).**

**Why?**
* **Vectorization Speed:** With small data, modern CPUs/GPUs can process the entire matrix at once almost instantly. Splitting it up adds code complexity without saving any meaningful time.
* **Stability:** It is the most stable method. The cost function decreases monotonically (smoothly) on every single step, making it easy to debug and verify convergence.

---

## 2. Mini-Batch Gradient Descent (Batch Size = 64, 128, 256, 512)

**Use this when:**
* **Dataset size is large ($m > 2,000$).** This is the **standard** for Deep Learning.

**Why?**
* **Faster "Time to First Step":** With 5 million examples, Batch Gradient Descent forces you to wait for the entire dataset to process before taking *one* step. Mini-Batch lets you take 5,000 steps in that same time.
* **Vectorization Efficiency:** Unlike Stochastic (size=1), a batch size of 64 or 128 is large enough to utilize vectorization (matrix multiplication) on your hardware, keeping operations fast.
* **Hardware Alignment:** Using powers of 2 (64, 128, 256) aligns with the way GPU/CPU memory is structured, often yielding slight performance gains.

---

## 3. Stochastic Gradient Descent (Batch Size = 1)

**Use this when:**
* **Rarely/Never in deep learning.** It is mostly used for real-time online learning (where data arrives one by one) or in specific theoretical contexts.

**Why avoid it?**
* **Loss of Vectorization:** Processing 1 example at a time kills your speed. You lose the massive parallel processing power of your GPU.
* **Too Noisy:** The path to the minimum is extremely chaotic. While it generally heads in the right direction, it will oscillate forever around the minimum rather than converging, unless you carefully anneal (lower) the learning rate.
