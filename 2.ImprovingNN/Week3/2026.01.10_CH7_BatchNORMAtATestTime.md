# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository


# Batch Normalization at Test Time

## 1. The Problem: No Mini-Batch at Inference
During **Training**, Batch Norm calculates the mean ($\mu$) and variance ($\sigma^2$) using the current mini-batch (e.g., 64 examples).
$$\mu = \frac{1}{m} \sum z^{(i)}, \quad \sigma^2 = \frac{1}{m} \sum (z^{(i)} - \mu)^2$$

However, at **Test Time** (Inference), you often process examples **one at a time** (e.g., a single image from a user).
* **Issue:** If you try to calculate the mean and variance of a *single* example, the mean is just the value itself, and the variance is 0. This breaks the math ($\frac{z - \mu}{0}$), and the normalization becomes meaningless.

## 2. The Solution: Exponentially Weighted Averages
To fix this, we need a stable estimate of the "global" mean and variance that we can use for any single test example.

### During Training (The Setup)
While the network is training, we keep a separate **Running Average** (Exponentially Weighted Average) of the means and variances seen across all mini-batches.
* For each layer $l$, track:
    * $\mu_{\text{running}}$
    * $\sigma^2_{\text{running}}$
* On every step, update them just like we updated momentum:
    $$\mu_{\text{avg}} = \beta \mu_{\text{avg}} + (1-\beta) \mu_{\text{current\_batch}}$$
    *(This estimates the average statistics of your entire training dataset)*.

### During Testing (The Execution)
When you deploy the model, you stop calculating batch statistics. Instead, you use the **final running averages** ($\mu_{\text{avg}}, \sigma^2_{\text{avg}}$) as fixed constants.

**The Test-Time Formula:**
$$z_{\text{norm}} = \frac{z_{\text{test}} - \mu_{\text{avg}}}{\sqrt{\sigma^2_{\text{avg}} + \epsilon}}$$
$$\tilde{z}_{\text{test}} = \gamma z_{\text{norm}} + \beta$$
*(Note: $\gamma$ and $\beta$ are the standard learned parameters)*.



## 3. Summary
* **Training:** Normalize using the **current batch's** mean/variance.
* **Testing:** Normalize using the **running average** mean/variance collected during training.
* **Result:** The network can handle single examples instantly without needing a full batch.




