# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository




# Optimization Strategy: Choosing Mini-Batch Size

## 1. Executive Summary
The size of your mini-batch determines the behavior of your optimizer.
* **Too Large (Batch):** Slow per step.
* **Too Small (Stochastic):** Loses vectorization speedup and oscillates wildly.
* **Just Right:** Balances the speed of vectorization with the frequency of updates.

## 2. The Three Regimes

### A. Batch Gradient Descent (Size = $m$)
* **Definition:** You use the **entire training set** for every step.
* **Pros:** The error curve decreases smoothly on every iteration. It heads straight for the minimum.
* **Cons:** If $m$ is large (e.g., 5 million), a single step takes forever. You might wait hours just to move once.

### B. Stochastic Gradient Descent (Size = 1)
* **Definition:** You use a **single example** for every step.
* **Pros:** You take steps instantly.
* **Cons:**
    1.  **No Vectorization:** You process data one-by-one, which is extremely inefficient for CPUs/GPUs.
    2.  **Noisy:** The gradient points roughly to the minimum, but individual examples might point elsewhere (e.g., mislabeled data). The model oscillates around the minimum but never settles.

### C. Mini-Batch Gradient Descent (Size = $64 \to 512$)
* **Definition:** You use a chunk of data (e.g., 128 examples).
* **The Sweet Spot:**
    * **Fast Steps:** You update parameters frequently (like Stochastic).
    * **Vectorization:** You still get the speed boost of matrix multiplication (like Batch).
    * **Convergence:** It is noisier than Batch but more stable than Stochastic.


## 3. Visualizing the Cost Function
* **Batch:** A smooth, monotonic curve downward.
* **Mini-Batch:** A jagged, noisy curve that trends downward. Since each batch is different (some are "harder" than others), the cost might go up briefly, but the trend should decrease.


## 4. Practical Guidelines

### When to use what?
1.  **Small Dataset ($m \le 2000$):**
    * Use **Batch Gradient Descent**.
    * *Why:* If the data is small, vectorization can handle it instantly. Splitting it just adds code complexity without speed gains.

2.  **Big Dataset:**
    * Use **Mini-Batch Gradient Descent**.
    * **Standard Sizes:** **64, 128, 256, 512**.
    * *Power of 2 Rule:* Computer memory is organized in powers of 2. Using these numbers (e.g., $2^6, 2^7, 2^8$) often makes matrix operations run faster.

### The "Memory Cliff"
**Critical Check:** Ensure your mini-batch fits in your **CPU/GPU Memory**.
If you pick a size that is slightly too big (e.g., larger than your GPU VRAM), the system will start swapping memory to disk, and performance will fall off a cliff.
