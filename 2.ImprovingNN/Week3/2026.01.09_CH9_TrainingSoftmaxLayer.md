



# Softmax Classification: Deeper Insights

## 1. Softmax vs. Hardmax
Why is it called "Softmax"? It comes from contrasting it with "Hardmax".

* **Hardmax:** This function looks at the raw scores ($Z$) and strictly assigns a **1** to the largest value and **0** to everything else.
    * *Example:* If $Z = [5, 2, -1, 3]$, Hardmax outputs $[1, 0, 0, 0]$.
    * It is a very abrupt, "hard" mapping.
* **Softmax:** This function provides a "gentler" mapping. It assigns probabilities based on magnitude.
    * *Example:* Softmax outputs $[0.842, 0.042, 0.004, 0.114]$.
    * It preserves information about how close the other classes were to the winner.

## 2. Generalization of Logistic Regression
**Softmax is not a separate algorithm; it is the generalized form of Logistic Regression.**
* **Case $C=2$:** If you set the number of classes to 2 (Binary Classification), the Softmax formula mathematically reduces to the standard Logistic Regression formula.
* **Redundancy:** With $C=2$, Softmax outputs two numbers (e.g., $0.842$ and $0.158$). Since they must sum to 1, computing both is redundant. Logistic regression simplifies this by just computing one ($p$) and assuming the other is $(1-p)$.

## 3. Training Setup: One-Hot Encoding
To train a Softmax layer, we must represent our target labels ($y$) as vectors, not just single numbers.

* **Example:** If the input is a **Cat** (Class 1 out of 4 classes):
    * **Target $y$:** $[0, 1, 0, 0]$ (This is called a "One-hot vector").
    * **Prediction $\hat{y}$:** $[0.1, 0.4, 0.3, 0.2]$ (The network's current probabilities).
* **The Goal:** The learning algorithm tries to push the prediction vector $\hat{y}$ to look more like the target vector $y$.
