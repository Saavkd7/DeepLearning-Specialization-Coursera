# My Diary

Today  **Saturday, 17 Jan 2026**.

## General Counter
It has been    **153** days since I started this diary.

# Repository Counter

Day **25** From I started this repositor


# Multi-Class Classification: Softmax Regression

## 1. Executive Summary
Binary classification (Logistic Regression) answers "Yes/No" (0 or 1).
**Softmax Regression** answers "Which one?" among multiple choices (0, 1, 2, 3...).
* **Goal:** Classify an input into one of $C$ possible classes.
* **Constraint:** The probabilities for all classes must sum to exactly 1.

## 2. The Scenario: Cats, Dogs, and Baby Chicks
Imagine you want to classify an image into one of four categories:
* Class 0: None of the above (Other)
* Class 1: Cat
* Class 2: Dog
* Class 3: Baby Chick
Total Classes ($C$) = 4.


## 3. The Architecture
Unlike binary classification where the output layer has 1 unit, in Softmax, the output layer has **$C$ units** (in this case, 4).
* **Input:** $X$
* **Output ($\hat{y}$):** A $(4 \times 1)$ vector.
    * $\hat{y}_0$: Probability it is "Other".
    * $\hat{y}_1$: Probability it is "Cat".
    * $\hat{y}_2$: Probability it is "Dog".
    * $\hat{y}_3$: Probability it is "Baby Chick".
* **Rule:** $\sum \hat{y}_i = 1$.


## 4. The Algorithm (The Softmax Layer)
How do we turn raw numbers ($Z$) into probabilities?

**Step 1: Compute Linear Output**
$$Z^{[L]} = W^{[L]}a^{[L-1]} + b^{[L]}$$
* Result: A vector of raw scores (logits), e.g., $Z = [5, 2, -1, 3]$.

**Step 2: Exponentiate (Create temporary vector $t$)**
Apply $e^z$ element-wise. This makes all values positive and emphasizes larger scores.
$$t = e^{Z^{[L]}} = [e^5, e^2, e^{-1}, e^3] \approx [148.4, 7.4, 0.4, 20.1]$$
.

**Step 3: Normalize (The Softmax Activation)**
Divide each element by the sum of all elements so they add up to 1.
$$a^{[L]}_i = \frac{t_i}{\sum_{j=1}^{C} t_j}$$
* Example Calculation: Sum = $176.3$.
    * $P(\text{Class 0}) = 148.4 / 176.3 \approx 0.842$ (84.2%)
    * $P(\text{Class 3}) = 20.1 / 176.3 \approx 0.114$ (11.4%)
.


## 5. Visual Intuition: Linear Decision Boundaries
Just like Logistic Regression draws a straight line to separate two classes, Softmax draws **multiple linear boundaries** to separate multiple classes.
* It carves the input space into distinct regions (e.g., a "Yellow" region for Class 0, a "Purple" region for Class 1, etc.).
* Even with no hidden layers, Softmax can separate classes as long as they are linearly separable.
