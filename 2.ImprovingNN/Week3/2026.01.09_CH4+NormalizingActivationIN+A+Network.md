# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository

# Optimization Strategy: Batch Normalization

## 1. Executive Summary
Just as **Normalizing Inputs** ($X$) speeds up training for the first layer, **Batch Normalization** applies that same logic deep inside the network.
It normalizes the mean and variance of the hidden layer activations ($Z$). This ensures that every layer receives "clean," standardized data, making the network much more robust and easier to train.


## 2. The Core Intuition
* **Input Normalization:** We know that centering input $X$ to mean 0 and variance 1 makes the cost function rounder and easier to optimize.
* **Hidden Layer Normalization:** In a deep network, the input to Layer 3 is the activation from Layer 2 ($A^{[2]}$). If we normalize $A^{[2]}$ (or technically $Z^{[2]}$), then Layer 3 can learn much faster, just like Layer 1 did.

## 3. The Algorithm (Step-by-Step)
For a specific layer $l$, take the intermediate values $Z^{(1)}, \dots, Z^{(m)}$ from the current mini-batch:

1.  **Calculate Mean:**
    $$\mu = \frac{1}{m} \sum_i z^{(i)}$$
2.  **Calculate Variance:**
    $$\sigma^2 = \frac{1}{m} \sum_i (z^{(i)} - \mu)^2$$
3.  **Normalize:**
    $$z_{\text{norm}}^{(i)} = \frac{z^{(i)} - \mu}{\sqrt{\sigma^2 + \epsilon}}$$
    *(Note: $\epsilon$ is added for numerical stability to avoid dividing by zero)*.
4.  **Scale and Shift (Critical Step):**
    $$\tilde{z}^{(i)} = \gamma z_{\text{norm}}^{(i)} + \beta$$
    *(Note: $\gamma$ and $\beta$ are **learnable parameters**, just like weights $W$ and $b$)*.


## 4. Why the "Scale and Shift" ($\gamma, \beta$)?
You might ask: *Why normalize to mean 0 and then immediately multiply by $\gamma$ and add $\beta$ to change the mean again?*

**The Reason:** We don't want to **force** the hidden units to always have mean 0 and variance 1.
* Example: If you are using a **Sigmoid** activation, mean 0 forces the data into the linear middle section of the sigmoid curve. The network might *want* the data to be spread out further to use the non-linear "tails" of the function.
* **Flexibility:** $\gamma$ and $\beta$ allow the network to decide the optimal mean and variance for itself. If the network decides that the raw data was better, it can learn to set $\gamma = \sqrt{\sigma^2}$ and $\beta = \mu$, effectively cancelling out the normalization.
