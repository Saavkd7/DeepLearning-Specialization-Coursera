# My Diary

Today  **Saturday, 17 Jan 2026**.

## General Counter
It has been    **153** days since I started this diary.

# Repository Counter

Day **25** From I started this repository

# Implementation: Batch Norm in Deep Networks

## 1. Where does it fit?
Batch Normalization is applied **between** the linear calculation ($Z$) and the activation function ($A$).
Instead of feeding the raw $Z$ into the activation (e.g., Sigmoid/ReLU), you feed the normalized $\tilde{Z}$.

**The Flow:**
$$X \xrightarrow{W, b} Z \xrightarrow{\text{Batch Norm}} \tilde{Z} \xrightarrow{\text{Activation}} A$$


## 2. The New Parameters
When you add Batch Norm to a layer $l$, you introduce two new learnable parameters that control the shape of the data for that layer:
1.  **$\gamma^{[l]}$ (Gamma):** Controls the scale (variance).
2.  **$\beta^{[l]}$ (Beta):** Controls the shift (mean).
    * *Note:* Do not confuse this $\beta$ with the momentum hyperparameter. They share the same symbol but are completely unrelated.

### The Fate of the Bias ($b$)
**Crucial Detail:** You can **delete** the bias term $b^{[l]}$ from your network.
* **Why?** Batch Normalization calculates the mean of $Z$ ($Z = W A + b$) and immediately subtracts it.
* **The Math:** Since $b$ is a constant added to every example, it is also added to the mean. When you do $Z - \mu$, the $b$ terms cancel out perfectly.
* **The Replacement:** The parameter $\beta^{[l]}$ in Batch Norm now handles the job of "shifting" the data, so $b^{[l]}$ is redundant. Set it to zero or remove it.


## 3. The Algorithm (Mini-Batch Gradient Descent with BN)

You iterate through mini-batches $X^{\{t\}}$. For each step:

1.  **Forward Propagation:**
    * Compute $Z^{[l]}$ using weights $W^{[l]}$.
    * **Batch Norm:** Calculate $\mu$ and $\sigma^2$ *on this specific mini-batch*.
    * Normalize and Scale: Compute $\tilde{Z}^{[l]}$ using $\gamma^{[l]}$ and $\beta^{[l]}$.
    * Activation: Compute $A^{[l]} = g(\tilde{Z}^{[l]})$.
2.  **Back Propagation:**
    * Compute gradients $dW^{[l]}$, $d\beta^{[l]}$, and $d\gamma^{[l]}$.
3.  **Update Parameters:**
    * Update $W^{[l]} = W^{[l]} - \alpha dW^{[l]}$
    * Update $\beta^{[l]} = \beta^{[l]} - \alpha d\beta^{[l]}$
    * Update $\gamma^{[l]} = \gamma^{[l]} - \alpha d\gamma^{[l]}$
    * *(Works with Momentum, RMSprop, and Adam as well)*.
