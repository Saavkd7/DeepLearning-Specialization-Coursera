# My Diary

Today  **Saturday, 17 Jan 2026**.

## General Counter
It has been    **153** days since I started this diary.

# Repository Counter

Day **25** From I started this repository




# TensorFlow: The Basic Structure

## 1. Executive Summary
TensorFlow is a framework that handles the heavy lifting of optimization.
* **The Old Way (NumPy):** You had to mathematically derive gradients and implement backpropagation manually.
* **The TensorFlow Way:** You simply write the **Forward Propagation** (the cost function), and TensorFlow automatically figures out the derivatives and backpropagation for you.

## 2. Core Components

### A. Variables (`tf.Variable`)
You must define the parameters you want to optimize (like weights $w$ or $b$) as **Variables**. This tells TensorFlow, "Track this value and update it as we learn."
* Syntax: `w = tf.Variable(0.0, dtype=tf.float32)`.

### B. The GradientTape (`tf.GradientTape`)
This is the "magic" of TensorFlow.
* **Analogy:** Think of an old-school **Cassette Tape**.
* **How it works:** As you compute your cost function (Forward Prop), the Tape "records" every operation. To calculate gradients, you simply "play the tape backward." This allows TensorFlow to revisit operations in reverse order to perform backpropagation automatically.

### C. The Optimizer
You choose a pre-built algorithm (like **Adam**) to update your variables based on the gradients.
* Syntax: `optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)`.

---

## 3. The Code Structure (Step-by-Step)

Here is the standard "loop" for a custom training process in TensorFlow:

1.  **Initialize** parameters and optimizer.
2.  **Loop** through your training steps (iterations).
3.  **Record** the cost calculation using `GradientTape`.
4.  **Calculate** gradients using the tape.
5.  **Apply** gradients using the optimizer.

### Implementation Snippet
```python
import tensorflow as tf

# 1. Define Parameters
w = tf.Variable(0.0, dtype=tf.float32)
optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)

# 2. Training Loop
for i in range(1000):
    # 3. Record the Forward Pass
    with tf.GradientTape() as tape:
        cost = w**2 - 10*w + 25  # (w-5)^2
        
    # 4. Calculate Gradients (Backprop)
    grads = tape.gradient(cost, [w])
    
    # 5. Update Parameters
    optimizer.apply_gradients(zip(grads, [w]))
