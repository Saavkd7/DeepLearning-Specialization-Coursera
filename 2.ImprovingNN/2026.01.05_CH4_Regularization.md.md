# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository


# Deep Learning Fundamentals: Regularization (L2)

## 1. Executive Summary
If your model has **High Variance** (Overfitting), it means the model is trying too hard to fit every single noise point in the training data, often by using very large, aggressive weight values.
**Regularization** forces the model to keep its weights ($W$) small. It does this by adding a "penalty" to the Cost Function.
* **Result:** The model becomes simpler, smoother, and generalizes better to new data.

## 2. Technical Deep Dive

### The Logic: Penalizing Complexity
We modify the Cost Function $J$ to include a penalty term based on the size of the weights $W$.
$$J_{regularized} = J_{original} + \underbrace{\frac{\lambda}{2m} ||W||^2}_{\text{The Penalty}}$$
* **$\lambda$ (Lambda):** The **Regularization Hyperparameter**. You tune this on the Dev Set.
    * High $\lambda$ $\rightarrow$ Weights are crushed toward zero (High Bias).
    * Low $\lambda$ $\rightarrow$ Standard model (High Variance risk).

### 1. Logistic Regression (L2 Norm)
For a single vector $w$:
$$||w||^2 = \sum_{j=1}^{n_x} w_j^2 = w^T w$$
* This is the squared Euclidean Norm (L2 Norm).
* *Note:* We usually **do not** regularize the bias $b$ because it is just one parameter compared to thousands in $w$.

### 2. Neural Networks (Frobenius Norm)
For a matrix $W^{[l]}$, we sum the squares of *every element* in the matrix.
$$||W^{[l]}||_F^2 = \sum_{i=1}^{n^{[l]}} \sum_{j=1}^{n^{[l-1]}} (w_{ij}^{[l]})^2$$
* This is called the **Frobenius Norm** (denoted with a subscript $F$).

### 3. "Weight Decay" (The Gradient Update)
When you derive the gradient with this new cost function, the update rule changes:
$$W^{[l]} = W^{[l]} - \alpha \cdot \left( \text{from\_backprop} + \frac{\lambda}{m} W^{[l]} \right)$$

This can be rewritten as:
$$W^{[l]} = \underbrace{\left( 1 - \frac{\alpha \lambda}{m} \right)}_{\text{Decay Factor}} W^{[l]} - \alpha \cdot (\text{from\_backprop})$$
* **Why "Weight Decay"?** Before the model even learns from the data, the weights are multiplied by a number slightly less than 1 (e.g., 0.99). They constantly "decay" toward zero unless the data fights back.

## 3. "In Plain English"

### The "Simplify" Button
Imagine you are trying to fit a curve to data points.
* **Without Regularization:** The model is allowed to use massive coefficients (e.g., $1000x^5 - 5000x^4...$) to wiggle the line through every single dot.
* **With Regularization:** You tell the model, "You can use any equation you want, but **every time you use a large number, I will fine you.**"
* **The Result:** The model stops using massive numbers to chase outliers. It settles for a simpler, smoother curve (smaller numbers) that captures the main trend but ignores the noise.

## 4. Expert Nuance

### L1 vs. L2 Regularization
* **L2 (Ridge):** Uses squared weights ($w^2$). Penalizes outliers heavily. Keeps all weights small but non-zero. **(Standard for Deep Learning)**.
* **L1 (Lasso):** Uses absolute values ($|w|$). Can force weights to become **exactly zero**.
    * *Use Case:* If you want "Model Compression" (sparse models with fewer parameters), L1 helps. However, in practice, L2 almost always performs better for training accuracy.

## 5. Implementation Code

This snippet shows how to implement L2 Regularization in the cost and backward steps. Note that `lambda` is a reserved keyword in Python, so we use `lambd`.

```python
import numpy as np

def compute_cost_with_regularization(A3, Y, parameters, lambd):
    """
    Implements the cost function with L2 regularization.
    """
    m = Y.shape[1]
    W1 = parameters["W1"]
    W2 = parameters["W2"]
    W3 = parameters["W3"]
    
    cross_entropy_cost = compute_cost(A3, Y) # The standard cost
    
    # Calculate L2 Penalty (Sum of squares of all weights)
    L2_regularization_cost = (lambd / (2 * m)) * (
        np.sum(np.square(W1)) + 
        np.sum(np.square(W2)) + 
        np.sum(np.square(W3))
    )
    
    cost = cross_entropy_cost + L2_regularization_cost
    return cost

def backward_propagation_with_regularization(X, Y, cache, lambd):
    """
    Implements the updates with weight decay.
    """
    m = X.shape[1]
    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache
    
    dZ3 = A3 - Y
    
    # Add the regularization term (lambd/m * W) to the gradient
    dW3 = 1./m * np.dot(dZ3, A2.T) + (lambd/m) * W3
    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)
    
    dA2 = np.dot(W3.T, dZ3)
    dZ2 = np.multiply(dA2, np.int64(A2 > 0)) # ReLU derivative
    
    # Add the regularization term
    dW2 = 1./m * np.dot(dZ2, A1.T) + (lambd/m) * W2
    # ... continue for layer 1 ...
    
    return gradients
