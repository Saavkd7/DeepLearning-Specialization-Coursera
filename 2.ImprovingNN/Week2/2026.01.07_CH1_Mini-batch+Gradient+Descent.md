# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository

# Optimization Algorithms: Mini-Batch Gradient Descent

## 1. Executive Summary
When you have a massive dataset (e.g., 5,000,000 examples), standard **Batch Gradient Descent** is too slow. It requires you to process *all* 5 million examples just to take **one single step** toward the minimum.
**Mini-Batch Gradient Descent** solves this by splitting the data into smaller chunks (e.g., 1,000 examples). You calculate the error and update the weights after seeing just one chunk.
* **Result:** In one pass through the data, you take 5,000 steps instead of 1. This makes training drastically faster.

## 2. The Notation Shift
We introduce a new notation to distinguish between examples, layers, and batches.

* $x^{(i)}$: The $i$-th training example.
* $z^{[l]}$: The activation of the $l$-th layer.
* $X^{\{t\}}$: The $t$-th **Mini-Batch** of data.


## 3. How it Works

### The Setup
Imagine you have $m = 5,000,000$ examples.
You decide on a **Mini-Batch size** of 1,000.
This gives you 5,000 distinct batches ($X^{\{1\}}$ to $X^{\{5000\}}$).

### The Algorithm (One "Epoch")
An **Epoch** is a single pass through the entire training set.

```python
# One Epoch
for t in range(1, 5001):
    # 1. Forward Prop on JUST the current batch (1000 examples)
    Z = forward_prop(X{t}, parameters)
    
    # 2. Compute Cost J{t}
    cost = compute_cost(Z, Y{t})
    
    # 3. Backprop on JUST the current batch
    grads = backward_prop(Z, Y{t})
    
    # 4. Update Parameters (Take a step!)
    parameters = update(parameters, grads)


