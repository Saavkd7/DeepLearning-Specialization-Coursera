# My Diary

Today  **Saturday, 17 Jan 2026**.

## General Counter
It has been    **153** days since I started this diary.

# Repository Counter

Day **25** From I started this repository




# Optimization Strategy: Learning Rate Decay

## 1. Executive Summary
When using Mini-Batch Gradient Descent, your algorithm will never perfectly converge to the minimum. Because the mini-batches are noisy, the algorithm will oscillate around the center forever if the learning rate $\alpha$ remains constant.
**Learning Rate Decay** solves this by slowly reducing $\alpha$ over time.
* **Early Phase:** $\alpha$ is large, allowing for fast learning and big steps.
* **Late Phase:** $\alpha$ becomes tiny, forcing the steps to become smaller. This dampens the noise and allows the algorithm to settle into a tighter region around the minimum.

## 2. Visual Intuition
* **Fixed Learning Rate (Blue Line):** The algorithm approaches the minimum quickly but then "wanders" around it in wide loops. It can't settle because the step size is too big for the final, delicate adjustments.
* **Decayed Learning Rate (Green Line):** As training progresses, the steps spiral inward. The path gets tighter and tighter, effectively "landing" on the minimum.


## 3. How to Implement It
There are several mathematical formulas to decay the rate. The most common one uses the **Epoch Number** (number of full passes through the data).

### The Standard Formula
$$\alpha = \frac{1}{1 + d \cdot t} \alpha_0$$

* **$\alpha_0$:** The initial learning rate (e.g., 0.2).
* **decay_rate:** A hyperparameter (e.g., 1.0).
* **epoch_num:** The current pass through the data.

**Example Calculation:**
If $\alpha_0 = 0.2$ and decay
