# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository

# Deep Learning Strategy: The Basic Recipe

## 1. Executive Summary
The "Basic Recipe" is a flowchart for improving your model. Instead of guessing random changes, you first **diagnose** the specific problem (Bias or Variance) and then apply the specific solution for *that* problem.
* **Rule 1:** If you can't fit the training set (Bias), don't bother collecting more data yet. Fix the model size first.
* **Rule 2:** If you fit the training set but fail the dev set (Variance), don't bother making the model larger. Get more data or regularize.

## 2. The Flowchart

You repeat this cycle until you achieve low bias and low variance.

### Step 1: Check High Bias? (Training Data Performance)
* **Diagnosis:** Is the Training Error high? (e.g., 15% error on cats).
* **The Fixes:**
    1.  **Bigger Network:** Add more hidden layers or more hidden units per layer. (This almost always helps).
    2.  **Train Longer:** Increase iterations or try a better optimizer (e.g., Adam).
    3.  **New Architecture:** Try a different model structure (e.g., CNN, RNN).

### Step 2: Check High Variance? (Dev Set Performance)
* **Diagnosis:** Is the Dev Error much higher than Training Error? (e.g., Train 1% vs Dev 11%).
* **The Fixes:**
    1.  **More Data:** Get more training examples. (The best solution).
    2.  **Regularization:** Apply techniques like L2 Regularization or Dropout to prevent overfitting.
    3.  **New Architecture:** Sometimes a better structure helps generalization.


## 3. "In Plain English"

### The Modern "Trade-off" (Or lack thereof)
In traditional statistics, there was a strict **Bias-Variance Trade-off**: making a model more complex reduced bias but *increased* variance.
**In Deep Learning, this trade-off is often broken.**
* **Bigger Networks:** We can increase complexity to reduce bias *without* hurting variance (as long as we regularize).
* **Big Data:** We can add data to reduce variance *without* hurting bias.
* **Result:** We can drive both errors down independently, which is why Deep Learning is so powerful.

## 4. Expert Nuance

### Does a bigger network hurt?
Almost never.
* **The Myth:** "A network that is too big will overfit."
* **The Reality:** If you use **Regularization** (which we cover next), a larger network will almost always equal or outperform a smaller one. The only downside is **Computational Cost** (it takes longer to train).
