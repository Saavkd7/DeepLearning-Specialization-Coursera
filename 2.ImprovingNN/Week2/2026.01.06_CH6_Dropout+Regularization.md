# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository

# Deep Learning Fundamentals: Dropout Regularization

## 1. Executive Summary
**Dropout** is a technique where, during training, you randomly "kill" (set to zero) a percentage of the neurons in each layer.
* **The Effect:** On every iteration, you are training a different, smaller, "diminished" network.
* **The Goal:** This prevents neurons from becoming overly reliant on any specific feature (co-adaptation). It forces the network to learn robust features that work even if some inputs go missing.


## 2. How it Works (The Coin Toss)

### Training Phase
For every hidden layer $l$ and every training example:
1.  **Flip a Coin:** For each neuron, you decide whether to keep it or kill it based on a probability `keep_prob` (e.g., 0.8).
2.  **Eliminate:** If a neuron is dropped, its value becomes 0. It loses all connections to the next layer.
3.  **Forward/Backprop:** You perform the update on this simplified network.
4.  **Repeat:** On the next iteration, you flip the coins again and drop a *different* set of neurons.

## 3. Technical Implementation: "Inverted Dropout"

The standard way to implement this is **Inverted Dropout**. This ensures that the scale of activations remains consistent between training and testing.

### The Algorithm (Layer $l=3$ example)
1.  **Create Mask ($D^{[3]}$):** Generate a matrix of random numbers. Convert them to 1 (True) or 0 (False) based on `keep_prob`.
    ```python
    D3 = np.random.rand(A3.shape[0], A3.shape[1]) < keep_prob
    ```
2.  **Apply Mask:** Zero out the corresponding activations.
    ```python
    A3 = np.multiply(A3, D3)
    ```
3.  **Scale Up (The "Inverted" Step):** Divide by `keep_prob`.
    ```python
    A3 /= keep_prob
    ```
    * **Why?** If `keep_prob = 0.8`, you removed 20% of the energy. The expected value of $A^{[3]}$ drops by 20%. By dividing by 0.8, you bump the remaining values back up to their original scale. This means **you don't need to touch the code at test time**.

## 4. Test Time: No Dropout
When you use the model to make actual predictions (Test/Production):
* **Turn Dropout OFF.** You want deterministic results, not random noise.
* **No Scaling:** Because we scaled *up* during training (Inverted Dropout), the weights are already the correct size. You simply run $Z = W A + b$ normally.

## 5. Implementation Code

```python
def forward_propagation_with_dropout(X, parameters, keep_prob=0.5):
    """
    Implements forward prop with Inverted Dropout.
    """
    # ... retrieve parameters ...
    
    # LAYER 1 (Linear -> ReLU -> Dropout)
    Z1 = np.dot(W1, X) + b1
    A1 = relu(Z1)
    
    # 1. Initialize Matrix D1
    D1 = np.random.rand(A1.shape[0], A1.shape[1])
    # 2. Convert to 0/1 (using keep_prob)
    D1 = D1 < keep_prob
    # 3. Apply mask to A1
    A1 = A1 * D1
    # 4. Scale A1 (Inverted Dropout)
    A1 = A1 / keep_prob
    
    # ... continue to next layers ...
    
    return A3, cache
