# My Diary

Today  **Saturday, 17 Jan 2026**.

## General Counter
It has been    **153** days since I started this diary.

# Repository Counter

Day **25** From I started this repository

# Foundation: Exponentially Weighted Averages

## 1. Executive Summary
Real-world data is noisy. To find the trend (the signal) amidst the noise, we use **Exponentially Weighted Averages**.
It creates a "moving average" where recent data points carry more weight, and older points fade away exponentially. This smooths out the curve, allowing the algorithm to see the general direction.

## 2. The Formula
To calculate the average $V$ on day $t$:
$$V_t = \beta V_{t-1} + (1-\beta) \theta_t$$

* **$V_t$:** The new average (smoothed value).
* **$\theta_t$:** The actual raw data point for today (e.g., today's temperature).
* **$\beta$ (Beta):** The hyperparameter that controls how much we trust the *past* versus the *current* data.


## 3. The Role of Beta ($\beta$)
The value of $\beta$ determines the "window size" of your average.
**Rule of Thumb:** You are averaging over approximately $\frac{1}{1-\beta}$ days.

### Case A: $\beta = 0.9$ (The Red Line)
* **Window:** $\frac{1}{1-0.9} = 10$ days.
* **Result:** A balanced curve. It smooths out the noise but adapts quickly when the temperature changes.

### Case B: $\beta = 0.98$ (The Green Line)
* **Window:** $\frac{1}{1-0.98} = 50$ days.
* **Result:** Extremely smooth, but **laggy**. Because it values history so much, it takes a long time to react when the weather changes (notice the green curve shifts to the right).

### Case C: $\beta = 0.5$ (The Yellow Line)
* **Window:** $\frac{1}{1-0.5} = 2$ days.
* **Result:** Very noisy. It reacts instantly to changes but fails to filter out the outliers.


## 4. Why does this matter for Neural Networks?
We aren't actually predicting weather. We use this to smooth out **Gradients**.
In Mini-Batch Gradient Descent, the gradients are noisy (zig-zagging). By applying this formula, we can smooth out the zig-zags to create a faster, straighter path to the minimum. This is the core mechanic behind **Momentum** optimization.
