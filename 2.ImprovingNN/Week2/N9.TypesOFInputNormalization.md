# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository


# 3 Types of Input Normalization (Short Guide)

While the video covered **Standardization**, there are two other common methods depending on your data.

### 1. Standardization (Z-Score) - *The method from the video*
* **Formula:** $x = \frac{x - \mu}{\sigma}$ (Subtract Mean, Divide by Variance)
* **Result:** Centers data at 0 with a variance of 1.
* **When to use:**
    * **Deep Learning Default:** This is the gold standard for neural networks (MLPs) as it keeps gradients stable.
    * **Gaussian Data:** Best when data follows a "Bell Curve."

### 2. Min-Max Normalization (Scaling)
* **Formula:** $x = \frac{x - x_{min}}{x_{max} - x_{min}}$
* **Result:** Squashes data strictly between **0 and 1**.
* **When to use:**
    * **Computer Vision:** Standard for images (pixel values 0-255) to map them to [0, 1].
    * **Bounded Data:** When you need positive values only (e.g., for ReLU inputs).

### 3. Robust Scaling
* **Formula:** Uses **Median** and **IQR** (Interquartile Range) instead of Mean/Variance.
* **Result:** Centers data but ignores extreme outliers.
* **When to use:**
    * **Dirty Data:** Use this if your dataset has massive errors or outliers (e.g., one house priced at $100 Billion). Standard mean/variance would be ruined by that one number; Robust Scaling ignores it.
