# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository


# The "Ultimate" Optimizer: Adam (Adaptive Moment Estimation)

## 1. Executive Summary
After years of research, **Adam** has emerged as one of the most robust optimization algorithms in Deep Learning.
It is essentially a hybrid that combines the best features of **Momentum** and **RMSprop**.
* **From Momentum:** It uses the "First Moment" (mean) of gradients to smooth out the path.
* **From RMSprop:** It uses the "Second Moment" (variance) to adapt the learning rate for each parameter.
* **Result:** It is fast, stable, and works well across a huge variety of architectures without needing excessive tuning.

## 2. The Algorithm: Step-by-Step

Adam implements both Momentum and RMSprop simultaneously on every iteration $t$.

### Step 1: Initialize
Initialize $V_{dW}, S_{dW}, V_{db}, S_{db}$ all to zero.

### Step 2: Compute Momentum (First Moment)
Use hyperparameter $\beta_1$ (usually 0.9).
$$V_{dW} = \beta_1 V_{dW} + (1 - \beta_1) dW$$
$$V_{db} = \beta_1 V_{db} + (1 - \beta_1) db$$
*(This is exactly the Momentum algorithm)*.

### Step 3: Compute RMSprop (Second Moment)
Use hyperparameter $\beta_2$ (usually 0.999).
$$S_{dW} = \beta_2 S_{dW} + (1 - \beta_2) (dW)^2$$
$$S_{db} = \beta_2 S_{db} + (1 - \beta_2) (db)^2$$
*(This is exactly the RMSprop algorithm)*.

### Step 4: Bias Correction
Because $V$ and $S$ are initialized to 0, they start off biased toward 0. Adam typically fixes this:
$$V_{dW}^{\text{corrected}} = \frac{V_{dW}}{1 - \beta_1^t} \quad , \quad S_{dW}^{\text{corrected}} = \frac{S_{dW}}{1 - \beta_2^t}$$
*(Note: $t$ is the current iteration number)*.

### Step 5: The Update Rule
Update weights using the combined logic:
$$W = W - \alpha \frac{V_{dW}^{\text{corrected}}}{\sqrt{S_{dW}^{\text{corrected}}} + \epsilon}$$
$$b = b - \alpha \frac{V_{db}^{\text{corrected}}}{\sqrt{S_{db}^{\text{corrected}}} + \epsilon}$$


---

## 3. The Hyperparameters (The "Defaults")

One of Adam's strengths is that its hyperparameters have established default values that work for most problems.

| Parameter | Symbol | Default Value | Description |
| :--- | :--- | :--- | :--- |
| **Learning Rate** | $\alpha$ | **Needs Tuning** | The only parameter you usually need to change. |
| **Momentum** | $\beta_1$ | **0.9** | Controls the weighted average of the gradient (First Moment). |
| **RMSprop** | $\beta_2$ | **0.999** | Controls the weighted average of the squares (Second Moment). |
| **Epsilon** | $\epsilon$ | **$10^{-8}$** | Prevents division by zero. Almost never changed. |



## 4. Why is it called "Adam"?
It stands for **Ada**ptive **M**oment Estimation.
* **$\beta_1$ (Momentum):** Estimates the **Mean** (First Moment) of the derivative.
* **$\beta_2$ (RMSprop):** Estimates the **Variance** (Second Moment) of the derivative.
