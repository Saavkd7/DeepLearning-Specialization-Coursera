# My Diary

Today  **Saturday, 17 Jan 2026**.

## General Counter
It has been    **153** days since I started this diary.

# Repository Counter

Day **25** From I started this repository

# Optimization Theory: Local Optima vs. Plateaus

## 1. Executive Summary
In the early days of AI, researchers feared **Local Optima**—getting stuck in a shallow valley and failing to find the true bottom.
Modern theory shows this fear is largely unfounded for Deep Learning. In high-dimensional spaces (millions of parameters), you rarely get stuck in local optima. The *real* enemy is **Plateaus**—flat regions where learning slows to a crawl.

## 2. The Intuition Shift

### Old View: Low-Dimensional Traps
When we look at 2D graphs, we see a "wavy" surface with many small dips.
* **The Fear:** Gradient descent rolls into a small dip (Local Optimum) and stops, missing the deeper valley nearby.
* **Why it's wrong:** This only happens in low dimensions (like 2 parameters).

### New View: High-Dimensional Saddle Points
Neural networks optimize over thousands or millions of dimensions.
* **The Math:** For a point to be a "Local Optimum," the curve must bend **UP** in *all* 20,000 directions simultaneously. The odds of this happening are astronomically low ($2^{-20,000}$).
* **The Reality (Saddle Points):** Most points with zero gradient are actually **Saddle Points**. The curve bends **UP** in some directions but **DOWN** in others.
* **Analogy:** Think of a **Horse Saddle**. The rider sits at a point where the leather goes up (front/back) but also goes down (left/right). The gradient is zero, but it's not a trap—you can still slide off to the side.


## 3. The Real Problem: Plateaus
If we don't get stuck, why is training sometimes so slow?
**The Plateau:** This is a vast, flat region where the gradient is very close to zero.
* **The Effect:** Gradient descent takes tiny steps across this flat land. It can take thousands of iterations just to cross the plateau and find the slope again.
* **The Fix:** This is exactly why we use **Momentum** and **Adam**. They build up speed (velocity) to "surf" across these flat plateaus quickly, whereas standard gradient descent would just crawl.


---

## 4. Course Wrap-Up (Week 2)
This concludes the second week of "Improving Deep Neural Networks." You now have a suite of advanced algorithms to speed up training:
1.  **Mini-Batch Gradient Descent:** To handle big data.
2.  **Momentum:** To damp oscillations.
3.  **RMSprop:** To adapt learning rates.
4.  **Adam:** The industry standard combining both.
5.  **Learning Rate Decay:** For final convergence.
