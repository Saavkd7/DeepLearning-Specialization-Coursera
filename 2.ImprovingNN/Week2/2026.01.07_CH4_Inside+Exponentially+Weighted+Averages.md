# My Diary

Today  **Saturday, 17 Jan 2026**.

## General Counter
It has been    **153** days since I started this diary.

# Repository Counter

Day **25** From I started this repository

# Deep Dive: Inside Exponentially Weighted Averages

## 1. The Math: Unrolling the Recursion
Why is it called "Exponentially Weighted"? Let's look at the math by expanding the formula for the 100th day ($V_{100}$), assuming $\beta = 0.9$.

$$V_{100} = 0.1 \theta_{100} + 0.9 V_{99}$$

If we substitute the formula for $V_{99}$ into this equation, and then for $V_{98}$, and so on, we get a long sum:

$$V_{100} = 0.1 \theta_{100} + 0.1 \times 0.9 \theta_{99} + 0.1 \times (0.9)^2 \theta_{98} + 0.1 \times (0.9)^3 \theta_{97} \dots$$


### The Insight
* **Weighted Sum:** $V_{100}$ is just a sum of all previous temperatures, each multiplied by a specific weight.
* **Exponential Decay:** The weights are $0.1, 0.1 \times 0.9, 0.1 \times 0.9^2 \dots$. Notice that as you go further back in time, the weight gets smaller and smaller exponentially. The data from 100 days ago has almost zero influence on today's average.

---

## 2. The "One Over $e$" Rule (The Window Size)
How do we know that $\beta=0.9$ equals "10 days"? It comes from a mathematical approximation involving the constant $e$ (Euler's number).

* **The Math:** $(1 - \epsilon)^{1/\epsilon} \approx \frac{1}{e} \approx 0.35$.
* **The Meaning:** It takes about $\frac{1}{1-\beta}$ days for the weight of a data point to decay to about **35%** (1/3) of its original value. After that point, its influence is negligible.

**Examples:**
* If $\beta = 0.9$: Window $\approx \frac{1}{1-0.9} = 10$ days.
* If $\beta = 0.98$: Window $\approx \frac{1}{1-0.98} = 50$ days.
* This rule of thumb tells you essentially how far back the average is "looking".

---

## 3. Why use this instead of a Standard Average?
You might ask: *"Why not just keep the last 10 days in a list and average them normally?"*

### The Efficiency Argument
1.  **Memory:** A standard moving average requires you to keep the last 50 numbers in memory (a buffer). If you have millions of variables (like in a deep neural network), keeping a history buffer for every single one is impossible.
2.  **Speed:** Exponentially Weighted Averages require **only one number** in memory ($V$). You simply overwrite it on every step.
    * Code: `v = beta * v + (1-beta) * theta`
3.  **Conclusion:** It is slightly less accurate than a perfect moving window, but it is massively more efficient, which is critical for optimization.


## 4. Implementation Code
Here is the full algorithm logic in Python pseudocode. Notice it relies on a single variable `v_theta`.

```python
# Initialization
v_theta = 0

# The Training Loop
for t in range(num_days):
    # Get new data point
    theta_t = get_temperature(t)
    
    # Update the moving average (One line of code!)
    v_theta = beta * v_theta + (1 - beta) * theta_t
