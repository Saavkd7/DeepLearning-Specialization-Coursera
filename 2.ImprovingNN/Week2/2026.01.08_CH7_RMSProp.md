# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository


# Optimization Algorithm: RMSprop

## 1. Executive Summary
Like Momentum, **RMSprop** is designed to speed up gradient descent.
However, it approaches the problem differently. Instead of using velocity to smooth out the path, it **adapts the learning rate** for each parameter individually.
* **Goal:** Slow down learning on parameters with high oscillation (steep slopes) and speed up learning on parameters with low oscillation (flat slopes).

## 2. The Intuition (Damping Oscillations)

### The Scenario
Imagine the standard "elongated bowl" cost function.
* **Vertical Axis ($b$):** Very steep. The algorithm oscillates wildly up and down. We want to **slow this down**.
* **Horizontal Axis ($W$):** Very flat. The algorithm moves too slowly toward the center. We want to **speed this up**.

### The RMSprop Solution
RMSprop calculates the "square" of the derivatives.
* **Vertical ($db$):** The derivative is large (e.g., 10). The square is huge (100). If we divide our update step by this huge number, the step becomes tiny. **Result:** Oscillations are dampened.
* **Horizontal ($dW$):** The derivative is small (e.g., 0.1). The square is tiny (0.01). If we divide our update step by this tiny number, the step becomes larger. **Result:** Progress accelerates.


---

## 3. The Algorithm

On iteration $t$, compute derivatives $dW, db$ on the current mini-batch. Then update the "Squared Gradient" term $S$:

$$S_{dW} = \beta_2 S_{dW} + (1 - \beta_2) (dW)^2$$
$$S_{db} = \beta_2 S_{db} + (1 - \beta_2) (db)^2$$
*(Note: The squaring operation is element-wise)*.

Then, update the weights by dividing the gradient by the square root of $S$:

$$W = W - \alpha \frac{dW}{\sqrt{S_{dW}} + \epsilon}$$
$$b = b - \alpha \frac{db}{\sqrt{S_{db}} + \epsilon}$$

### Key Components
* **$S_{dW}, S_{db}$:** These track the magnitude of recent gradients.
* **Square Root ($\sqrt{\dots}$):** This scales the magnitude back down so the units match.
* **$\epsilon$ (Epsilon):** A tiny number (e.g., $10^{-8}$) added to the denominator to prevent "Division by Zero" errors if $S$ is very small.

---

## 4. Interesting Fact
Unlike most algorithms, RMSprop was not published in a formal academic paper. It was introduced to the world by **Geoffrey Hinton** (one of the "Godfathers of AI") in a lecture for his Coursera class. It worked so well that it became a standard technique in the field immediately.
