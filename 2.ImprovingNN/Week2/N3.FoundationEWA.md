# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository

# Foundation: Exponentially Weighted Averages (EWA)

## 1. Executive Summary
Real-world data, such as network traffic or temperature, is inherently noisy. To identify the underlying trend (the signal) amidst this noise, we utilize **Exponentially Weighted Averages**. This method creates a "moving average" where recent data points carry the most weight, while older points fade away exponentially over time. This smoothing effect allows algorithms to see the general direction of the data rather than getting lost in individual fluctuations.



---

## 2. The Mathematical Formula
To calculate the smoothed average ($V$) on a specific day ($t$), we use the following formula:

$$V_t = \beta V_{t-1} + (1 - \beta)\theta_t$$

* **$V_t$**: The new average or smoothed value for today.
* **$\theta_t$**: The actual raw data point recorded today (e.g., current throughput or temperature).
* **$\beta$ (Beta)**: The hyperparameter (between 0 and 1) that controls the "memory" of the algorithmâ€”how much we trust past data versus current data.

---

## 3. The Impact of Beta ($\beta$)
The value of $\beta$ determines the "window size" of your average. As a **Rule of Thumb**, you are averaging over approximately the last $\frac{1}{1-\beta}$ units of time.

| Beta ($\beta$) | Window Size | Resulting Behavior |
| :--- | :--- | :--- |
| **0.9** | ~10 days | **Balanced**: Smooths out noise while remaining adaptive to changes. |
| **0.98** | ~50 days | **Extremely Smooth**: Highly values history, but suffers from significant **latency/lag**. |
| **0.5** | ~2 days | **Noisy**: Reacts instantly to every change but fails to filter out outliers. |

---

## 4. Why it Matters for Neural Networks (Momentum)
In the context of Deep Learning, we do not use EWA just to predict the weather; we use it to smooth out **Gradients**.

In **Mini-Batch Gradient Descent**, gradients often exhibit a "zig-zag" behavior because each batch is slightly different. By applying the EWA formula to these gradients:
1.  The vertical oscillations (noise) cancel each other out over time.
2.  The horizontal movement (the trend toward the minimum) is reinforced.
3.  This creates a **faster, straighter path** to the global minimum.

This specific application of EWA is the core mechanic behind the **Momentum** optimization algorithm.
