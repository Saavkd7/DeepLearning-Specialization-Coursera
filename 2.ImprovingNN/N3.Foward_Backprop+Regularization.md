# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository


# Short Answer: No, both apply to BOTH phases.

It is a common misconception, but technically both techniques modify **Forward** and **Backward** propagation.

### 1. L2 Regularization
* **Forward Prop:** You must modify the **Cost Function ($J$)** calculation to include the penalty term $\frac{\lambda}{2m} ||W||^2$.
* **Backward Prop:** You must modify the **Gradient ($dW$)** calculation to include the weight decay term $\frac{\lambda}{m} W$.

### 2. Dropout
* **Forward Prop:** You apply the mask ($D$) to zero out activations ($A$).
* **Backward Prop:** You must apply the **same mask** to the derivatives ($dA$). If a neuron was shut off in the forward pass, it cannot receive any error signal in the backward pass. You also apply the ` / keep_prob` scaling here to maintain mathematical consistency.
