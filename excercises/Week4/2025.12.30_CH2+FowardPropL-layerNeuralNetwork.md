---
# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository

# Deep Learning Fundamentals: Forward Propagation in Deep Networks

## 1. Executive Summary
Forward propagation in a deep network is just the process of Logistic Regression repeated multiple times.
* **Layer 1:** Takes the input $X$ and computes an activation $A^{[1]}$.
* **Layer 2:** Takes $A^{[1]}$ as *its* input and computes $A^{[2]}$.
* **Layer L:** Repeats this until the final output $\hat{y}$ is generated.
While we vectorize across examples to avoid loops over data, we **must** use a loop to iterate through the layers because Layer 3 cannot exist without the result of Layer 2.

## 2. Technical Deep Dive

### The General Recursive Formula
To compute any layer $l$, we use the output from the previous layer $l-1$.
* **The Linear Step:**
    $$Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$$
* **The Activation Step:**
    $$A^{[l]} = g^{[l]}(Z^{[l]})$$
* **Base Case:** $A^{[0]} = X$ (The Input Data).

### Vectorized Forward Propagation
Instead of processing one image at a time, we stack all training examples ($m$) into columns.
* **$Z^{[l]}$ Matrix:** Shape $(n^{[l]}, m)$.
* **$A^{[l]}$ Matrix:** Shape $(n^{[l]}, m)$.
* **Formula:**
    $$Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$$
    $$A^{[l]} = g^{[l]}(Z^{[l]})$$
    *(Note: $b^{[l]}$ is broadcasted across the $m$ columns)*.

## 3. "In Plain English"

### The "Bucket Brigade" Analogy
Imagine a line of people trying to put out a fire.
* **The Well ($A^{[0]}$):** The source of water (Input Data).
* **Person 1 (Layer 1):** Fills a bucket, maybe adds soap, passes it to Person 2.
* **Person 2 (Layer 2):** Takes the bucket from Person 1, adds foam, passes it to Person 3.
* **Person 3 (Layer 3):** Throws it on the fire (Prediction).
* **The Constraint:** Person 3 cannot do anything until Person 2 hands them the bucket. The process is **inherently sequential**. You cannot calculate Layer 3 before Layer 2 is finished.

## 4. Expert Nuance

### The "One Allowed Loop"
In Deep Learning, we usually ban `for` loops (to speed up training). However, **Forward Propagation over layers is the exception.**
* **Why:** You cannot vectorize *across layers* because of the sequential dependency.
* **Rule:** It is perfectly acceptable—and necessary—to use an explicit `for l in range(1, L):` loop in your code.

## 5. Implementation Code

This function implements the Forward Propagation loop for a network of arbitrary depth `L`.

```python
import numpy as np

def deep_forward_propagation(X, parameters):
    """
    Implements forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation
    
    Arguments:
    X -- data, numpy array of shape (input size, number of examples)
    parameters -- output of initialization function containing W1, b1, W2, b2, ...
    """
    caches = []
    A = X
    L = len(parameters) // 2  # number of layers (since parameters has W and b for each)
    
    # --- HIDDEN LAYERS LOOP (1 to L-1) ---
    # We use ReLU for all hidden layers
    for l in range(1, L):
        A_prev = A 
        
        # Retrieve W and b for this layer
        W = parameters['W' + str(l)]
        b = parameters['b' + str(l)]
        
        # Linear Step
        Z = np.dot(W, A_prev) + b
        
        # Activation Step (ReLU)
        A = np.maximum(0, Z) 
        
        # Save cache for Backprop later
        cache = (A_prev, W, b, Z)
        caches.append(cache)
        
    # --- OUTPUT LAYER (L) ---
    # We use Sigmoid for the final layer (Binary Classification)
    W = parameters['W' + str(L)]
    b = parameters['b' + str(L)]
    
    Z = np.dot(W, A) + b
    AL = 1 / (1 + np.exp(-Z)) # Sigmoid
    
    # Save final cache
    cache = (A, W, b, Z)
    caches.append(cache)
    
    return AL, caches
