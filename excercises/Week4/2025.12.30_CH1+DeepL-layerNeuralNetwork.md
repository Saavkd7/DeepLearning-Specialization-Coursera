# Deep Learning Fundamentals: Deep Neural Networks & Notation

## 1. Executive Summary
A **Deep Neural Network** is simply a neural network with multiple hidden layers (typically 2 or more). While "shallow" models (like Logistic Regression or 1-hidden-layer nets) can learn simple patterns, adding depth allows the network to learn increasingly complex and abstract functions. The number of layers is a **hyperparameter** you must tune to find the best fit for your specific data.

## 2. Technical Deep Dive

### Defining "Depth"
We distinguish networks by their number of layers.
* **Logistic Regression:** A "1-layer" network (Shallow).
* **Standard NN:** A "2-layer" network (1 hidden layer).
* **Deep NN:** A network with $L$ layers, where $L$ is large (e.g., 5, 50, or 100).

### The Notation Standard
To mathematically describe a network with arbitrarily many layers, we use the following notation:

* **$L$ (Capital):** The total number of layers in the network.
    * *Note:* We do **not** count the input layer (Layer 0) in this number.
    * Example: If a network has 3 hidden layers and 1 output layer, $L=4$.

* **$n^{[l]}$:** The number of units (nodes) in layer $l$.
    * $n^{[0]}$: Number of input features ($n_x$).
    * $n^{[L]}$: Number of output units (1 for binary classification).
    * $n^{[1]}, n^{[2]}, \dots$: Hidden units in respective layers.

* **$a^{[l]}$:** The activations of layer $l$.
    * $a^{[l]} = g^{[l]}(z^{[l]})$
    * **Special Cases:**
        * $a^{[0]} = X$ (Input Data)
        * $a^{[L]} = \hat{y}$ (Final Prediction).

* **$W^{[l]}, b^{[l]}$:** The parameters for layer $l$.
    * Weights $W^{[l]}$ compute the transition from layer $l-1$ to layer $l$.
    * Shape of $W^{[l]}$: $(n^{[l]}, n^{[l-1]})$.

## 3. "In Plain English"

### The "Assembly Line" Analogy
Think of a factory line:
* **Shallow Network (1 Worker):** You give raw metal to a worker, and they have to produce a finished car in one step. They can only make very simple toy cars.
* **Deep Network (50 Workers):**
    * Worker 1 (Layer 1): Bends the metal.
    * Worker 2 (Layer 2): Welds the frame.
    * Worker 3 (Layer 3): Installs the engine.
    * ...
    * Worker 50 (Output): Polishes the paint.
    * By breaking the problem down into many sequential steps ("Layers"), the factory can build something infinitely more complex (a Ferrari) than a single worker could.

## 4. Expert Nuance

### The "Layer 0" Convention
A common confusion arises in counting layers. In Deep Learning, we use **1-based indexing** for the weights but **0-based indexing** for the data.
* The Input $X$ is **Layer 0**. It has no weights feeding into it.
* The first set of weights $W^{[1]}$ connects Layer 0 to Layer 1.
* Therefore, a network with 3 hidden layers and 1 output is a **4-Layer Network** ($L=4$), not 5.

## 5. Implementation Code

This Python snippet demonstrates how to define a Deep Network architecture using a dictionary to hold the structure, matching the notation.

```python
def describe_deep_network():
    # Example: A 4-Layer Network (3 Hidden + 1 Output)
    # Layer 0 (Input): 3 Features
    # Layer 1: 5 Neurons
    # Layer 2: 5 Neurons
    # Layer 3: 3 Neurons
    # Layer 4 (Output): 1 Neuron
    
    layer_dims = [3, 5, 5, 3, 1]  # [n0, n1, n2, n3, n4]
    L = len(layer_dims) - 1       # L = 4
    
    print(f"Network Depth L = {L}")
    print(f"Input Features n[0] = {layer_dims[0]}")
    print("-" * 30)
    
    # Simulate Parameter Initialization shapes
    for l in range(1, L + 1):
        # Shape of W[l]: (Current Layer nodes, Previous Layer nodes)
        w_shape = (layer_dims[l], layer_dims[l-1])
        b_shape = (layer_dims[l], 1)
        
        print(f"Layer {l}:")
        print(f"  - Nodes n[{l}]: {layer_dims[l]}")
        print(f"  - W[{l}] shape: {w_shape}")
        print(f"  - b[{l}] shape: {b_shape}")

if __name__ == "__main__":
    describe_deep_network()
