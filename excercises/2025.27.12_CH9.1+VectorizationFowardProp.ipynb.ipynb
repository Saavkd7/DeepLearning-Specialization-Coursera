{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60380ab2-00cd-4194-a6ec-d75f285572da",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5. Implementation Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b050c4b5-dc78-4240-b68c-b61ced4c3025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape: (1, 5) (Should be 1, 5)\n",
      "Values: [[0.98201379 0.98201379 0.98201379 0.98201379 0.98201379]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def propagate_vectorized(w, b, X):\n",
    "    \"\"\"\n",
    "    Computes Forward Propagation for the entire dataset X without loops.\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, shape (n_x, 1)\n",
    "    b -- bias, scalar\n",
    "    X -- data, shape (n_x, m)\n",
    "    \n",
    "    Returns:\n",
    "    A -- The vector of predictions, shape (1, m)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Linear Step (Z)\n",
    "    # np.dot computes w.T * X\n",
    "    # Python broadcasts 'b' to every element of the resulting vector\n",
    "    Z = np.dot(w.T, X) + b\n",
    "    \n",
    "    # 2. Activation Step (A)\n",
    "    # Applies sigmoid to every element of Z at once\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    return A\n",
    "\n",
    "# --- VERIFICATION ---\n",
    "if __name__ == \"__main__\":\n",
    "    n_x, m = 3, 5\n",
    "    w = np.ones((n_x, 1))\n",
    "    b = 1.0\n",
    "    X = np.ones((n_x, m))\n",
    "    \n",
    "    A = propagate_vectorized(w, b, X)\n",
    "    \n",
    "    print(f\"Output Shape: {A.shape} (Should be 1, {m})\")\n",
    "    print(f\"Values: {A}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d54fca5-040e-4683-ae45-0336d5958e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Logistic Regression ---\n",
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.564679\n",
      "Cost after iteration 200: 0.484211\n",
      "Cost after iteration 300: 0.430080\n",
      "Cost after iteration 400: 0.391193\n",
      "Cost after iteration 500: 0.361773\n",
      "Cost after iteration 600: 0.338612\n",
      "Cost after iteration 700: 0.319809\n",
      "Cost after iteration 800: 0.304166\n",
      "Cost after iteration 900: 0.290897\n",
      "Cost after iteration 1000: 0.279458\n",
      "Cost after iteration 1100: 0.269466\n",
      "Cost after iteration 1200: 0.260640\n",
      "Cost after iteration 1300: 0.252768\n",
      "Cost after iteration 1400: 0.245689\n",
      "Cost after iteration 1500: 0.239277\n",
      "Cost after iteration 1600: 0.233434\n",
      "Cost after iteration 1700: 0.228078\n",
      "Cost after iteration 1800: 0.223145\n",
      "Cost after iteration 1900: 0.218581\n",
      "Cost after iteration 2000: 0.214342\n",
      "Cost after iteration 2100: 0.210390\n",
      "Cost after iteration 2200: 0.206695\n",
      "Cost after iteration 2300: 0.203227\n",
      "Cost after iteration 2400: 0.199966\n",
      "Cost after iteration 2500: 0.196890\n",
      "Cost after iteration 2600: 0.193983\n",
      "Cost after iteration 2700: 0.191229\n",
      "Cost after iteration 2800: 0.188615\n",
      "Cost after iteration 2900: 0.186129\n",
      "Cost after iteration 3000: 0.183761\n",
      "Cost after iteration 3100: 0.181502\n",
      "Cost after iteration 3200: 0.179343\n",
      "Cost after iteration 3300: 0.177277\n",
      "Cost after iteration 3400: 0.175298\n",
      "Cost after iteration 3500: 0.173398\n",
      "Cost after iteration 3600: 0.171574\n",
      "Cost after iteration 3700: 0.169819\n",
      "Cost after iteration 3800: 0.168130\n",
      "Cost after iteration 3900: 0.166502\n",
      "Cost after iteration 4000: 0.164932\n",
      "Cost after iteration 4100: 0.163417\n",
      "Cost after iteration 4200: 0.161952\n",
      "Cost after iteration 4300: 0.160535\n",
      "Cost after iteration 4400: 0.159165\n",
      "Cost after iteration 4500: 0.157837\n",
      "Cost after iteration 4600: 0.156550\n",
      "Cost after iteration 4700: 0.155301\n",
      "Cost after iteration 4800: 0.154090\n",
      "Cost after iteration 4900: 0.152913\n",
      "\n",
      "--- Test Prediction ---\n",
      "Test Input:\n",
      "[[ 1.5 -2. ]\n",
      " [ 1.5 -2. ]]\n",
      "Predictions: [[1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Generic Logistic Regression Implementation (Vectorized)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class LogisticRegressionModel:\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.costs = []\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Compute the sigmoid of z\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def initialize_parameters(self, n_x):\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "        n_x -- size of the input layer (number of features)\n",
    "        \n",
    "        Returns:\n",
    "        w -- initialized vector of shape (n_x, 1)\n",
    "        b -- initialized scalar (corresponds to the bias)\n",
    "        \"\"\"\n",
    "        self.w = np.zeros((n_x, 1))\n",
    "        self.b = 0.0\n",
    "        return self.w, self.b\n",
    "\n",
    "    def propagate(self, w, b, X, Y):\n",
    "        \"\"\"\n",
    "        Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "        Arguments:\n",
    "        w -- weights, a numpy array of size (n_x, 1)\n",
    "        b -- bias, a scalar\n",
    "        X -- data of size (n_x, m)\n",
    "        Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, m)\n",
    "\n",
    "        Return:\n",
    "        cost -- negative log-likelihood cost for logistic regression\n",
    "        dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "        db -- gradient of the loss with respect to b, thus same shape as b\n",
    "        \"\"\"\n",
    "        m = X.shape[1]\n",
    "        \n",
    "        # --- FORWARD PROPAGATION (FROM X TO COST) ---\n",
    "        # Matrix Op: Z = w.T * X + b\n",
    "        Z = np.dot(w.T, X) + b\n",
    "        A = self.sigmoid(Z)                                 \n",
    "        \n",
    "        # Cost Function (Log Loss)\n",
    "        # We add 1e-15 to avoid log(0) errors\n",
    "        cost = (-1/m) * np.sum(Y * np.log(A + 1e-15) + (1-Y) * np.log(1-A + 1e-15))\n",
    "        cost = np.squeeze(cost)  # Ensure cost is a scalar (e.g. 17) not an array [[17]]\n",
    "        \n",
    "        # --- BACKWARD PROPAGATION (TO FIND GRADIENTS) ---\n",
    "        dZ = A - Y\n",
    "        dw = (1/m) * np.dot(X, dZ.T)\n",
    "        db = (1/m) * np.sum(dZ)\n",
    "        \n",
    "        grads = {\"dw\": dw, \"db\": db}\n",
    "        \n",
    "        return grads, cost\n",
    "\n",
    "    def optimize(self, w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n",
    "        \"\"\"\n",
    "        This function optimizes w and b by running a gradient descent algorithm\n",
    "        \"\"\"\n",
    "        costs = []\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            # 1. Calculate Gradients and Cost\n",
    "            grads, cost = self.propagate(w, b, X, Y)\n",
    "            \n",
    "            # 2. Retrieve derivatives\n",
    "            dw = grads[\"dw\"]\n",
    "            db = grads[\"db\"]\n",
    "            \n",
    "            # 3. Update Parameters (Gradient Descent Step)\n",
    "            w = w - learning_rate * dw\n",
    "            b = b - learning_rate * db\n",
    "            \n",
    "            # Record the cost\n",
    "            if i % 100 == 0:\n",
    "                costs.append(cost)\n",
    "                if print_cost:\n",
    "                    print(f\"Cost after iteration {i}: {cost:.6f}\")\n",
    "        \n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.costs = costs\n",
    "        \n",
    "        return w, b, grads, costs\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "        \"\"\"\n",
    "        m = X.shape[1]\n",
    "        Y_prediction = np.zeros((1, m))\n",
    "        w = self.w.reshape(X.shape[0], 1)\n",
    "        \n",
    "        # Compute vector A\n",
    "        A = self.sigmoid(np.dot(w.T, X) + self.b)\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        for i in range(A.shape[1]):\n",
    "            if A[0, i] > 0.5:\n",
    "                Y_prediction[0, i] = 1\n",
    "            else:\n",
    "                Y_prediction[0, i] = 0\n",
    "        \n",
    "        return Y_prediction\n",
    "\n",
    "    def fit(self, X_train, Y_train, num_iterations=2000, learning_rate=0.5, print_cost=True):\n",
    "        \"\"\"\n",
    "        Main function to train the model\n",
    "        \"\"\"\n",
    "        # Initialize\n",
    "        n_x = X_train.shape[0]\n",
    "        self.initialize_parameters(n_x)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimize(self.w, self.b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "        \n",
    "        return self\n",
    "\n",
    "# --- USAGE EXAMPLE ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Synthetic Data Generation\n",
    "    # 2 Features, 1000 Examples\n",
    "    m_train = 1000\n",
    "    n_features = 2\n",
    "    \n",
    "    # Random input data (2, 1000)\n",
    "    X = np.random.randn(n_features, m_train)\n",
    "    \n",
    "    # Generate labels: If sum of features > 0, label is 1, else 0\n",
    "    Y = (np.sum(X, axis=0, keepdims=True) > 0).astype(int)\n",
    "    \n",
    "    # 2. Create and Train Model\n",
    "    print(\"--- Training Logistic Regression ---\")\n",
    "    model = LogisticRegressionModel()\n",
    "    model.fit(X, Y, num_iterations=5000, learning_rate=0.01, print_cost=True)\n",
    "    \n",
    "    # 3. Predict on new data\n",
    "    X_test = np.array([[1.5, -2.0], [1.5, -2.0]]) # 2 examples\n",
    "    print(\"\\n--- Test Prediction ---\")\n",
    "    predictions = model.predict(X_test)\n",
    "    print(f\"Test Input:\\n{X_test}\")\n",
    "    print(f\"Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4563bbb8-9865-4433-b73f-bbe8c1c317a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Kev-3.11.11)",
   "language": "python",
   "name": "kev3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
