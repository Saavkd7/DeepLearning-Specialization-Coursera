{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61a34145-bdea-48e4-84a3-6df674965436",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5. Implementation Code\n",
    "\n",
    "Here is how to implement these functions and their derivatives (needed for backprop) in NumPy.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c899c1b-3ab2-4985-8382-1a3e796e45c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [-10  -1   0   1  10]\n",
      "Sigmoid:    [4.53978687e-05 2.68941421e-01 5.00000000e-01 7.31058579e-01\n",
      " 9.99954602e-01]\n",
      "Tanh:       [-1.         -0.76159416  0.          0.76159416  1.        ]\n",
      "ReLU:       [ 0  0  0  1 10]\n",
      "Leaky ReLU: [-0.1  -0.01  0.    1.   10.  ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ActivationFunctions:\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        \"\"\"Standard choice for Binary Output Layer\"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(z):\n",
    "        \"\"\"Better choice for Hidden Layers (Zero-centered)\"\"\"\n",
    "        return np.tanh(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        \"\"\"Best choice for Hidden Layers (No vanishing gradient)\"\"\"\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    @staticmethod\n",
    "    def leaky_relu(z, alpha=0.01):\n",
    "        \"\"\"Fixes 'Dead ReLU' problem\"\"\"\n",
    "        return np.maximum(alpha * z, z)\n",
    "\n",
    "    # --- DERIVATIVES (For Backpropagation) ---\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        \"\"\"\n",
    "        Gradient is 1 if z > 0, else 0.\n",
    "        Technically undefined at z=0, but we set it to 0 or 1 in practice.\n",
    "        \"\"\"\n",
    "        dz = np.array(z, copy=True)\n",
    "        dz[z <= 0] = 0\n",
    "        dz[z > 0] = 1\n",
    "        return dz\n",
    "\n",
    "# --- DEMO ---\n",
    "if __name__ == \"__main__\":\n",
    "    z_test = np.array([-10, -1, 0, 1, 10])\n",
    "    \n",
    "    print(f\"Input: {z_test}\")\n",
    "    print(f\"Sigmoid:    {ActivationFunctions.sigmoid(z_test)}\")\n",
    "    print(f\"Tanh:       {ActivationFunctions.tanh(z_test)}\")\n",
    "    print(f\"ReLU:       {ActivationFunctions.relu(z_test)}\")\n",
    "    print(f\"Leaky ReLU: {ActivationFunctions.leaky_relu(z_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b2d338-1ff5-4037-b283-5db3117e06a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Kev-3.11.11)",
   "language": "python",
   "name": "kev3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
