



# Handling Incorrectly Labeled Data

## 1. The Problem
Sometimes, your data has the wrong labels (e.g., a picture of a dog labeled as "Cat" by the human labeler).
* **Mislabeled:** The algorithm predicts wrong.
* **Incorrectly Labeled:** The ground truth label ($Y$) in the dataset is actually wrong.


## 2. Training Set: Do you need to fix it?
**Generally, No.**
* **Random Errors:** Deep learning algorithms are remarkably robust to random labeling errors (e.g., a labeler accidentally hitting the wrong key). If the dataset is large enough, the algorithm will ignore these outliers.
* **Systematic Errors:** These **ARE** a problem. If a labeler consistently marks "White Dogs" as "Cats," the model will learn this mistake. You must fix systematic errors.

## 3. Dev/Test Sets: Do you need to fix it?
It depends on how much the label errors are affecting your evaluation. You determine this using **Error Analysis**.

**The Procedure:**
Add a column to your error analysis table for "Incorrectly Labeled."

| Image | Dog | Great Cat | Blurry | **Incorrect Label** | Comments |
| :--- | :--- | :--- | :--- | :--- | :--- |
| 98 | | | | ✓ | Labeler missed cat in background |
| 100 | | | | ✓ | Drawing of a cat, not real |
| ... | ... | ... | ... | ... | ... |
| **%** | 8% | 43% | 61% | **6%** | |


**The Decision Criteria (Two Examples):**

| Metric | Scenario A (High Error) | Scenario B (Low Error) |
| :--- | :--- | :--- |
| **Total Dev Error** | 10% | 2% |
| **Errors due to Label** | 0.6% (6% of total errors) | 0.6% (**30%** of total errors) |
| **Errors due to Model** | 9.4% | 1.4% |
| **Recommendation** | **Ignore.** Focus on the 9.4% (Blurry/Great Cats). | **Fix.** Label errors are now a huge chunk (30%) of your problem. |

* **Why Fix in Scenario B?** If you are trying to decide between Classifier A (2.1% error) and Classifier B (1.9% error), you can't trust the result if 0.6% of that error is just noise from bad labels.


## 4. Guidelines for Correcting Labels
If you decide to fix the labels in your Dev/Test sets, follow these rules:

1.  **Fix Both Dev and Test:** You must apply the same correction process to **both** sets to ensure they remain on the same distribution.
2.  **Check "Right" Examples Too:** Ideally, you should check images the model got *right* as well. It's possible the model got it "right" only because the label was wrong (lucky guess). Only fixing the "wrong" ones introduces bias (though checking "right" ones is tedious and often skipped).
3.  **Training Set Can Differ:** It is acceptable to fix the Dev/Test sets while leaving the Training set alone. Deep learning is robust to the training data coming from a slightly different distribution than the dev/test data.
