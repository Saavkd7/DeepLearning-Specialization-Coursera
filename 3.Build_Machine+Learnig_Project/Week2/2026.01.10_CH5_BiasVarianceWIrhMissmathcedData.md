Today  **Saturday, 17 Jan 2026**.

## General Counter
It has been    **153** days since I started this diary.

# Repository Counter

Day **25** From I started this repository


# Machine Learning Strategy: Bias and Variance with Mismatched Data

## 1. The Challenge: The "Fog of Distribution"
When your **Training Set** comes from a different distribution than your **Dev/Test Sets** (e.g., Web images vs. Mobile images), you cannot simply compare Training Error to Dev Error to diagnose "Variance."

* **The Problem:** If Training Error is 1% and Dev Error is 10%, two things have changed:
    1.  The model hasn't seen the specific examples in the Dev set.
    2.  The *distribution* of the Dev set is different.
    * Is the model overfitting (Variance)? Or is the Dev set simply *harder* because it's a different distribution (Data Mismatch)? You can't tell.
* **The Solution:** You need a new "Bridge" dataset called the **Training-Dev Set**.

## 2. The Training-Dev Set
* **Definition:** A random subset of the **Training Data** that you *hold out* (do not train on).
* **Properties:**
    * It comes from the **same distribution** as the Training Set.
    * The model is **not trained** on it (not used for backpropagation).
* **Purpose:** It isolates the "Variance" problem from the "Data Mismatch" problem.



## 3. Diagnosing the Error Gaps
You now have four error rates to compare. The gaps between them tell you exactly what is wrong.

### Example Scenario A: High Variance
* **Training Error:** 1%
* **Training-Dev Error:** 9%
* **Dev Error:** 10%
* **Analysis:**
    * **Variance Gap (Train $\to$ Train-Dev):** $9\% - 1\% = 8\%$.
    * *Diagnosis:* The model fails on unseen data *from the same distribution*. This is a **Variance** problem.
    * **Mismatch Gap (Train-Dev $\to$ Dev):** $10\% - 9\% = 1\%$.
    * *Diagnosis:* The model handles the distribution shift well. Low Data Mismatch.

### Example Scenario B: High Data Mismatch
* **Training Error:** 1%
* **Training-Dev Error:** 1.5%
* **Dev Error:** 10%
* **Analysis:**
    * **Variance:** $1.5\% - 1\% = 0.5\%$ (Low).
    * **Mismatch:** $10\% - 1.5\% = 8.5\%$ (High).
    * *Diagnosis:* The model generalizes well within the training distribution, but fails specifically because the Dev data is different. This is a **Data Mismatch** problem.



## 4. Summary of Gaps (The General Formulation)
By arranging your data sources, you can define every type of error:

| Gap Name | Calculation | Diagnosis |
| :--- | :--- | :--- |
| **Avoidable Bias** | Train Error - Human Error | Underfitting the training data. |
| **Variance** | Train-Dev Error - Train Error | Overfitting the training distribution. |
| **Data Mismatch** | Dev Error - Train-Dev Error | Model fails to handle the distribution shift. |
| **Overfitting Dev** | Test Error - Dev Error | Overfitting to the Dev set specifically. |
