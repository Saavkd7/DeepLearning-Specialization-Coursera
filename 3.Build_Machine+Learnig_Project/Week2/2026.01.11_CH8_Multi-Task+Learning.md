# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository


# Multi-task Learning

## 1. Executive Summary
Unlike **Transfer Learning** (which is sequential: Learn A $\to$ Transfer to B), **Multi-task Learning** trains a single neural network to perform multiple tasks at the same time.
* **The Goal:** The network uses shared lower-level features to help performance across all distinct tasks simultaneously.

## 2. The Example: Autonomous Driving
Imagine building a self-driving car. The car needs to detect multiple objects in a single image:
1.  Pedestrians
2.  Cars
3.  Stop Signs
4.  Traffic Lights

* **Input ($x$):** An image of the road.
* **Output ($y$):** Unlike Softmax (where an image is *either* a cat *or* a dog), here an image can have **multiple labels**.
    * $y^{(i)}$ is a $4 \times 1$ vector.
    * Example: $y = [0, 1, 1, 0]^T$ means "No pedestrian, Yes car, Yes stop sign, No traffic light".


## 3. The Architecture & Loss Function
You build one neural network with **four output nodes** (one for each class).

* **The Loss Function:** The cost is the average of the logistic regression losses for each of the 4 individual output components.
$$J = \frac{1}{m} \sum_{i=1}^{m} \sum_{j=1}^{4} \mathcal{L}(\hat{y}_j^{(i)}, y_j^{(i)})$$
* **Difference from Softmax:** Softmax assumes classes are mutually exclusive (sum to 1). Multi-task learning asks 4 independent "Yes/No" questions for every image.


### Handling Missing Data
Multi-task learning works even if your labels are incomplete.
* *Example:* The labeler marked "Pedestrian" and "Car" but forgot to check for "Stop Signs" (labeled as `?`).
* *Solution:* The loss function simply sums over the valid labels ($0$ or $1$) and ignores the undefined ones ($?$).

## 4. When Does Multi-task Learning Make Sense?
It is not always better than training separate neural networks. It makes sense when:

1.  **Shared Low-Level Features:** The tasks are related enough that they benefit from similar processing (e.g., detecting cars and stop signs both require recognizing edges, curves, and road shapes).
2.  **Similar Data Amounts:** Usually, the tasks have roughly similar amounts of data. This allows the massive aggregate data (e.g., 99 tasks) to boost the performance of the single task you are focusing on.
3.  **Big Network:** You can train a sufficiently large neural network. (If the network is too small, performance will degrade compared to separate networks).


## 5. Summary vs. Transfer Learning
* **Transfer Learning:** Used when you have a lot of data for Task A and very little for Task B. **(Very Common)**.
* **Multi-task Learning:** Used when you want to predict multiple things simultaneously and have decent data for all of them. **(Less Common, mostly used in Object Detection/Computer Vision)**.
