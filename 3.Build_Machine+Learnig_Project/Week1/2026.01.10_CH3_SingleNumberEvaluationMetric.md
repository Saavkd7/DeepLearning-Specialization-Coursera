# My Diary

Today  **{{DATE_PRETTY}}**.

## General Counter
It has been    **{{DAY_SINCE_2025_08_18}}** days since I started this diary.

# Repository Counter

Day **{{DAYS_SINCE_REPO_START}}** From I started this repository


# Machine Learning Strategy: Single Number Evaluation Metric

## 1. Executive Summary
Machine learning is an iterative process: **Idea $\rightarrow$ Code $\rightarrow$ Experiment**.
To move fast, you need to quickly evaluate the results of your experiments.
* **The Rule:** Create a **Single Number Evaluation Metric**.
* **Why:** If you have multiple metrics (e.g., Precision vs. Recall, or Accuracy in 4 different countries), it is hard to compare models. One model might be better at A, while the other is better at B. This slows down decision-making. A single number allows you to instantly rank your models and pick the winner.


## 2. Example: Precision vs. Recall
Suppose you have two classifiers, A and B.
* **Precision:** When it predicts "Cat," how often is it right?
* **Recall:** Of all the real "Cats," how many did it find?

| Classifier | Precision (P) | Recall (R) |
| :--- | :--- | :--- |
| **A** | 95% | 90% |
| **B** | 98% | 85% |

**The Problem:** Classifier A has better Recall, but B has better Precision. Which one is "best"? It's ambiguous.

**The Solution (F1 Score):**
Combine them into a single score using the **Harmonic Mean** (F1 Score).
$$F1 = \frac{2}{\frac{1}{P} + \frac{1}{R}}$$

| Classifier | F1 Score |
| :--- | :--- |
| **A** | **92.4%** |
| **B** | 91.0% |

Now the choice is obvious: **Classifier A** is the winner.


## 3. Example: Multiple Geographies
Suppose you are tracking error rates in different markets (US, China, India, Other).
* **The Problem:** Model A is better in the US, but Model B is better in China. It's hard to compare.
* **The Solution:** Take the **Average** error across all geographies. Now you have a single target to optimize.
