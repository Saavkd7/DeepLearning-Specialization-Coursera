# My Diary

Today  **Saturday, 17 Jan 2026**.

## General Counter
It has been    **153** days since I started this diary.

# Repository Counter

Day **25** From I started this repository


# Machine Learning Strategy: Avoidable Bias

## 1. The Concept
Traditionally, we look at **Training Error** and **Dev Error** to decide if we have a bias or variance problem. However, this is incomplete without knowing the **Bayes Error** (the theoretical limit of performance).

Since we often cannot know the exact Bayes Error, we use **Human-Level Performance** as a proxy.
* **Avoidable Bias:** The difference between *Training Error* and *Bayes Error* (Human-Level). This is the error you can actually "fix" by training a better model.
* **Variance:** The difference between *Dev Error* and *Training Error*.


## 2. Why Context Matters (The 8% / 10% Example)
Imagine your model has **8% Training Error** and **10% Dev Error**. Should you focus on Bias or Variance? The answer depends entirely on how hard the task is (Human-Level Performance).

### Scenario A: The Task is Easy
* **Human Error:** 1% (e.g., clear images).
* **Training Error:** 8%.
* **Dev Error:** 10%.
* **Analysis:**
    * **Avoidable Bias:** $8\% - 1\% = 7\%$ (Huge gap).
    * **Variance:** $10\% - 8\% = 2\%$ (Small gap).
* **Strategy:** Focus on **Bias Reduction** (train a bigger network, train longer, better optimizer) because the model is failing to fit the training set compared to humans.

### Scenario B: The Task is Hard
* **Human Error:** 7.5% (e.g., extremely blurry images, even humans struggle).
* **Training Error:** 8%.
* **Dev Error:** 10%.
* **Analysis:**
    * **Avoidable Bias:** $8\% - 7.5\% = 0.5\%$ (Tiny gap; you are essentially doing as well as humans).
    * **Variance:** $10\% - 8\% = 2\%$ (Larger gap).
* **Strategy:** Focus on **Variance Reduction** (regularization, more data) because your training performance is already near the theoretical limit.


## 3. Summary of Gaps
Using Human-Level error as a baseline allows you to break down your total error into two actionable parts:

1.  **Gap 1 (Human $\to$ Training):** "Avoidable Bias."
    * *Fix:* Bigger model, better optimization.
2.  **Gap 2 (Training $\to$ Dev):** "Variance."
    * *Fix:* Regularization, more data.
